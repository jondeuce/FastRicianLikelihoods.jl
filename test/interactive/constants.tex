\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage[margin=1in]{geometry}

\title{Stable, efficient evaluation of gradients and Hessians for the Rician log-likelihood}
\author{Jonathan Doucette}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}

We consider the Rician probability density for a positive observation $x>0$ with noncentrality parameter $\nu>0$ and scale $\sigma>0$.
The pdf is
%
\begin{align}
  p(x \mid \nu, \sigma) = \frac{x}{\sigma^2} \exp\left(-\frac{x^2+\nu^2}{2\sigma^2}\right) I_0\left(\frac{x\nu}{\sigma^2}\right) \label{eq:rician-pdf}
\end{align}
%
where $I_0$ is the modified Bessel function of the first kind of order zero.

We seek numerically stable, machine-precision formulas for the negative log-likelihood and its first- through third-order partial derivatives with respect to $x$ and $\nu$.
If we define
%
\begin{align}
  f(x, \nu) \coloneqq -\log p(x \mid \nu, \sigma = 1),
\end{align}
%
then the log of the likelihood of \cref{eq:rician-pdf} can be given in terms of $f$ via
%
\begin{align}
  \log p(x \mid \nu, \sigma) = \log\sigma - f\left(\frac{x}{\sigma}, \frac{\nu}{\sigma}\right).
\end{align}
%
Similarly, the derivatives of $\log p$ with respect to $x$, $\nu$, and $\sigma$ are given in terms of the derivatives of $f$ evaluated at $(x', \nu') = (x / \sigma, \nu / \sigma)$:
%
\begin{align}
  \frac{\partial \log p}{\partial x}      = -\frac{1}{\sigma} \frac{\partial f}{\partial x'},                                                                                        \qquad
  \frac{\partial \log p}{\partial \nu}    = -\frac{1}{\sigma} \frac{\partial f}{\partial \nu'},                                                                                      \qquad
  \frac{\partial \log p}{\partial \sigma} = \frac{1}{\sigma} \left( x' \frac{\partial f}{\partial x'} + \nu' \frac{\partial f}{\partial \nu'} - 1 \right)
\end{align}
%
and likewise for higher-order derivatives.

Henceforth we take $\sigma=1$ and, by slight abuse of notation, write $x, \nu$ for the rescaled variables $x', \nu'$.
We will show that the key to numerically stable evaluation of $f$ and its derivatives is to carefully consider the limiting behavior in the regimes where $z = x\nu \ll 1$ and $z \gg 1 \Leftrightarrow u=1/z \ll 1$.
Naive evaluation of $f$ and its derivatives produces expressions with catastrophic cancellation in both regimes.
To evaluate these expressions in a numerically stable way, we derive algebraic simplifications in both the $z \ll 1$ and $z \gg 1$ regimes.

\section{The Rician log-likelihood and basic simplifications}

With $\sigma=1$, the Rician negative log-likelihood is given by
%
\begin{align}
  f(x, \nu) & \eqqcolon -\log p(x \mid \nu, \sigma=1)                                                                                                                        \\
            & = \frac{x^2 + \nu^2}{2} - \log x - \log I_0(z) \label{eq:rician-neg-log-likelihood}                                                                            \\
            & = \frac{(x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{x}{\nu}\right) - \log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi). \label{eq:rician-neg-log-likelihood-scaled}
\end{align}
%
Here, $\hat{I}_0$ is the modified Bessel function of the first kind normalized such that $\lim_{z\to\infty} \hat{I}_0(z) = 1$:
%
\begin{align}
  I_0(z) = \frac{e^z}{\sqrt{2\pi z}} \hat{I}_0(z) \quad \Leftrightarrow \quad \hat{I}_0(z) = \sqrt{2\pi z} e^{-z} I_0(z)
\end{align}
%
where $z = x\nu \ge 0$.
Next, we consider the numerical stability of these algebraically equivalent forms of $f$.

\paragraph{Small $z \ll 1$.}

For small $z$, we have that $\log I_0(z) = z^2/4 + \mathcal{O}(z^4)$, and thus \cref{eq:rician-neg-log-likelihood} becomes
%
\begin{align}
  f(x, \nu) = \frac{x^2 + \nu^2}{2} - \log x - \frac{z^2}{4} + \mathcal{O}(z^4).
\end{align}
%
The terms in this expression are generally not of the same order of magnitude, so there are no cancellation issues.
% This is slightly more numerically stable than using $\log \hat{I}_0(z) = \log I_0(z) - z + \frac{1}{2}\log(2\pi z) = \frac{1}{2}\log(2\pi z) - z + \mathcal{O}(z^2)$, which may suffer small cancellation issues when computing $-\frac{1}{2}\log(\frac{x}{\nu}) - \log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi) = -\log x + z + \mathcal{O}(z^2)$.
% For example, if $x \approx 1$ and $\nu \approx z$, then $\frac{1}{2}\log(\frac{x}{\nu}) \approx -\frac{1}{2}\log z$ and $-\log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi) \approx -\frac{1}{2}\log z$ but the result is $z \ll -\log z$.

\paragraph{Large $z \gg 1$.}

For large $z$, we have that $\log \hat{I}_0(z) = \frac{1}{8 z} + \mathcal{O}(z^{-2})$, and thus \cref{eq:rician-neg-log-likelihood-scaled} becomes
%
\begin{align}
  f(x, \nu) = \frac{(x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{x}{\nu}\right) + \frac{1}{2}\log(2\pi) - \frac{1}{8 z} + \mathcal{O}(z^{-2}).
\end{align}
%
The terms in this expression are generally not of the same order of magnitude -- unless $x \approx \nu$, which we address next -- so there are typically no cancellation issues.

\paragraph{High SNR $x \approx \nu \approx \sqrt{z} \gg 1$.}

A common case in practice is when the signal-to-noise ratio (SNR) is large:
observations are concentrated near the noncentrality parameter such that $x \approx \nu \pm 1$ (recall that the noise level is normalized to $\sigma=1$).
As was the case for large $z$, we should use the scaled form of \cref{eq:rician-neg-log-likelihood-scaled} but with a stable evaluation of $\log(x/\nu)$:
%
\begin{align}
  \log\left(\frac{x}{\nu}\right) = \begin{cases}
    \log\left(1 + \frac{x-\nu}{\nu}\right) & \text{if } x \ge \nu \\
    -\log\left(1 + \frac{\nu-x}{x}\right)  & \text{if } x < \nu
  \end{cases}
\end{align}
%
where $\log(1 + \epsilon) = \epsilon - \epsilon^2/2 + \mathcal{O}(\epsilon^3)$ is computed using the common special-function routine \texttt{log1p($\epsilon$)} which is designed to be accurate when $\epsilon$ is small.
With this modification, \cref{eq:rician-neg-log-likelihood-scaled} is accurate for all $z \gg 1$.

We note that \cref{eq:rician-neg-log-likelihood} is not stable in the high SNR regime, since $\log I_0(z) = \log \hat{I}_0(z) + z - \frac{1}{2}\log(2\pi z)$ has a large component $z$ which catastrophically cancels with $(x^2 + \nu^2) / 2 \approx z$.

\subsection{First derivatives}

Now, we differentiate $f$ with respect to $x$ and $\nu$ and simplify algebraically:
%
\begin{align}
  f_x   & = x-\nu -\frac{1}{2x} - \nu\frac{d}{dz}\log\hat{I}_0(z) \label{eq:first-derivatives-unsimplified-x}  \\
  f_\nu & = \nu-x +\frac{1}{2\nu} - x\frac{d}{dz}\log\hat{I}_0(z) \label{eq:first-derivatives-unsimplified-nu}
\end{align}
%
where
%
\begin{align}
  \frac{d}{dz}\log\hat{I}_0(z) & = r(z) - 1 + \frac{1}{2z} \label{eq:log-scaled-bessel-derivative} \\
  r(z)                         & = \frac{I_1(z)}{I_0(z)}. \label{eq:ratio-r}
\end{align}
%
Substituting \cref{eq:log-scaled-bessel-derivative} and $z=x\nu$ into \cref{eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu} gives
%
\begin{align}
  f_x   & = x - \nu r - \frac{1}{x} \label{eq:first-derivatives-simplified-x} \\
  f_\nu & = \nu - x r \label{eq:first-derivatives-simplified-nu}
\end{align}

\subsection{Second derivatives}

Next, we differentiate the simplified first derivatives in \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu} to obtain the second derivatives:
%
\begin{align}
  f_{xx}     & = \frac{\partial}{\partial x}\left(x - \nu r(z) - \frac{1}{x}\right) = 1 + \frac{1}{x^2} - \nu^2 r' \label{eq:second-derivatives-unsimplified-x-x} \\
  f_{x\nu}   & = \frac{\partial}{\partial \nu}\left(x - \nu r(z) - \frac{1}{x}\right) = -(r + z r') = -z(1 - r^2) \label{eq:second-derivatives-unsimplified-x-nu} \\
  f_{\nu\nu} & = \frac{\partial}{\partial \nu}\left(\nu - x r(z)\right) = 1 - x^2 r' \label{eq:second-derivatives-unsimplified-nu-nu}
\end{align}
%
where the last equality in \cref{eq:second-derivatives-unsimplified-x-nu} follows from the recurrence relation \cref{eq:r-prime-recurrence}.

\subsection{Third derivatives}

Differentiate once more to obtain third derivatives;
each depends only on $r'$ and $r''$:
%
\begin{align}
  f_{xxx}       & = \frac{\partial}{\partial x}\left(1+\frac{1}{x^2}-\nu^2 r'(z)\right) = -\frac{2}{x^3} - \nu^3 r'' \label{eq:third-derivatives-unsimplified-x-x-x}                                 \\
  f_{xx\nu}     & = \frac{\partial}{\partial \nu}\left(1+\frac{1}{x^2}-\nu^2 r'(z)\right) = -2\nu r' - x\nu^2 r'' = -\nu(2r' + z r'')              \label{eq:third-derivatives-unsimplified-x-x-nu}  \\
  f_{x\nu\nu}   & = \frac{\partial}{\partial x}\left(1 - x^2 r'(z)\right) = -2x r' - x^2\nu r'' = -x(2r' + z r'')                                  \label{eq:third-derivatives-unsimplified-x-nu-nu} \\
  f_{\nu\nu\nu} & = \frac{\partial}{\partial \nu}\left(1 - x^2 r'(z)\right) = -x^3 r''. \label{eq:third-derivatives-unsimplified-nu-nu-nu}
\end{align}
%
Therefore it suffices to compute $r$, $r'$, and $r''$ with high relative accuracy.

\subsection{Recurrence relations for derivatives of $r$}

To obtain $r$, $r'$, and $r''$, we differentiate $r=I_1/I_0$.
Using $I_0'=I_1$ and $I_1'=I_0 - I_1/z$, we have the recurrence relations
%
\begin{align}
  r'  & = \frac{I_1'}{I_0} - r\frac{I_0'}{I_0} = \frac{I_0 - \frac{1}{z} I_1}{I_0} - r^2 = 1 - \frac{r}{z} - r^2 \label{eq:r-prime-recurrence}                                           \\
  r'' & = -\left(\frac{r}{z}\right)' - 2 r r' = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r' = 2 r (r^2 - 1) + \frac{3 r^2 - 1}{z} + \frac{2 r}{z^2} \label{eq:r-second-derivative-recurrence}
\end{align}

These are exact identities but are numerically unstable as $z \to 0$ and as $z \to \infty$.
To address this, for each of $r$, $r'$, and $r''$, we split the domain $z>0$ into three regimes and approximate a single well-scaled quantity in each regime:
\begin{itemize}
  \item Small $z$: use a Taylor expansion, group terms by order in $z$, and fit a minimax polynomial to the residual.
  \item Large $z$: use the asymptotic expansion in $u = 1/z$, group terms by order in $u$, and fit a minimax polynomial to the residual.
  \item Intermediate $z$: cancellation is negligible; employ a rational minimax approximant.
\end{itemize}

\section{Small-$z$ analysis}

For small $z$, $r \approx z/2$.
Thus if we define
%
\begin{align}
  \boxed{
    \begin{aligned}
      a_0(z)           & = \frac{r(z)}{z} = \frac{1}{2} - \frac{1}{16}z^2 + \mathcal{O}(z^4) \\
      \Rightarrow r(z) & = z a_0(z)
    \end{aligned}
  }
\end{align}
%
then for computing $r$, we need only approximate $a_0$ to high precision.
Similarly, to compute $r'$, we have
%
\begin{align}\label{eq:r-prime-reparametrized}
  \boxed{r'(z) = 1 - \frac{r}{z} - r^2 = 1 - a_0 - z^2 a_0^2}
\end{align}
%
which is numerically stable since $a_0 = \frac{1}{2} + \mathcal{O}(z^2)$ and thus both the constant coefficient $1 - a_0$ and the quadratic coefficient $-a_0^2$ do not suffer from cancellation in floating-point arithmetic.

However, the second derivative $r''$ requires more care.
Observe that
%
\begin{align}
  r''(z) & = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r'                                             \\
         & = -\frac{1 - a_0 - z^2 a_0^2}{z} + \frac{z a_0}{z^2} -  2 z a_0(1 - a_0 - z^2 a_0^2) \\
         & = \frac{2 a_0 - 1}{z} + z a_0 (3 a_0 - 2) + 2 z^3 a_0^3.
\end{align}
%
The leading $\mathcal{O}(1/z)$ term has coefficient $2a_0 - 1$, but for small $z$, $a_0 \approx \frac{1}{2} - \frac{1}{16}z^2$ and thus computing $2a_0 - 1$ requires subtracting two $\mathcal{O}(1)$ terms to obtain a $\mathcal{O}(z^2)$ term.
We therefore reparametrize to
%
\begin{align}
  a_0                 & = \frac{1}{2} + z^2 a_1 \label{eq:a0-reparametrized}                                                          \\
  \Leftrightarrow a_1 & = \frac{1}{z^2} (a_0 - \frac{1}{2}) = \frac{1}{z^2} (\frac{r}{z} - \frac{1}{2}). \label{eq:a1-reparametrized}
\end{align}
%
Note that since $r = \frac{1}{2}z - \frac{1}{16}z^3 + \mathcal{O}(z^5)$, it follows that $a_1 = -\frac{1}{16} + \mathcal{O}(z^2)$.
We can then rewrite the second derivative into a numerically stable form using $(2 a_0 - 1) / z = 2 z a_1$:
%
\begin{align}
  \boxed{r''(z) = z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3} \label{eq:r-second-derivative-reparametrized}
\end{align}
%
We also note that the quantity $2r' + z r''$ from \cref{eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu} does not suffer cancellation issues, since $2r' = 1 + \mathcal{O}(z^2)$ and $z r'' = \mathcal{O}(z^2)$.

Thus, we need only fit one minimax polynomial to \cref{eq:a1-reparametrized} to estimate $a_1(z)$, and then $r$, $r'$, and $r''$ can be formed from $a_1$ and $a_0 = \frac{1}{2} + z^2 a_1$ without catastrophic cancellation using \cref{eq:r-prime-reparametrized,eq:r-second-derivative-reparametrized}.

% We can then rewrite the first and second derivatives of $r$ as
% %
% \begin{align}
%   r'(z) & = 1 - a_0 - z^2 a_0^2 \\
%   & = 1 - (\frac{1}{2} + z^2 a_1) - z^2 (\frac{1}{2} + z^2 a_1)^2 \\
%   &= \frac{1}{2} - z^2 (a_1 + \frac{1}{4}) - z^4 a_1 - z^6 a_1^2
% \end{align}
% %
% \begin{align}
%   r''(z) & = \frac{2 a_0 - 1}{z} + z a_0 (3 a_0 - 2) + 2 z^3 a_0^3 \\
%   &= \frac{2(\frac{1}{2} + z^2 a_1)-1}{z} - 2z(\frac{1}{2} + z^2 a_1) + 2z(\frac{1}{2} + z^2 a_1)^{2} + 2z^3 (\frac{1}{2} + z^2 a_1)^3 \\
%          & = z (2a_1 - \frac{1}{2}) + \frac{1}{4} z^3 + z^5 a_1 (\frac{3}{2} + 2 a_1) + z^7 (3 a_1^2) + z^9 (2 a_1^3)
% \end{align}

% Finally, we see from \cref{eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu} that we should double-check the quantity $2r' + zr''$ for cancellation issues:
% %
% \begin{align}
%   2r' + zr'' & = 2(1 - a_0 - z^2 a_0^2) + z(z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3) \\
%              & = 2 - 2a_0 - 2z^2 a_0^2 + z^2 (2a_1 + a_0 (3 a_0 - 2)) + 2 z^4 a_0^3 \\
%              & = 2 a_0^3 z^4 + a_0^2 z^2 - 2 a_0 z^2 - 2 a_0 + 2 a_1 z^2 + 2 \\
%              & = 2 - 2a_0 + z^2 (2a_1 - a_0 (2 - a_0)) + 2 a_0^3 z^4
% \end{align}

\section{Large-$z$ analysis}

As $z \to \infty$, $r \to 1$.
To avoid cancellation issues analogous to the small-$z$ case, we work with $u=1/z$ and rescale the residual $1-r$ to obtain a well-scaled quantity.
Let
%
\begin{align}
  \boxed{
    \begin{aligned}
      b_0(u)           & = \frac{1-r(z)}{u} = \frac{1}{2} + \frac{1}{8} u + \frac{1}{8} u^2 + \mathcal{O}(u^3) \\
      \Rightarrow r(z) & = 1 - u b_0(u) \label{eq:r-large-reparametrized}
    \end{aligned}
  }
\end{align}
%
Approximating $b_0$ with a minimax polynomial allows us to compute $r$ without catastrophic cancellation.

Next, we consider $r'$:
%
\begin{align}
  r'(z) & = 1 - \frac{r}{z} - r^2                                                     \\
        & = 1 - u(1 - u b_0) - (1 - u b_0)^2                                          \\
        & = u (2 b_0 - 1) + u^2 b_0 (1 - b_0) \label{eq:r-prime-large-reparametrized}
\end{align}
%
Analogous to the small-$z$ case, this will lead to cancellation issues since $2 b_0 - 1 = \mathcal{O}(u)$.
Thus, we reparametrize to
%
\begin{align}
  b_1(u)             & = \frac{2 b_0(u)-1}{u} = \frac{1}{4} + \frac{1}{4} u + \mathcal{O}(u^2) \\
  \Rightarrow b_0(u) & = \frac{1 + u b_1(u)}{2}
\end{align}
%
Then, $r'$ simplifies to
%
\begin{align}
  \boxed{
    \begin{aligned}
      r' = u(2 b_0-1) + u^2 b_0(1-b_0) = u^2 (b_1 + b_0(1-b_0)) \label{eq:r-prime-large-reparametrized-simplified}
    \end{aligned}
  }
\end{align}
%
which avoids cancellation issues since $b_1 \to \frac{1}{4}$ and $b_0 \to \frac{1}{2}$ as $u \to 0$, thus $r' = \frac{1}{2} u^2 + \mathcal{O}(u^3)$.

Next, we consider $r''$:
%
\begin{align}
  r'' & = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r'                                                                                                        \\
      & = -u(u^2 (b_1 + b_0(1-b_0))) + u^2 (1 - u b_0) - 2 (1 - u b_0) (u^2 (b_1 + b_0(1-b_0)))                                                         \\
      & = u^2 (2 b_0^2 - 2 b_1 - (2 b_0 - 1)) + u^3 (b_1 (2 b_0 - 1) - 2 b_0 + 3b_0^2 - 2 b_0^3)                                                        \\
      & = u^2 (2 b_0^2 - 2 b_1) + u^3 (b_0 (- 2 + 3b_0 - 2 b_0^2) - b_1) + u^4 b_1^2                                                                    \\
      & = u^2 (2 b_0^2 - 2 b_1) + u^3 (b_0 (2(2 b_0 - 1) - b_0 (1 + 2 b_0)) - b_1) + u^4 b_1^2                                                          \\
      & = u^2 (2 b_0^2 - 2 b_1) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0) \label{eq:r-second-derivative-large-reparametrized-unsimplified}
\end{align}
%
Now, since $b_0 = \frac{1}{2} + \mathcal{O}(u)$ and $b_1 = \frac{1}{4} + \mathcal{O}(u)$, we have $b_0^2 - b_1 = \mathcal{O}(u)$, and thus the leading term is order $\mathcal{O}(u^3)$, as expected since $r = 1 + \mathcal{O}(1/z)$.
To avoid cancellation issues, we introduce one final change of variables:
%
\begin{align}
  b_2(u)             & = \frac{4 b_1(u) - 1}{u} = 1 + \mathcal{O}(u) \label{eq:b2-reparametrized} \\
  \Rightarrow b_1(u) & = \frac{1 + u b_2(u)}{4} \label{eq:b1-reparametrized}
\end{align}
%
Then, we have
%
\begin{align}
  2 b_0^2 - 2 b_1 & = 2(\frac{1 + u b_1}{2})^2 - 2 b_1 = \frac{1}{2} (1 - 4 b_1 + 2 u b_1 + u^2 b_1^2) \\
                  & = \frac{1}{2} u (-b_2 + 2 b_1) + \frac{1}{2} u^2 b_1^2
\end{align}
%
which we substitute into \cref{eq:r-second-derivative-large-reparametrized-unsimplified} and simplify to obtain
%
\begin{align}
  r'' & = u^2 (2 b_0^2 - 2 b_1) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0)                          \\
      & = \frac{1}{2} u^2 (u (-b_2 + 2 b_1) + u^2 b_1^2) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0) \\
      & = -u^3 (\frac{1}{2}b_2 + b_0^2 (1 + 2 b_0)) + u^4 b_1 (\frac{3}{2} b_1 + 2 b_0)
\end{align}
%
and finally
%
\begin{align}\label{eq:r-second-derivative-large-reparametrized}
  \boxed{r'' = -\frac{1}{2} u^3 (b_2 + 2 b_0^2 (1 + 2 b_0) - u b_1 (3 b_1 + 4 b_0))}
\end{align}
%
Thus, we need only fit one minimax polynomial to \cref{eq:b2-reparametrized} to estimate $b_2(u)$, and then $r$, $r'$, and $r''$ can be formed from $b_2$, $b_1 = \frac{1 + u b_2}{4}$, and $b_0 = \frac{1 + u b_1}{2}$ using \cref{eq:r-prime-large-reparametrized,eq:r-second-derivative-large-reparametrized} without catastrophic cancellation.

% \section{Intermediate-$z$ analysis}
%
% TODO?

\section{Further Rician derivative simplifications}

\subsection{First derivatives}\label{sec:rician-first-derivatives}

\paragraph{Small-$z$}

The algebraic simplification of \cref{eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu} leading to \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu} is sufficient to avoid catastrophic cancellation.
In particular, when $\nu \ll 1$, the problematic $\frac{1}{2\nu}$ and $\frac{-x}{2z} = \frac{-1}{2\nu}$ terms in \cref{eq:first-derivatives-unsimplified-nu} are cancelled analytically, avoiding subtraction of two $\mathcal{O}(1/\nu)$ terms.
%
\begin{align}\label{eq:first-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_x   & = x - \nu r - \frac{1}{x} \\
      f_\nu & = \nu - x r
    \end{aligned}
  }
\end{align}

\paragraph{Large-$z$}

For $z \gg 1$, we have $r = 1 - b_0(1/z) / z$ from \cref{eq:r-large-reparametrized}, resulting in
%
\begin{align}
  f_x   & = x - \nu (1 - \frac{b_0}{z}) - \frac{1}{x}
  = x - \nu - \frac{1 - b_0}{x}                       \\
  f_\nu & = \nu - x (1 - \frac{b_0}{z})
  = \nu - x + \frac{b_0}{\nu}
\end{align}
%
These forms of $f_x$ and $f_\nu$ are more numerically stable when $x \approx \nu \approx \sqrt{z}$.
The improved numerical stability results from the approximation error being scaled by $1/x$ and $1/\nu$ instead of by $\nu$ and $x$.
If $\hat{r}=r(1+\delta_r)$ and $\hat{b}_0=b_0(1+\delta_b)$, the naive forms incur absolute errors $\mathcal{O}(\nu|\delta_r|)$ in $f_x$ and $\mathcal{O}(x|\delta_r|)$ in $f_\nu$, whereas the simplified forms incur errors $\mathcal{O}(|\delta_b|/x)$ and $\mathcal{O}(|\delta_b|/\nu)$.
Since $z = x\nu \gg 1$ and the relative errors $|\delta_r|$ and $|\delta_b|$ are comparably small, this yields an absolute-error reduction approximately by a factor of $z$.
%
%
\begin{align}\label{eq:first-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_x   & = x - \nu - \frac{1 - b_0}{x} \\
      f_\nu & = \nu - x + \frac{b_0}{\nu}
    \end{aligned}
  }
\end{align}

\subsection{Second derivatives}\label{sec:rician-second-derivatives}

\paragraph{Small-$z$}

Similar to the first derivatives, the algebraically simplified expressions in \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu} are sufficient to avoid catastrophic cancellation.
%
%
\begin{align}\label{eq:second-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_{xx}     & = 1 + \frac{1}{x^2} - \nu^2 r' \\
      f_{x\nu}   & = -z (1 - r^2) = -(r+z r')     \\
      f_{\nu\nu} & = 1 - x^2 r'
    \end{aligned}
  }
\end{align}
%
Here $r= z a_0$ and $r' = 1-a_0 - z^2 a_0^2$, with $1-a_0=\frac{1}{2}+\mathcal{O}(z^2)$ and $z^2 a_0^2=\mathcal{O}(z^2)$, so no catastrophic cancellation occurs.
The cross term may use either $-z(1-r^2)$ or $-(r+z r')$:
since $r \approx z/2$ and $zr' \approx z/2$, the former subtracts terms of different magnitudes, and the latter adds terms of the same magnitude but whose leading terms do not suffer cancellation issues.

\paragraph{Large-$z$}

We start by rewriting the second derivatives expressions in \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu} for stability at large-$z$:
%
\begin{align}
  f_{xx}     & = 1 + \frac{1}{x^2} - \nu^2 r' = 1 + \frac{1}{x^2}(1 - z^2 r') \\
  f_{x\nu}   & = -z(1 - r^2) = -z (1 - r) (1 + r)                             \\
  f_{\nu\nu} & = 1 - x^2 r'
\end{align}
%
Recall that for large-$z$, we have from \cref{eq:r-large-reparametrized,eq:r-prime-large-reparametrized-simplified} that
\begin{align}
  r                      & = 1 - \frac{b_0}{z} = 1 - \frac{1}{2z} - \frac{1}{8z^2} + \mathcal{O}(1/z^3)                \\
  r'                     & = \frac{1}{z^2} (b_1 + b_0(1 - b_0)) = \frac{1}{2z^2} + \frac{1}{4z^3} + \mathcal{O}(1/z^4) \\
  \Rightarrow z(1 - r^2) & = r + z r' = 1 + \frac{1}{z} (b_1 - b_0^2) = 1 + \frac{1}{8z^2} + \mathcal{O}(1/z^3)
\end{align}
where $b_0 = \frac{1}{2} (1 + u b_1)$.
%
We make the following observations:
%
\begin{itemize}
  \item $f_{xx}$: since $z^2 r' \approx \frac{1}{2}$, $1 - z^2 r'$ has no cancellation issues and $f_{xx} \approx 1 + \frac{1}{2x^2}$.
  \item $f_{\nu\nu}$: we have that $x^2 r' \approx \frac{x^2}{2z^2} = \frac{1}{2\nu^2}$; this also has no cancellation issues, and $f_{\nu\nu} \approx 1 - \frac{1}{2\nu^2}$.
  \item $f_{x\nu}$: $1 + r$ is stable to compute, but we must rewrite $z(1 - r) = b_0$ to avoid cancellation, so $f_{x\nu} = -b_0(1 + r) \approx -1 - \frac{1}{8z^2}$.
\end{itemize}
%
and therefore the stable forms are:
%
\begin{align}\label{eq:second-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_{xx}     & = 1 + \frac{1}{x^2}(1 - z^2 r') \\
      f_{x\nu}   & = -b_0 (1 + r)                  \\
      f_{\nu\nu} & = 1 - x^2 r'
    \end{aligned}
  }
\end{align}

% \subsubsection{Error analysis}
%
% Following the large-$z$ analysis for the first derivatives, we substitute $r = 1 - \frac{b_0}{z}$ from \cref{eq:r-large-reparametrized} and $r' = \frac{1}{z^2} (b_1 + b_0(1 - b_0))$ from \cref{eq:r-prime-large-reparametrized-simplified} into the second derivatives expressions \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu}:
% %
% \begin{align}\label{eq:second-derivatives-large-z}
%   \boxed{
%     \begin{aligned}
%       f_{xx}     & = 1 + \frac{1 - (b_1 + b_0(1-b_0))}{x^2} \\
%       f_{x\nu}   & = -b_0 (2 - \frac{b_0}{z})               \\
%       f_{\nu\nu} & = 1 - \frac{b_1 + b_0(1-b_0)}{\nu^2}
%     \end{aligned}
%   }
% \end{align}
% %
% To analyze the error propagation, let $\hat{r}=r(1+\delta_r)$, $\hat{r}' = r'(1+\delta_{r'})$, $\hat{b}_0=b_0(1+\delta_{b_0})$, and $\hat{b}_1=b_1(1+\delta_{b_1})$ with comparable small relative errors.
% For the naive forms, we have
% %
% \begin{align}
%   |\hat{f}_{xx} - f_{xx}|         & = \nu^2 |\hat{r}' - r'| = \nu^2 |r'| |\delta_{r'}| \sim \nu^2 \frac{1}{z^2} |\delta_{r'}| = \frac{|\delta_{r'}|}{x^2}      \\
%   |\hat{f}_{\nu\nu} - f_{\nu\nu}| & = x^2 |\hat{r}' - r'| = x^2 |r'| |\delta_{r'}| \sim x^2 \frac{1}{z^2} |\delta_{r'}| = \frac{|\delta_{r'}|}{\nu^2}          \\
%   |\hat{f}_{x\nu} - f_{x\nu}|     & = |\hat{r} + z\hat{r}' - (r + zr')| \le |r| |\delta_r| + z |r'| |\delta_{r'}| \sim |\delta_r| + \frac{1}{z} |\delta_{r'}|.
% \end{align}
% %
% Next, the stable large-$z$ forms:
% Let $c = b_1 + b_0(1 - b_0)$ so that $f_{xx} = 1 + (1-c)/x^2$ and $f_{\nu\nu} = 1 - c/\nu^2$.
% Then, we have that:
% %
% \begin{align}
%   \hat{c} - c & = (\hat{b}_1 - b_1) + (\hat{b}_0 - b_0) - (\hat{b}_0^2 - b_0^2)             \\
%               & = b_1 \delta_{b_1} + b_0 \delta_{b_0} - b_0 \delta_{b_0} (\hat{b}_0 + b_0).
% \end{align}
% %
% This yields the first-order error bounds:
% %
% \begin{align}
%   |\hat{f}_{xx} - f_{xx}|         & = | \frac{c - \hat{c}}{x^2} | \lesssim \frac{|b_1| |\delta_{b_1}| + |b_0| (1 + |\hat{b}_0 + b_0|) |\delta_{b_0}|}{x^2}     \\
%   |\hat{f}_{\nu\nu} - f_{\nu\nu}| & = | \frac{c - \hat{c}}{\nu^2} | \lesssim \frac{|b_1| |\delta_{b_1}| + |b_0| (1 + |\hat{b}_0 + b_0|) |\delta_{b_0}|}{\nu^2} \\
%   |\hat{f}_{x\nu} - f_{x\nu}|     & = | 2 (b_0 - \hat{b}_0) + \frac{\hat{b}_0^2 - b_0^2}{z} | \le 2 (|b_0| + \frac{|b_0||\hat{b}_0 + b_0|}{z}) |\delta_{b_0}|.
% \end{align}

\subsection{Third derivatives}\label{sec:rician-third-derivatives}

\paragraph{Small-$z$}

We begin by noting that the third derivative expressions in \cref{eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-nu-nu-nu} depend only on $r'$ and $r''$.
Recall the small-$z$ parametrizations for $r'$ and $r''$ from \cref{eq:r-prime-reparametrized,eq:r-second-derivative-reparametrized}:
%
\begin{align}
  r'                      & = 1 - a_0 - z^2 a_0^2 = \frac{1}{2} - \frac{3}{16}z^2 + \mathcal{O}(z^4)                          \\
  r''                     & = z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3 = -\frac{3}{8}z + \mathcal{O}(z^3)                     \\
  \Rightarrow 2r' + z r'' & = 2 (1 - a_0) + (a_0 (a_0 - 2) + 2 a_1) z^2 + 2 a_0^3 z^4 = 1 - \frac{3}{4}z^2 + \mathcal{O}(z^3)
\end{align}
%
where $a_0 = \frac{r}{z}$ and $a_1 = \frac{1}{z^2} (a_0 - \frac{1}{2})$.
We see that for small-$z$, the original simplified expressions in \cref{eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-nu-nu-nu}
%
\begin{align}
  \boxed{
    \begin{aligned}
      f_{xxx}       & = -\frac{2}{x^3} - \nu^3 r'' \\
      f_{xx\nu}     & = -\nu(2r' + z r'')          \\
      f_{x\nu\nu}   & = -x(2r' + z r'')            \\
      f_{\nu\nu\nu} & = -x^3 r''
    \end{aligned}
  }
\end{align}
%
are stable to compute, since neither $r''$ nor $2r' + z r''$ suffer cancellation issues.

\paragraph{Large-$z$}

For large $z$, the expression for $r''$ in \cref{eq:r-second-derivative-large-reparametrized} is stable.
The term $2r'+zr''$ is more delicate.
We derive a stable form from the identity $2r' + zr'' = r' + r/z - 2zrr'$ which follows from \cref{eq:r-prime-large-reparametrized-simplified,eq:r-second-derivative-large-reparametrized}.
Substituting $r=1-ub_0$ and $r'=u^2(b_1+b_0(1-b_0))$ gives
%
\begin{align}
  2r' + zr'' & = r' + r/z - 2zrr'                                              \\
             & = u^2 (b_1+b_0(1-b_0)) + u(1-ub_0) - 2u(1-ub_0)(b_1+b_0(1-b_0)) \\
             & = u ( u(b_1+b_0-b_0^2) + (1-ub_0)(1 - 2(b_1+b_0-b_0^2)) )
\end{align}
%
The term $1-2(b_1+b_0-b_0^2) = -\frac{1}{2}u + \mathcal{O}(u^2)$ is prone to cancellation issues which can be resolved by substituting $b_0=(1+ub_1)/2$ followed by $b_1=(1+ub_2)/4$:
%
\begin{align}
  1-2(b_1+b_0-b_0^2) = \frac{1-4b_1+u^2 b_1^2}{2} = -\frac{1}{2}u(b_2 - u b_1^2)
\end{align}
%
Substituting this back yields
%
\begin{align}
  2r' + zr'' & = u^2 \left( (b_1+b_0-b_0^2) + (1-ub_0)\frac{u b_1^2 - b_2}{2} \right)                                          \\
             & = \frac{1}{2} u^2 \left( (1-b_2) + u (b_2 (\frac{1}{2}+b_0 ) + b_1^2 ) - u^2 b_1^2 (\frac{1}{2} + b_0 ) \right)
\end{align}
%
but now, since $b_2 = 1 + \mathcal{O}(u)$, the term $1-b_2$ causes cancellation issues and so we must define one final change of variables $b_2 = 1 + u b_3$ to handle the cancellation.
Substituting this gives the stable form:
%
\begin{align}
  2r' + zr'' & = \frac{1}{2} u^2 \left( -u b_3 + u (b_2 (\frac{1}{2}+b_0 ) + b_1^2 \right) - u^2 b_1^2 ( \frac{1}{2} + b_0 )                                \\
             & = \frac{1}{2} u^3 \left( b_2 (\frac{1}{2}+b_0 ) + b_1^2 - b_3 - u b_1^2 (\frac{1}{2} + b_0) \right) \label{eq:2rprime-zrsecondprime-large-z}
\end{align}
%
This expression is $\mathcal{O}(u^3)$ with no remaining cancellation issues.
All terms are computed from $b_0, b_1, b_2$, and $b_3$, which depend on a single minimax approximation for $b_3(u)$.

With these stable forms, the third derivatives for large $z$ are computed as:
\begin{align}\label{eq:third-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_{xxx}       & = -\frac{2}{x^3} - \nu^3 r''                                                                       \\
      f_{xx\nu}     & = -\frac{1}{2 x z^2}   \left( b_1^2 - b_3 + (b_2  - \frac{1}{z} b_1^2) (\frac{1}{2} + b_0) \right) \\
      f_{x\nu\nu}   & = -\frac{1}{2 \nu z^2} \left( b_1^2 - b_3 + (b_2  - \frac{1}{z} b_1^2) (\frac{1}{2} + b_0) \right) \\
      f_{\nu\nu\nu} & = -x^3 r''
    \end{aligned}
  }
\end{align}

\section{The Quantized Rician distribution}

In magnetic resonance imaging (MRI), the magnitude of the signal in the presence of noise is often modeled with a Gaussian distribution for simplicity.
However, a more physically accurate model is the Rician distribution, which correctly accounts for the non-negativity of magnitude data.
Furthermore, MRI data is invariably quantized during acquisition, meaning the continuous signal is stored as discrete integer values.
A truly faithful statistical model must therefore account for this quantization step.
This leads us to the Quantized Rician distribution, which describes the probability of observing a signal within a specific quantization bin.

Given the Rician probability density function $p(y \mid \nu, \sigma)$ from \cref{eq:rician-pdf}, the probability of an observation $y$ falling into a bin $[x, x+\delta]$ is given by the integral of the Rician density over the interval:
%
\begin{align}\label{eq:quantized-rician-pdf}
  p(x \mid \nu, \sigma, \delta) = \int_x^{x+\delta} p(y \mid \nu, \sigma) dy.
\end{align}
%
This defines the probability mass function for the Quantized Rician distribution, which we denote $\mathrm{QRice}(\nu, \sigma, \delta)$.

\subsection{Numerical evaluation strategy}

We are interested in performing statistical inference procedures such as maximum likelihood estimation (MLE), maximum a posteriori (MAP) estimation, and Markov-chain Monte Carlo (MCMC) sampling under a Quantized Rician likelihood model.
These methods require many evaluations of the likelihood and its gradient, necessitating fast and accurate evaluation of the defining integral in \cref{eq:quantized-rician-pdf}.
While adaptive quadrature routines can compute the integral to arbitrary precision, they are generally too slow for this context, especially for GPU-based implementations.
Our goal is to use a fixed-order, non-adaptive quadrature scheme which is significantly faster.

The suitability of such a scheme depends on the integration regime.
Integrating an exponentially decaying function over a large interval in its tails, for example, can be numerically catastrophic for a fixed-order rule.
Fortunately, the nature of statistical inference itself ensures that we predominantly operate in a numerically favourable regime when the noise scale $\sigma$ is a nuisance parameter which is to be estimated jointly with other model parameters.
Crucially, the inference algorithms operate in high-likelihood regions:
%
\begin{itemize}
  \item MLE and MAP explicitly drive the parameters $(\nu,\sigma)$ toward the mode of the likelihood or posterior.
  \item MCMC methods spend the most time sampling from regions of high posterior density, visiting the tails only rarely.
\end{itemize}
%
This behavior is made concrete by considering the high signal-to-noise ratio (SNR) regime, where the Rician distribution is well-approximated by a Gaussian.
For a Gaussian model, a property of the MLE is that the noise variance estimate equals the mean squared error:
%
\begin{align}
  \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \nu_i)^2.
\end{align}
%
This implies that for a well-fit model the residuals satisfy $|x_i - \nu_i| = \mathcal{O}(\sigma)$, i.e., the normalized residual is on the order of one, $|x' - \nu'| = \mathcal{O}(1)$.
Thus, inference naturally results in evaluation of the density near the mode.
Pathological cases where $|x'-\nu'| \gg 1$ correspond to regions of low likelihood that are comparatively less important;
coarse estimates suffice there.

This operating regime is further constrained by practical considerations from signal acquisition.
The signal-to-quantization-noise ratio (SQNR) is engineered to exceed the physical SNR, ensuring quantization error is a smaller source of variance than the physical noise of the signal.
Empirically, the ratio of maximum signal magnitude to quantization bin width is large ($\sim\!500$--$1000$), and with 16-bit signal magnitude quantization (Int16) the dynamic range allows a theoretical upper bound of about $2^{15} \approx 30000$.
Consequently, the signal-to-bin-width ratio typically exceeds the SNR, i.e. $x/\delta \gtrsim x/\sigma$, which implies that the normalized integration width satisfies $\delta' = \delta/\sigma \lesssim 1$.
Thus, we have reasonable justification for the following two assumptions about the model parameters:
%
\begin{enumerate}
  \item The observed magnitude $x$ is close to the noncentrality parameter $\nu$, i.e., $|x - \nu| \lesssim \sigma \Rightarrow |x' - \nu'| \lesssim 1$.
  \item The quantization width $\delta$ is not larger than the noise level $\sigma$, i.e. $\delta' = \delta / \sigma \lesssim 1$.
\end{enumerate}
%
Combined, these factors create a best-case scenario for numerical integration:
we are integrating a smooth, well-behaved density over a short interval located near its mode.
This justifies the use of a fixed-order quadrature rule.
As a safeguard, on the rare occasions where a poor parameter choice leads to evaluating the likelihood in the tails (i.e., $|x'-\nu'| \gg 1$), one could temporarily increase the quadrature order or fall back to an adaptive routine;
under normal inference, these cases are exceptional.

\subsection{Gauss-Legendre quadrature}

We choose Gauss-Legendre quadrature to approximate the likelihood integral.
On the canonical interval $[-1,1]$, the $N$‑point Gauss–Legendre rule approximates
%
\begin{align}
  \int_{-1}^{1} g(x) \, dx \approx \sum_{i=1}^{N} w_i g(x_i),
\end{align}
%
where $\{x_i\}_{i=1}^N$ are the Legendre nodes, defined as the roots of the $N^\text{th}$ Legendre polynomial $P_N(x)$, and $\{w_i\}_{i=1}^N$ the associated positive weights.
This rule is extremely powerful despite its simplicity:
it is exact for all polynomials of degree up to $2N-1$, and for analytic functions the approximation error decreases exponentially fast as $N$ increases.
The rule's fixed nodes and weights are easy to precompute, and the positivity of the weights ensures that no additional cancellation issues are introduced.
Furthermore, the weighted sum structure is trivial to differentiate, making it straightforward to compute gradients and Hessians of the integral approximation.

To apply the rule on $[0,1]$, use the affine change of variables $t = (x+1)/2$, transforming the nodes and weights to $t_i = (x_i+1)/2$ and $w_i/2$, respectively:
%
\begin{align}
  \int_{0}^{1} g(t) \, dt \approx \sum_{i=1}^{N} \frac{w_i}{2} g(t_i).
\end{align}
%
To integrate over $[x, x+\delta]$, change variables to $y = x + \delta t$ with $t\in[0,1]$ and $dy = \delta dt$:
%
\begin{align}
  \int_{x}^{x+\delta} g(y) \, dy = \int_{0}^{1} g(x + \delta t) \, \delta dt
  \approx \delta \sum_{i=1}^{N} \frac{w_i}{2} g(x + \delta t_i).
\end{align}
%
In our regime, $\delta$ is small and $g$ is smooth, so moderate orders $N \approx 8\!-\!16$ achieve high accuracy.

\subsection{Log-likelihood formulation and simplification}

Before deriving expressions for the gradient and Hessian, we first simplify the problem by exploiting the scale-invariance of the Quantized Rician distribution.
If we change variables in the integrand to $y' = y / a$ for some $a > 0$ and similarly rescale $x$, $\nu$, $\sigma$, and $\delta$, the Rician pdf transforms as
%
\begin{align}
  p(a y' \mid a \nu', a \sigma')
  = \frac{a y'}{(a \sigma')^2} \exp\left(-\frac{(a y')^2+(a \nu')^2}{2(a \sigma')^2}\right) I_0\left(\frac{a y' (a \nu')}{(a \sigma')^2}\right)
  = \frac{1}{a} p(y' \mid \nu', \sigma').
\end{align}
%
Thus, the quantized likelihood integral is scale-invariant:
%
\begin{align}
  p(x \mid \nu, \sigma, \delta)
  = \int_{x/a}^{(x+\delta)/a} p(a y' \mid a\nu', a\sigma') (a dy') = \int_{x'}^{x'+\delta'} p(y' \mid \nu', \sigma') dy'
  = p(x' \mid \nu', \sigma', \delta').
\end{align}
%
Similar to the Rician distribution analysis, we choose $a = \sigma$ such that $\sigma'=1$;
for the remainder of this analysis, we will work with these normalized parameters and write $p(x \mid \nu, \delta)$ as shorthand for $p(x \mid \nu, \sigma=1, \delta)$, dropping the prime annotations.

In terms of $f$, the negative log-likelihood of the standard Rician distribution from \cref{eq:rician-neg-log-likelihood}, the Quantized Rician likelihood is given by
%
\begin{align}
  p(x \mid \nu, \delta) = \int_x^{x+\delta} \exp(-f(y, \nu)) dy \coloneqq I(\theta)
\end{align}
%
where we have introduced a shorthand notation for the normalized parameters $\theta = (x, \nu, \delta)$ and the integral $I(\theta)$.
Similarly, we introduce $\Omega(\theta) = -\log I(\theta)$ for the negative logarithm of the quantized likelihood:
%
\begin{align}
  -\log p(x \mid \nu, \delta) = -\log I(\theta) \coloneqq \Omega(\theta).
\end{align}
%
We now make the affine change variables $y = x + \delta t$ such that the integration bounds are independent of $\theta$, resulting in an integral over the unit interval $t \in [0, 1]$:
%
\begin{align}
  I(\theta)                  & = \int_0^1 \exp(-f(x + \delta t, \nu)) \cdot \delta \, dt                    \\
  \Rightarrow \Omega(\theta) & = -\log\left(\int_0^1 \exp(-f(x + \delta t, \nu)) \, dt\right) - \log\delta.
\end{align}
%
This expression has the form of a log-partition function;
denote the energy function as $\tilde{f}(t, \theta) = f(x + \delta t, \nu)$ and the partition function as
%
\begin{align}
  Z(\theta) = \int_0^1 \exp(-\tilde{f}(t, \theta)) \, dt.
\end{align}
%
Then, $\Omega(\theta) = -\log Z(\theta) - \log\delta$, and we can define a probability density over $t \in [0,1]$ as
%
\begin{align}
  P(t \mid \theta) = \frac{\exp(-\tilde{f}(t, \theta))}{Z(\theta)}.
\end{align}
%
This formulation will prove useful for computing gradients and higher derivatives of $\Omega$.

\subsection{The residual likelihood method}

Since the quadrature scheme approximates the partition function $Z(\theta)$ as a weighted sum of positive terms, $\log Z(\theta)$ can be stably computed using the log-sum-exp trick:
%
\begin{align}
  -\log Z(\theta)
  = -\log\left(\sum_i \frac{w_i}{2} \exp(-\tilde{f}(t_i, \theta))\right)
  = \tilde{f}_{\min}(\theta) - \log\left(\sum_i \frac{w_i}{2} \exp(\tilde{f}_{\min}(\theta) - \tilde{f}(t_i, \theta))\right)
\end{align}
%
where $\tilde{f}_{\min}(\theta) = \min_i \tilde{f}(t_i, \theta)$, avoiding naive overflow.
However, a more subtle numerical issue arises at high SNR ($x \approx \nu$).
In this regime, the Rician negative log-likelihood $f$ is very nearly Gaussian:
%
\begin{align}
  f(x, \nu) \approx f_G(x,\nu) = \frac{(x-\nu)^2}{2} + \frac{1}{2}\log(2\pi).
\end{align}
%
In other words, the residual energy $\tilde{r}(t,\theta) = \tilde{f}(t,\theta) - f_G(x,\nu) = f(x + \delta t, \nu) - f_G(x,\nu)$ is relatively small.
Since both integration and Gauss-Legendre quadrature are linear functionals, we can factor $\exp(-f_G(x,\nu))$ out of the $Z(\theta)$ integral, resulting in a better-conditioned quadrature problem via the residual energy $\tilde{r}$:
%
\begin{align}
  \log Z(\theta)
  = - \log\left(\int_0^1 \exp(-(\tilde{r}(t,\theta) + f_G(x,\nu))) \, dt \right)
  = f_G(x,\nu) - \log\left(\int_0^1 \exp(-\tilde{r}(t,\theta)) \, dt \right).
\end{align}
%
This transformation is most beneficial at large $z$ but -- as we will see -- introduces no numerical issues at small $z$, so we apply it universally for simplicity.

\subsubsection{Properties of the residual energy}

Since $f_G$ is quadratic in $x$ and $\nu$ and independent of $\delta$, it follows from $\tilde{r} = \tilde{f} - f_G$ that third and higher-order partial derivatives of $\tilde{r}$ with respect to $x$ and $\nu$ are equal to those of $\tilde{f}$.
Similarly, any partial derivative of $\tilde{r}$ with respect to $\delta$ equals that of $\tilde{f}$.
We therefore need only derive stable expressions for $\tilde{r}$ and its first and second partial derivatives with respect to $x$ and $\nu$.
From \cref{eq:rician-neg-log-likelihood-scaled}, we have:
%
\begin{align}
  \tilde{r}(t,\theta) = f(y, \nu) - f_G(x,\nu) & = \frac{(y-\nu)^2 - (x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{y}{\nu}\right) - \log \hat{I}_0(y \nu)                                               \\
                                               & = \delta t \left(x - \nu + \frac{\delta t}{2}\right) - \frac{1}{2}\log\left(\frac{y}{\nu}\right) - \log \hat{I}_0(y \nu) \label{eq:residual-energy}
\end{align}
%
where $y = x + \delta t$.
The first term in \cref{eq:residual-energy} is small when $|x-\nu| \lesssim 1$ and $\delta \lesssim 1$.
The size of the second and third terms $\tilde{c}(y,\nu) = \frac{1}{2}\log(y/\nu) + \log\hat{I}_0(y\nu)$ depends on the regime of $z=y\nu$.

\paragraph{Small $z=y\nu$}

Rewriting $\tilde{c}(y,\nu) = \log y + \frac{1}{2}\log(2\pi) + \log I_0(z) - z$ and Taylor expanding $\log I_0(z)$, we have
%
\begin{align}\label{eq:c-small-z}
  \tilde{c}(y,\nu) = \log y + \frac{1}{2}\log(2\pi) - z + \frac{z^2}{4} + \mathcal{O}(z^4).
\end{align}

\paragraph{Large $z=y\nu$}

Using the asymptotic expansion $\log \hat{I}_0(z) = \frac{1}{8z} + \mathcal{O}(z^{-2})$, we have
%
\begin{align}\label{eq:c-large-z}
  \tilde{c}(y,\nu) = \frac{1}{2}\log\left(\frac{y}{\nu}\right) + \frac{1}{8z} + \mathcal{O}(z^{-2}).
\end{align}

\paragraph{High-SNR $\nu \approx y \gg 1$}

Let $y = x + \delta t = \nu + \epsilon$ with $|\epsilon| = |x - \nu + \delta t| \lesssim 1$ and $\nu$ large.
Then, expanding \cref{eq:c-large-z} to first order in $\epsilon/\nu \ll 1$ gives:
%
\begin{align}
  \tilde{c}(\nu+\epsilon, \nu) & = \frac{1}{2}\log\left(1 + \frac{\epsilon}{\nu}\right) + \frac{1}{8\nu(\nu+\epsilon)} + \mathcal{O}(z^{-2})                                                                                                   \\
                               & = \frac{\epsilon}{2\nu} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right) + \frac{1}{8\nu^2}\left(1 - \frac{\epsilon}{\nu} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right)\right) + \mathcal{O}(\nu^{-4}) \\
                               & = \frac{\epsilon}{2\nu}\left(1 - \frac{1}{4\nu^2}\right) + \frac{1}{8\nu^2} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right).
\end{align}
%
Thus, $\tilde{c}$ is small in the high-SNR regime and therefore $\tilde{r} = \delta t (\epsilon - \frac{\delta t}{2}) - \tilde{c}$ is also small when $\delta \lesssim 1$.

\subsubsection{First derivatives of the residual}\label{sec:first-derivatives-residual}

The first partial derivatives of $\tilde{r}$ can be simplified using \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu}:
%
\begin{align}
  \tilde{r}_x   & = f_x(y,\nu) - (x-\nu) = (y - \nu r - \frac{1}{y}) - (x-\nu) = \delta t  + \nu (1 - r) - \frac{1}{y} \\
  \tilde{r}_\nu & = f_\nu(y,\nu) - (\nu-x) = (\nu - y r) - (\nu-x) = -\delta t r + x (1 - r)
\end{align}
%
These forms are stable for small $z = y \nu$, but for large $z$, we substitute $1-r = \frac{b_0}{z}$ from \cref{eq:r-large-reparametrized}:
%
\begin{align}
  \tilde{r}_x   & = \delta t  + \nu (1 - r) - \frac{1}{y} = \delta t - \frac{1 - b_0}{y}          \\
  \tilde{r}_\nu & = -\delta t r + x (1 - r) = -\delta t + y (1 - r) = -\delta t + \frac{b_0}{\nu}
\end{align}

\subsubsection{Second derivatives of the residual}\label{sec:second-derivatives-residual}

The second partial derivatives of the $\tilde{r}$ can be simplified using \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu}:
%
\begin{align}
  \tilde{r}_{xx}     & = f_{xx} - 1     = \frac{1}{y^2} - \nu^2 r'        \\
  \tilde{r}_{x\nu}   & = f_{x\nu} + 1   = 1 - (r + z r') = 1 - z(1 - r^2) \\
  \tilde{r}_{\nu\nu} & = f_{\nu\nu} - 1 = - y^2 r'
\end{align}
%
Similar to the small- and large $z$ expressions in \cref{eq:second-derivatives-small-z,eq:second-derivatives-large-z}, the above expressions for $\tilde{r}_{xx}$ and $\tilde{r}_{\nu\nu}$ are numerically stable.
However, for large $z$, the mixed-derivative $\tilde{r}_{x\nu}$ requires some care, since $f_{x\nu} = -1 + \mathcal{O}(\frac{1}{z^2})$ and so computing $\tilde{r}_{x\nu} = \mathcal{O}(\frac{1}{z^2})$ naively leads to catastrophic cancellation.
A stable form can be derived by substituting the large-$z$ reparametrizations $1 - r = u b_0$, $2 b_0 - 1 = u b_1$, and $4 b_1 - 1 = u b_2$ where $u = \frac{1}{z}$:
%
\begin{align}
  \tilde{r}_{x\nu} & = 1 - z(1 - r)(1 + r) = 1 - b_0 (1 + r) = 1 - b_0(2 - u b_0)                       \\
                   & = (1 - 2 b_0) + u b_0^2 = -u b_1 + u b_0^2 = u (b_0^2 - b_1)                       \\
                   & = u ((\frac{1 + u b_1}{2})^2 - b_1) = \frac{u}{4} (1 - 4b_1 + 2u b_1 + u^2 b_1^2)  \\
                   & = \frac{u}{4}(-u b_2 + 2u b_1 + u^2 b_1^2) = \frac{u^2}{4}(2 b_1 - b_2 + u b_1^2).
\end{align}
%
This final expression is stable to compute and $\mathcal{O}(\frac{1}{z^2})$ as expected.

\subsubsection{Expectations over $P(t \mid \theta)$}\label{sec:expectation-covariance}

Since $f_G(x,\nu)$ is constant with respect to $t$, we can factor $\exp(-f_G(x,\nu))$ out of the numerator and denominator of $P(t \mid \theta)$, resulting in the more numerically stable form
%
\begin{align}
  P(t \mid \theta)
  = \frac{\exp(-\tilde{f}(t, \theta))}{\int_0^1 \exp(-\tilde{f}(t', \theta)) \, dt'}
  = \frac{\exp(-\tilde{r}(t, \theta))}{\int_0^1 \exp(-\tilde{r}(t', \theta)) \, dt'}.
\end{align}
%
In the high SNR regime with $\delta$ small such that $\tilde{r}$ is small, we can compute $\log P$ in a more stable form:
%
\begin{align}
  \log P(t \mid \theta)
   & = -\log \left( \int_0^1 \exp(\tilde{r}(t, \theta) - \tilde{r}(t', \theta)) \, dt' \right)        \\
   & = -\log \left(1 + \int_0^1 \exp(\tilde{r}(t, \theta) - \tilde{r}(t', \theta)) - 1 \, dt' \right)
\end{align}
%
where $\log(1 + \epsilon)$ and $\exp(\epsilon - 1)$ are computed stably for small $\epsilon$ using the special-function routines \texttt{log1p($\epsilon$)} and \texttt{expm1($\epsilon$)}.

\subsubsection{Differentiating expectations over $P(t \mid \theta)$}\label{sec:jacobian-expectation-covariance}

Let $g : [0, 1] \times \mathbb{R}^3 \to \mathbb{R}^m$, and denote as $\mathbb{E}[g] \in \mathbb{R}^m$ the expectation of $g(t, \theta)$ with respect to the density $P(t \mid \theta)$.
The Jacobian $\nabla_\theta \mathbb{E}[g] \in \mathbb{R}^{m \times 3}$ is given by
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{f}) \label{eq:gradient-expectation-covariance}          \\
                              & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{r}) \label{eq:gradient-expectation-covariance-residual}
\end{align}
%
where, letting $\mu_g = \mathbb{E}[g]$ and $\mu_h = \mathbb{E}[h]$, the covariance is defined as
%
\begin{align}
  \mathrm{Cov}(g, h) = \mathbb{E}[(g - \mu_g)(h - \mu_h)^T] = \mathbb{E}[g h^T] - \mu_g \mu_h^T.
\end{align}
%
This result follows from differentiating the definition of the expectation, $\mathbb{E}[g] = Z^{-1} \int_0^1 g \exp(-\tilde{f})$.
Crucially, this identity holds exactly even when the expectation is computed via a fixed-node quadrature, $\mathbb{E}[g] \approx \sum_i \frac{w_i}{2} g(t_i) P(t_i \mid \theta)$, because both integration and finite summation are linear functionals that commute with differentiation.

\paragraph{Proof of \cref{eq:gradient-expectation-covariance}}

We start with the definition of the expectation and apply the product rule:
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \nabla_\theta \left( Z(\theta)^{-1} \int_0^1 g(t,\theta) \exp(-\tilde{r}(t,\theta)) \, dt \right)                                            \\
                              & = \left(\int_0^1 g e^{-\tilde{r}} \, dt\right) (\nabla_\theta Z^{-1})^T + Z^{-1} \nabla_\theta \left( \int_0^1 g e^{-\tilde{r}} \, dt \right).
\end{align}
%
The first term simplifies using the identity $\nabla_\theta Z^{-1} = -Z^{-1} \nabla_\theta \log Z$:
%
\begin{align}
  \left(Z \mathbb{E}[g]\right) \left(-Z^{-1} \nabla_\theta \log Z\right)^T = -\mathbb{E}[g] (\nabla_\theta \log Z)^T.
\end{align}
%
For the second term, we move the derivative inside the integral and apply the product rule again:
%
\begin{align}
  Z^{-1} \int_0^1 \nabla_\theta(g e^{-\tilde{r}}) \, dt & = Z^{-1} \int_0^1 \left( (\nabla_\theta g) e^{-\tilde{r}} - g (\nabla_\theta \tilde{r})^T e^{-\tilde{r}} \right) \, dt \\
                                                        & = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^T].
\end{align}
%
Combining these results gives
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^T] - \mathbb{E}[g](\nabla_\theta \log Z)^T.
\end{align}
%
To simplify the final term, we find the gradient of the log-partition function:
%
\begin{align}
  \nabla_\theta \log Z = \frac{\nabla_\theta Z}{Z} = Z^{-1} \int_0^1 \nabla_\theta e^{-\tilde{r}} \, dt = -Z^{-1} \int_0^1 \nabla_\theta \tilde{r} e^{-\tilde{r}} \, dt = -\mathbb{E}[\nabla_\theta \tilde{r}].
\end{align}
%
Substituting this back yields the final identity:
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^T] - \mathbb{E}[g] (-\mathbb{E}[\nabla_\theta \tilde{r}])^T \\
                              & = \mathbb{E}[\nabla_\theta g] - (\mathbb{E}[g (\nabla_\theta \tilde{r})^T] - \mathbb{E}[g] \mathbb{E}[\nabla_\theta \tilde{r}]^T)  \\
                              & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{r}).
\end{align}
%
This proof relies only on the linearity of differentiation and integration (or finite summation for quadrature), so the identity holds for both the continuous integral and its discrete quadrature approximation.

\subsubsection{Differentiating covariances with respect to $P(t \mid \theta)$}\label{sec:derivative-covariance}

For any two scalar functions $g(t,\theta)$ and $h(t,\theta)$, the derivative of their covariance with respect to a parameter $\alpha \in \{x, \nu, \delta\}$ is given by
%
\begin{align}\label{eq:derivative-covariance-identity}
  \partial_\alpha \mathrm{Cov}(g, h) = \mathrm{Cov}(\partial_\alpha g, h) + \mathrm{Cov}(g, \partial_\alpha h) - \mathrm{Cov3}(g, h, \partial_\alpha \tilde{r}),
\end{align}
%
where $\mathrm{Cov3}(a,b,c) = \mathbb{E}[(a-\mathbb{E}[a])(b-\mathbb{E}[b])(c-\mathbb{E}[c])]$ is the third joint central moment.

\paragraph{Proof of \cref{eq:derivative-covariance-identity}}

We start from the definition of covariance and apply the product rule:
%
\begin{align}
  \partial_\alpha \mathrm{Cov}(g, h) & = \partial_\alpha \left( \mathbb{E}[g h] - \mathbb{E}[g]\mathbb{E}[h] \right) = \partial_\alpha \mathbb{E}[g h] - (\partial_\alpha \mathbb{E}[g]) \mathbb{E}[h] - \mathbb{E}[g] (\partial_\alpha \mathbb{E}[h]).
\end{align}
%
Next, we apply the gradient-of-expectation identity from \cref{eq:gradient-expectation-covariance-residual} to each of the three derivative terms:
%
\begin{align}
  \partial_\alpha \mathbb{E}[g h] & = \mathbb{E}[\partial_\alpha(g h)] - \mathrm{Cov}(g h, \partial_\alpha \tilde{r}) = \mathbb{E}[(\partial_\alpha g) h + g (\partial_\alpha h)] - \mathrm{Cov}(g h, \partial_\alpha \tilde{r}) \\
  \partial_\alpha \mathbb{E}[g]   & = \mathbb{E}[\partial_\alpha g] - \mathrm{Cov}(g, \partial_\alpha \tilde{r})                                                                                                                 \\
  \partial_\alpha \mathbb{E}[h]   & = \mathbb{E}[\partial_\alpha h] - \mathrm{Cov}(h, \partial_\alpha \tilde{r}).
\end{align}
%
Substituting these back into the main expression and grouping terms yields:
%
\begin{align}
  \partial_\alpha \mathrm{Cov}(g, h) & = \left( \mathbb{E}[(\partial_\alpha g) h] - \mathbb{E}[\partial_\alpha g] \mathbb{E}[h] \right) + \left( \mathbb{E}[g (\partial_\alpha h)] - \mathbb{E}[g] \mathbb{E}[\partial_\alpha h] \right) \\
                                     & - \mathrm{Cov}(g h, \partial_\alpha \tilde{r}) + \mathrm{Cov}(g, \partial_\alpha \tilde{r}) \mathbb{E}[h] + \mathbb{E}[g] \mathrm{Cov}(h, \partial_\alpha \tilde{r}).
\end{align}
%
The term in the parenthesis is the third joint central moment, or third-order covariance, of $(g, h, \partial_\alpha\tilde{r})$. Expanding the definition of the third central moment confirms its equivalence to this term:
%
\begin{align}
  \mathrm{Cov3}(g, h, \partial_\alpha \tilde{r}) & = \mathbb{E}[(g-\mathbb{E}[g])(h-\mathbb{E}[h])(\partial_\alpha \tilde{r}-\mathbb{E}[\partial_\alpha \tilde{r}])]                                                                                                                                                                   \\
                                                 & = \mathbb{E}[(gh - g\mathbb{E}[h] - h\mathbb{E}[g] + \mathbb{E}[g]\mathbb{E}[h])(\partial_\alpha \tilde{r} - \mathbb{E}[\partial_\alpha \tilde{r}])]                                                                                                                                \\
                                                 & = \mathbb{E}[gh(\partial_\alpha \tilde{r} - \mathbb{E}[\partial_\alpha \tilde{r}])] - \mathbb{E}[h]\mathbb{E}[g(\partial_\alpha \tilde{r} - \mathbb{E}[\partial_\alpha \tilde{r}])] - \mathbb{E}[g]\mathbb{E}[h(\partial_\alpha \tilde{r} - \mathbb{E}[\partial_\alpha \tilde{r}])] \\
                                                 & = \mathrm{Cov}(gh, \partial_\alpha \tilde{r}) - \mathbb{E}[h]\mathrm{Cov}(g, \partial_\alpha \tilde{r}) - \mathbb{E}[g]\mathrm{Cov}(h, \partial_\alpha \tilde{r}).
\end{align}
%
Substituting this back gives the final expression for the derivative of the covariance.

\subsection{Residual gradient}

We compute the gradient of $\Omega(\theta) = -\log Z(\theta) - \log\delta$ in terms of the gradient of $\log Z$:
%
\begin{align}
  \nabla_{\theta} \log Z(\theta) = \frac{1}{Z(\theta)}\nabla_{\theta} Z(\theta) = \frac{1}{Z(\theta)}\int_0^1 -\nabla_{\theta}\tilde{f} \cdot \exp(-\tilde{f}) \, dt = -\mathbb{E}[\nabla_{\theta}\tilde{f}].
\end{align}
%
Rewriting the expectation in terms of the residual energy $\tilde{r}$, we have
%
\begin{align}
  \mathbb{E}[\nabla_{\theta}\tilde{f}] = \mathbb{E}[\nabla_{\theta} (\tilde{r} + f_G)] = \mathbb{E}[\nabla_{\theta} \tilde{r}] + \nabla_{\theta} f_G
\end{align}
%
since $f_G$ is constant with respect to $t$.
Finally, the gradient $\nabla_{\theta} \Omega$ simplifies to
%
\begin{align}
  \nabla_{\theta} \Omega & = - \nabla_\theta \log Z(\theta) - \nabla_\theta \log\delta = \mathbb{E}[\nabla_{\theta} \tilde{f}] - (0, 0, \delta^{-1})^T \\
                         & = \mathbb{E}\left[ \begin{pmatrix} \tilde{r}_x \\ \tilde{r}_\nu \\ t \tilde{f}_x \end{pmatrix} \right]
  + \begin{pmatrix} x - \nu \\ \nu - x \\ -\delta^{-1} \end{pmatrix}
\end{align}
%
where $\frac{\partial}{\partial x} f_G = x - \nu$, $\frac{\partial}{\partial \nu} f_G = \nu - x$, and $\tilde{r}_x$, $\tilde{r}_\nu$, and $f_x$ are evaluated at $y=x+\delta t$ as in \cref{sec:first-derivatives-residual}.

\subsection{Residual Hessian}

We can compute the Hessian of $\Omega$ by differentiating $\nabla_{\theta} \Omega$ using the gradient of the expectation identity \cref{eq:gradient-expectation-covariance}:
%
\begin{align}
  \nabla_{\theta}^2 \Omega & = \nabla_\theta \left( \mathbb{E}[\nabla_{\theta} \tilde{f}] - (0, 0, \delta^{-1})^T \right)                                                       \\
                           & = \mathbb{E}[\nabla_{\theta}^2 \tilde{f}] - \mathrm{Cov}(\nabla_{\theta} \tilde{f}, \nabla_{\theta} \tilde{f}) + \mathrm{diag}(0, 0, \delta^{-2}).
\end{align}
%
Substituting $\tilde{f} = \tilde{r} + f_G$ and recalling that $f_G$ is constant with respect to $t$, we have
%
\begin{align}
  \nabla_{\theta}^2 \Omega
   & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r} + \nabla_{\theta}^2 f_G] - \mathrm{Cov}(\nabla_{\theta} \tilde{r} + \nabla_{\theta} f_G, \nabla_{\theta} \tilde{r} + \nabla_{\theta} f_G) + \mathrm{diag}(0, 0, \delta^{-2}) \\
   & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r}] + \nabla_{\theta}^2 f_G - \mathrm{Cov}(\nabla_{\theta} \tilde{r}, \nabla_{\theta} \tilde{r}) + \mathrm{diag}(0, 0, \delta^{-2}) \label{eq:residual-hessian-full}            \\
   & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r}] - \mathrm{Cov}(\nabla_{\theta} \tilde{r}, \nabla_{\theta} \tilde{r}) + \begin{pmatrix} 1 & -1 & 0 \\ -1 & 1 & 0 \\ 0 & 0 & \delta^{-2} \end{pmatrix} \label{eq:residual-hessian-full-simplified}
\end{align}
%
where $\frac{\partial^2}{\partial x^2} f_G = \frac{\partial^2}{\partial \nu^2} f_G = 1$, $\frac{\partial^2}{\partial x \partial \nu} f_G = -1$.
The Hessian of the residual energy $\nabla_{\theta}^2 \tilde{r}$ is
%
\begin{align}
  \nabla_{\theta}^2 \tilde{r} = \begin{pmatrix} \tilde{r}_{xx} & \tilde{r}_{x\nu} & t \tilde{f}_{xx} \\ \tilde{r}_{x\nu} & \tilde{r}_{\nu\nu} & t \tilde{f}_{x\nu} \\ t \tilde{f}_{xx} & t \tilde{f}_{x\nu} & t^2 f_{xx} \end{pmatrix}
\end{align}
%
where $\tilde{r}_{xx}$, $\tilde{r}_{x\nu}$, and $\tilde{r}_{\nu\nu}$ are evaluated at $y=x+\delta t$ as in \cref{sec:second-derivatives-residual}.

\section{Third-Order Derivatives and Vector-Jacobian Products}

For applications requiring differentiation through statistical inference procedures, it may be necessary to compute the derivatives of the gradient and Hessian of the log-likelihood.
We define a single vector containing these quantities:
%
\begin{align}
  \Phi(\theta) = \left( \nabla_\theta \Omega(\theta), \ \mathrm{vech}(\nabla_\theta^2 \Omega(\theta)) \right)^T \in \mathbb{R}^9,
\end{align}
%
where $\theta = (x, \nu, \delta)$ and $\mathrm{vech}$ stacks the lower–triangular elements of the symmetric Hessian.
This section details the computation of the Jacobian of $\Phi$ and the corresponding vector-Jacobian product (VJP).

\subsection{Third derivatives of the log-likelihood}

The Jacobian of $\Phi$ contains the Hessian of $\Omega$ in its first three rows and the third-order partial derivatives of $\Omega$ in its last six rows. We derive the third derivatives by differentiating the Hessian of $\Omega$ from \cref{eq:residual-hessian-full} with respect to a parameter $\theta_k$ for $k \in \{x, \nu, \delta\}$:
%
\begin{align}
  \partial_k(\nabla_\theta^2 \Omega) = \partial_k \mathbb{E}[\nabla_\theta^2 \tilde{r}] - \partial_k\mathrm{Cov}(\nabla_\theta \tilde{r}, \nabla_\theta \tilde{r}) + \mathrm{diag}(0,0,-2\delta^{-3}).
\end{align}
%
The components of the third-derivative tensor are given by applying \cref{eq:gradient-expectation-covariance-residual,eq:derivative-covariance-identity} to differentiate the expectation and covariance terms:
%
\begin{align}
  \partial_{ijk} \Omega & = \left( \mathbb{E}[\partial_{ijk} \tilde{r}] - \mathrm{Cov}(\partial_{ij}\tilde{r}, \partial_k \tilde{f}) \right)                                                                                                                                                    \\
                        & - \left( \mathrm{Cov}(\partial_{ki} \tilde{r}, \partial_j \tilde{r}) + \mathrm{Cov}(\partial_i \tilde{r}, \partial_{kj} \tilde{r}) - \mathrm{Cov3}(\partial_i \tilde{r}, \partial_j \tilde{r}, \partial_k \tilde{f}) \right) - 2\delta^{-3} \mathbb{I}(i=j=k=\delta).
\end{align}
%
This final form, while complex, is composed entirely of expectations and covariances of quantities for which we have already derived stable numerical expressions. It can therefore be robustly evaluated using quadrature.

\subsection{Vector-Jacobian product of $\Phi$}

In the context of reverse-mode automatic differentiation, we require a function that computes the vector-Jacobian product (VJP) of $\Phi(\theta)$.
Given an incoming sensitivity vector $\Delta_\Phi \in \mathbb{R}^9$, the VJP is
%
\begin{align}
  (\nabla_\theta \Phi)^T \Delta_\Phi \in \mathbb{R}^3.
\end{align}
%
Let $\Delta_g = (\Delta_{\Phi,1}, \Delta_{\Phi,2}, \Delta_{\Phi,3})^T$ be the sensitivity for the gradient components of $\Phi$, and let $\Delta_H$ be the $3 \times 3$ symmetric matrix formed by un-vectorizing the remaining six elements of $\Delta_\Phi$.
The VJP can be split into two parts corresponding to the gradient and Hessian components of $\Phi$:
%
\begin{align}
  (\nabla_\theta \Phi)^T \Delta_\Phi = (\nabla_\theta^2 \Omega)^T \Delta_g + (\nabla_\theta (\mathrm{vech}(\nabla_\theta^2 \Omega)))^T \mathrm{vech}(\Delta_H).
\end{align}
%
The first term is simply $(\nabla_\theta^2 \Omega)^T \Delta_g = H \Delta_g$, since the Hessian $H$ is symmetric.
The second term is a contraction of the third-derivative tensor with the sensitivity matrix $\Delta_H$. Its $k$-th component is given by the sum over the unique elements of the Hessian:
%
\begin{align}
  \left[ (\nabla_\theta (\mathrm{vech}(H)))^T \mathrm{vech}(\Delta_H) \right]_k
  = \sum_{1 \le i \le j \le 3} (\partial_k H_{ij}) (\Delta_H)_{ij}
  = \sum_{1 \le i \le j \le 3} \partial_{ijk}\Omega \cdot (\Delta_H)_{ij}.
\end{align}
%
Thus, the full VJP is given by
%
\begin{align}\label{eq:vjp-full}
  \left( (\nabla_\theta \Phi)^T \Delta_\Phi \right)_k = (H \Delta_g)_k + \sum_{1 \le i \le j \le 3} \partial_{ijk}\Omega \cdot (\Delta_H)_{ij},
\end{align}
%
which can be computed by first forming the Hessian and third-derivative tensor of $\Omega$ using quadrature and then performing the matrix-vector products and summations.

\subsection{Application: Differentiating Through a Laplace Approximation}

The need for third derivatives arises when differentiating through complex inference procedures. Consider a model where data $\{x_i\}_{i=1}^N$ are drawn from Quantized Rician distributions with parameters $\{\nu_i\}_{i=1}^N$ and a single shared noise parameter $\sigma$, with a known quantization width $\delta$. The total negative log-likelihood is
%
\begin{align}
  L(\nu, \sigma) = \sum_{i=1}^N -\log p(x_i \mid \nu_i, \sigma, \delta) = \sum_{i=1}^N \Omega(x_i/\sigma, \nu_i/\sigma, \delta/\sigma),
\end{align}
%
where $\nu = (\nu_1, \dots, \nu_N)^T$.

Suppose we construct a factorized approximate posterior $q(\nu, \sigma) = q(\nu) q(\sigma \mid \nu)$, where the conditional for $\sigma$ is a Laplace approximation of $p(\sigma \mid x, \nu, \delta)$. For a fixed $\nu$, this approximation is a Gaussian centered at the conditional posterior mode:
%
\begin{align}
  \hat{\sigma}(\nu) = \arg\min_\sigma L(\nu, \sigma).
\end{align}
%
The approximation is $q(\sigma \mid \nu) = \mathcal{N}(\sigma \mid \mu=\hat{\sigma}(\nu), \Sigma=H_\sigma^{-1})$, where the precision is the Hessian at the mode:
%
\begin{align}
  H_\sigma(\nu) = \nabla_\sigma^2 L(\nu, \sigma) \big|_{\sigma=\hat{\sigma}(\nu)}.
\end{align}
%
If the parameters $\nu$ are themselves outputs of another model (e.g., a neural network with variational parameters $\psi$), such that $\nu = \nu(\psi)$, then the parameters of our Laplace approximation, $\hat{\sigma}(\psi)$ and $H_\sigma(\psi)$, depend on $\psi$.
To optimize an objective function like the evidence lower bound (ELBO) in variational inference, we must differentiate a loss $J(\hat{\sigma}, H_\sigma)$ with respect to $\psi$ using reverse-mode automatic differentiation (AD).
This requires implementing pullback functions for the maps $\psi \mapsto \hat{\sigma}$ and $\psi \mapsto H_\sigma$, which compute the corresponding VJPs.

The pullback for $\hat{\sigma}$ is derived using the implicit function theorem. The mode $\hat{\sigma}$ satisfies the first-order optimality condition $\nabla_\sigma L(\nu(\psi), \hat{\sigma}(\psi)) = 0$. Differentiating this identity with respect to $\psi$ and rearranging gives the Jacobian of the map $\psi \mapsto \hat{\sigma}$:
%
\begin{align}
  \nabla_\psi \hat{\sigma} = - (H_\sigma)^{-1} (\nabla_\nu \nabla_\sigma L) (\nabla_\psi \nu).
\end{align}
%
Given an incoming sensitivity $\Delta_{\hat{\sigma}} = \partial J / \partial \hat{\sigma}$, the pullback computes the sensitivity with respect to $\nu$ by applying the transpose of the Jacobian:
%
\begin{align}
  \Delta_\nu \mathrel{+}= (\nabla_\psi \nu)^T \left[ -(\nabla_\nu \nabla_\sigma L)^T (H_\sigma)^{-1} \Delta_{\hat{\sigma}} \right].
\end{align}

The pullback for $H_\sigma$ involves third derivatives. The map is $H_\sigma(\nu(\psi), \hat{\sigma}(\psi))$. In reverse-mode AD, we are given a sensitivity $\Delta_{H_\sigma} = \partial J / \partial H_\sigma$ and compute the sensitivities for the inputs $\nu$ and $\hat{\sigma}$, which are then propagated further back. The pullback for $H_\sigma$ is a sequence of VJPs dictated by the chain rule:
%
\begin{align}
  \Delta_{\hat{\sigma}} & \mathrel{+}= (\nabla_{\hat{\sigma}} H_\sigma)^T \Delta_{H_\sigma} \\
  \Delta_\nu            & \mathrel{+}= (\nabla_\nu H_\sigma)^T \Delta_{H_\sigma}.
\end{align}
%
The term $\nabla_\nu H_\sigma = \nabla_\nu (\nabla_\sigma^2 L)$ is a third-order partial derivative tensor of $L$.
Since $L$ is a sum over $\Omega$, calculating this VJP requires computing terms involving the third derivatives of $\Omega$, such as $\frac{\partial^3 \Omega}{\partial \nu_i \partial \sigma^2}$.
This provides a concrete motivation for the VJP formalism developed in this section.

\end{document}
