\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[sort,compress]{cleveref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}

\title{Stable, efficient evaluation of gradients and Hessians for the Rician log-likelihood}
\author{Jonathan Doucette}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}

We consider the Rician probability density for a positive observation $x>0$ with noncentrality parameter $\nu>0$ and scale $\sigma>0$.
The pdf is
%
\begin{align}
  p(x \mid \nu, \sigma) = \frac{x}{\sigma^2} \exp\left(-\frac{x^2+\nu^2}{2\sigma^2}\right) I_0\left(\frac{x\nu}{\sigma^2}\right) \label{eq:rician-pdf}
\end{align}
%
where $I_0$ is the modified Bessel function of the first kind of order zero.

We seek numerically stable, machine-precision formulas for the negative log-likelihood and its first- through third-order partial derivatives with respect to $x$ and $\nu$.
If we define
%
\begin{align}
  f(x, \nu) \coloneqq -\log p(x \mid \nu, \sigma = 1),
\end{align}
%
then the log of the likelihood of \cref{eq:rician-pdf} can be given in terms of $f$ via
%
\begin{align}
  \log p(x \mid \nu, \sigma) = -\log\sigma - f\left(\frac{x}{\sigma}, \frac{\nu}{\sigma}\right).
\end{align}
%
Similarly, the derivatives of $\log p$ with respect to $x$, $\nu$, and $\sigma$ are given in terms of the derivatives of $f$ evaluated at $(x', \nu') = (x / \sigma, \nu / \sigma)$:
%
\begin{align}
  \frac{\partial \log p}{\partial x}      = -\frac{1}{\sigma} \frac{\partial f}{\partial x'},                                                                                        \qquad
  \frac{\partial \log p}{\partial \nu}    = -\frac{1}{\sigma} \frac{\partial f}{\partial \nu'},                                                                                      \qquad
  \frac{\partial \log p}{\partial \sigma} = \frac{1}{\sigma} \left( x' \frac{\partial f}{\partial x'} + \nu' \frac{\partial f}{\partial \nu'} - 1 \right)
\end{align}
%
and likewise for higher-order derivatives.

Henceforth we take $\sigma=1$ and, by slight abuse of notation, write $x, \nu$ for the rescaled variables $x', \nu'$.
We will show that the key to numerically stable evaluation of $f$ and its derivatives is to carefully consider the limiting behavior in the regimes where $z = x\nu \ll 1$ and $z \gg 1 \Leftrightarrow u=1/z \ll 1$.
Naive evaluation of $f$ and its derivatives produces expressions with catastrophic cancellation in both regimes.
To evaluate these expressions in a numerically stable way, we derive algebraic simplifications in both the $z \ll 1$ and $z \gg 1$ regimes.

\section{The Rician log-likelihood and basic simplifications}

With $\sigma=1$, the Rician negative log-likelihood is given by
%
\begin{align}
  f(x, \nu) & \eqqcolon -\log p(x \mid \nu, \sigma=1)                                                                                                                        \\
            & = \frac{x^2 + \nu^2}{2} - \log x - \log I_0(z) \label{eq:rician-neg-log-likelihood}                                                                            \\
            & = \frac{(x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{x}{\nu}\right) - \log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi). \label{eq:rician-neg-log-likelihood-scaled}
\end{align}
%
Here, $\hat{I}_0$ is the modified Bessel function of the first kind normalized such that $\lim_{z\to\infty} \hat{I}_0(z) = 1$:
%
\begin{align}
  I_0(z) = \frac{e^z}{\sqrt{2\pi z}} \hat{I}_0(z) \quad \Leftrightarrow \quad \hat{I}_0(z) = \sqrt{2\pi z} e^{-z} I_0(z)
\end{align}
%
where $z = x\nu \ge 0$.
Next, we consider the numerical stability of these algebraically equivalent forms of $f$.

\paragraph{Small $z \ll 1$.}

For small $z$, we have that $\log I_0(z) = z^2/4 + \mathcal{O}(z^4)$, and thus \cref{eq:rician-neg-log-likelihood} becomes
%
\begin{align}
  f(x, \nu) = \frac{x^2 + \nu^2}{2} - \log x - \frac{z^2}{4} + \mathcal{O}(z^4).
\end{align}
%
The terms in this expression are generally not of the same order of magnitude, so there are no cancellation issues.
% This is slightly more numerically stable than using $\log \hat{I}_0(z) = \log I_0(z) - z + \frac{1}{2}\log(2\pi z) = \frac{1}{2}\log(2\pi z) - z + \mathcal{O}(z^2)$, which may suffer small cancellation issues when computing $-\frac{1}{2}\log(\frac{x}{\nu}) - \log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi) = -\log x + z + \mathcal{O}(z^2)$.
% For example, if $x \approx 1$ and $\nu \approx z$, then $\frac{1}{2}\log(\frac{x}{\nu}) \approx -\frac{1}{2}\log z$ and $-\log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi) \approx -\frac{1}{2}\log z$ but the result is $z \ll -\log z$.

\paragraph{Large $z \gg 1$.}

For large $z$, we have that $\log \hat{I}_0(z) = \frac{1}{8 z} + \mathcal{O}(z^{-2})$, and thus \cref{eq:rician-neg-log-likelihood-scaled} becomes
%
\begin{align}
  f(x, \nu) = \frac{(x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{x}{\nu}\right) + \frac{1}{2}\log(2\pi) - \frac{1}{8 z} + \mathcal{O}(z^{-2}).
\end{align}
%
The terms in this expression are generally not of the same order of magnitude -- unless $x \approx \nu$, which we address next -- so there are typically no cancellation issues.

\paragraph{High SNR $x \approx \nu \approx \sqrt{z} \gg 1$.}

A common case in practice is when the signal-to-noise ratio (SNR) is large:
observations are concentrated near the noncentrality parameter such that $x \approx \nu \pm 1$ (recall that the noise level is normalized to $\sigma=1$).
As was the case for large $z$, we should use the scaled form of \cref{eq:rician-neg-log-likelihood-scaled} but with a stable evaluation of $\log(x/\nu)$:
%
\begin{align}
  \log\left(\frac{x}{\nu}\right) = \begin{cases}
    \log\left(1 + \frac{x-\nu}{\nu}\right) & \text{if } x \ge \nu \\
    -\log\left(1 + \frac{\nu-x}{x}\right)  & \text{if } x < \nu
  \end{cases}
\end{align}
%
where $\log(1 + \epsilon) = \epsilon - \epsilon^2/2 + \mathcal{O}(\epsilon^3)$ is computed using the common special-function routine \texttt{log1p($\epsilon$)} which is designed to be accurate when $\epsilon$ is small.
With this modification, \cref{eq:rician-neg-log-likelihood-scaled} is accurate for all $z \gg 1$.

We note that \cref{eq:rician-neg-log-likelihood} is not stable in the high SNR regime, since $\log I_0(z) = \log \hat{I}_0(z) + z - \frac{1}{2}\log(2\pi z)$ has a large component $z$ which catastrophically cancels with $(x^2 + \nu^2) / 2 \approx z$.

\subsection{First derivatives}

Now, we differentiate $f$ with respect to $x$ and $\nu$ and simplify algebraically:
%
\begin{align}
  f_x   & = x-\nu -\frac{1}{2x} - \nu\frac{d}{dz}\log\hat{I}_0(z) \label{eq:first-derivatives-unsimplified-x}  \\
  f_\nu & = \nu-x +\frac{1}{2\nu} - x\frac{d}{dz}\log\hat{I}_0(z) \label{eq:first-derivatives-unsimplified-nu}
\end{align}
%
where
%
\begin{align}
  \frac{d}{dz}\log\hat{I}_0(z) & = r(z) - 1 + \frac{1}{2z} \label{eq:log-scaled-bessel-derivative} \\
  r(z)                         & = \frac{I_1(z)}{I_0(z)}. \label{eq:ratio-r}
\end{align}
%
Substituting \cref{eq:log-scaled-bessel-derivative} and $z=x\nu$ into \cref{eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu} gives
%
\begin{align}
  f_x   & = x - \nu r - \frac{1}{x} \label{eq:first-derivatives-simplified-x} \\
  f_\nu & = \nu - x r \label{eq:first-derivatives-simplified-nu}
\end{align}

\subsection{Second derivatives}

Next, we differentiate the simplified first derivatives in \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu} to obtain the second derivatives:
%
\begin{align}
  f_{xx}     & = \frac{\partial}{\partial x}\left(x - \nu r(z) - \frac{1}{x}\right) = 1 + \frac{1}{x^2} - \nu^2 r' \label{eq:second-derivatives-unsimplified-x-x} \\
  f_{x\nu}   & = \frac{\partial}{\partial \nu}\left(x - \nu r(z) - \frac{1}{x}\right) = -(r + z r') = -z(1 - r^2) \label{eq:second-derivatives-unsimplified-x-nu} \\
  f_{\nu\nu} & = \frac{\partial}{\partial \nu}\left(\nu - x r(z)\right) = 1 - x^2 r' \label{eq:second-derivatives-unsimplified-nu-nu}
\end{align}
%
where the last equality in \cref{eq:second-derivatives-unsimplified-x-nu} follows from the recurrence relation \cref{eq:r-prime-recurrence}.

\subsection{Third derivatives}

Differentiate once more to obtain third derivatives;
each depends only on $r'$ and $r''$:
%
\begin{align}
  f_{xxx}       & = \frac{\partial}{\partial x}\left(1+\frac{1}{x^2}-\nu^2 r'(z)\right) = -\frac{2}{x^3} - \nu^3 r'' \label{eq:third-derivatives-unsimplified-x-x-x}                                 \\
  f_{xx\nu}     & = \frac{\partial}{\partial \nu}\left(1+\frac{1}{x^2}-\nu^2 r'(z)\right) = -2\nu r' - x\nu^2 r'' = -\nu(2r' + z r'')              \label{eq:third-derivatives-unsimplified-x-x-nu}  \\
  f_{x\nu\nu}   & = \frac{\partial}{\partial x}\left(1 - x^2 r'(z)\right) = -2x r' - x^2\nu r'' = -x(2r' + z r'')                                  \label{eq:third-derivatives-unsimplified-x-nu-nu} \\
  f_{\nu\nu\nu} & = \frac{\partial}{\partial \nu}\left(1 - x^2 r'(z)\right) = -x^3 r''. \label{eq:third-derivatives-unsimplified-nu-nu-nu}
\end{align}
%
Therefore it suffices to compute $r$, $r'$, and $r''$ with high relative accuracy.

\subsection{Recurrence relations for derivatives of $r$}

To obtain $r$, $r'$, and $r''$, we differentiate $r=I_1/I_0$.
Using $I_0'=I_1$ and $I_1'=I_0 - I_1/z$, we have the recurrence relations
%
\begin{align}
  r'  & = \frac{I_1'}{I_0} - r\frac{I_0'}{I_0} = \frac{I_0 - \frac{1}{z} I_1}{I_0} - r^2 = 1 - \frac{r}{z} - r^2 \label{eq:r-prime-recurrence}                                           \\
  r'' & = -\left(\frac{r}{z}\right)' - 2 r r' = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r' = 2 r (r^2 - 1) + \frac{3 r^2 - 1}{z} + \frac{2 r}{z^2} \label{eq:r-second-derivative-recurrence}
\end{align}

These are exact identities but are numerically unstable as $z \to 0$ and as $z \to \infty$.
To address this, for each of $r$, $r'$, and $r''$, we split the domain $z>0$ into three regimes and approximate a single well-scaled quantity in each regime:
\begin{itemize}
  \item Small $z$: use a Taylor expansion, group terms by order in $z$, and fit a minimax polynomial to the residual.
  \item Large $z$: use the asymptotic expansion in $u = 1/z$, group terms by order in $u$, and fit a minimax polynomial to the residual.
  \item Intermediate $z$: cancellation is negligible; employ a rational minimax approximant.
\end{itemize}

\section{Small-$z$ analysis}\label{sec:rician-small-z}

For small $z$, $r \approx z/2$.
Thus if we define
%
\begin{align}
  \boxed{
    \begin{aligned}
      a_0(z)           & = \frac{r(z)}{z} = \frac{1}{2} - \frac{1}{16}z^2 + \mathcal{O}(z^4) \\
      \Rightarrow r(z) & = z a_0(z)
    \end{aligned}
  }
\end{align}
%
then for computing $r$, we need only approximate $a_0$ to high precision.
Similarly, to compute $r'$, we have
%
\begin{align}\label{eq:r-prime-reparametrized}
  \boxed{r'(z) = 1 - \frac{r}{z} - r^2 = 1 - a_0 - z^2 a_0^2}
\end{align}
%
which is numerically stable since $a_0 = \frac{1}{2} + \mathcal{O}(z^2)$ and thus both the constant coefficient $1 - a_0$ and the quadratic coefficient $-a_0^2$ do not suffer from cancellation in floating-point arithmetic.

However, the second derivative $r''$ requires more care.
Observe that
%
\begin{align}
  r''(z) & = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r'                                             \\
         & = -\frac{1 - a_0 - z^2 a_0^2}{z} + \frac{z a_0}{z^2} -  2 z a_0(1 - a_0 - z^2 a_0^2) \\
         & = \frac{2 a_0 - 1}{z} + z a_0 (3 a_0 - 2) + 2 z^3 a_0^3.
\end{align}
%
The leading $\mathcal{O}(1/z)$ term has coefficient $2a_0 - 1$, but for small $z$, $a_0 \approx \frac{1}{2} - \frac{1}{16}z^2$ and thus computing $2a_0 - 1$ requires subtracting two $\mathcal{O}(1)$ terms to obtain a $\mathcal{O}(z^2)$ term.
We therefore reparametrize to
%
\begin{align}
  a_0                 & = \frac{1}{2} + z^2 a_1 \label{eq:a0-reparametrized}                                                          \\
  \Leftrightarrow a_1 & = \frac{1}{z^2} (a_0 - \frac{1}{2}) = \frac{1}{z^2} (\frac{r}{z} - \frac{1}{2}). \label{eq:a1-reparametrized}
\end{align}
%
Note that since $r = \frac{1}{2}z - \frac{1}{16}z^3 + \mathcal{O}(z^5)$, it follows that $a_1 = -\frac{1}{16} + \mathcal{O}(z^2)$.
We can then rewrite the second derivative into a numerically stable form using $(2 a_0 - 1) / z = 2 z a_1$:
%
\begin{align}
  \boxed{r''(z) = z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3} \label{eq:r-second-derivative-reparametrized}
\end{align}
%
We also note that the quantity $2r' + z r''$ from \cref{eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu} does not suffer cancellation issues, since $2r' = 1 + \mathcal{O}(z^2)$ and $z r'' = \mathcal{O}(z^2)$.

Thus, we need only fit one minimax polynomial to \cref{eq:a1-reparametrized} to estimate $a_1(z)$, and then $r$, $r'$, and $r''$ can be formed from $a_1$ and $a_0 = \frac{1}{2} + z^2 a_1$ without catastrophic cancellation using \cref{eq:r-prime-reparametrized,eq:r-second-derivative-reparametrized}.

% We can then rewrite the first and second derivatives of $r$ as
% %
% \begin{align}
%   r'(z) & = 1 - a_0 - z^2 a_0^2 \\
%   & = 1 - (\frac{1}{2} + z^2 a_1) - z^2 (\frac{1}{2} + z^2 a_1)^2 \\
%   &= \frac{1}{2} - z^2 (a_1 + \frac{1}{4}) - z^4 a_1 - z^6 a_1^2
% \end{align}
% %
% \begin{align}
%   r''(z) & = \frac{2 a_0 - 1}{z} + z a_0 (3 a_0 - 2) + 2 z^3 a_0^3 \\
%   &= \frac{2(\frac{1}{2} + z^2 a_1)-1}{z} - 2z(\frac{1}{2} + z^2 a_1) + 2z(\frac{1}{2} + z^2 a_1)^{2} + 2z^3 (\frac{1}{2} + z^2 a_1)^3 \\
%          & = z (2a_1 - \frac{1}{2}) + \frac{1}{4} z^3 + z^5 a_1 (\frac{3}{2} + 2 a_1) + z^7 (3 a_1^2) + z^9 (2 a_1^3)
% \end{align}

% Finally, we see from \cref{eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu} that we should double-check the quantity $2r' + zr''$ for cancellation issues:
% %
% \begin{align}
%   2r' + zr'' & = 2(1 - a_0 - z^2 a_0^2) + z(z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3) \\
%              & = 2 - 2a_0 - 2z^2 a_0^2 + z^2 (2a_1 + a_0 (3 a_0 - 2)) + 2 z^4 a_0^3 \\
%              & = 2 a_0^3 z^4 + a_0^2 z^2 - 2 a_0 z^2 - 2 a_0 + 2 a_1 z^2 + 2 \\
%              & = 2 - 2a_0 + z^2 (2a_1 - a_0 (2 - a_0)) + 2 a_0^3 z^4
% \end{align}

\section{Large-$z$ analysis}\label{sec:rician-large-z}

As $z \to \infty$, $r \to 1$.
To avoid cancellation issues analogous to the small-$z$ case, we work with $u=1/z$ and rescale the residual $1-r$ to obtain a well-scaled quantity.
Let
%
\begin{align}
  \boxed{
    \begin{aligned}
      b_0(u)           & = \frac{1-r(z)}{u} = \frac{1}{2} + \frac{1}{8} u + \frac{1}{8} u^2 + \mathcal{O}(u^3) \\
      \Rightarrow r(z) & = 1 - u b_0(u) \label{eq:r-large-reparametrized}
    \end{aligned}
  }
\end{align}
%
Approximating $b_0$ with a minimax polynomial allows us to compute $r$ without catastrophic cancellation.

Next, we consider $r'$:
%
\begin{align}
  r'(z) & = 1 - \frac{r}{z} - r^2                                                     \\
        & = 1 - u(1 - u b_0) - (1 - u b_0)^2                                          \\
        & = u (2 b_0 - 1) + u^2 b_0 (1 - b_0) \label{eq:r-prime-large-reparametrized}
\end{align}
%
Analogous to the small-$z$ case, this will lead to cancellation issues since $2 b_0 - 1 = \mathcal{O}(u)$.
Thus, we reparametrize to
%
\begin{align}
  b_1(u)             & = \frac{2 b_0(u)-1}{u} = \frac{1}{4} + \frac{1}{4} u + \mathcal{O}(u^2) \\
  \Rightarrow b_0(u) & = \frac{1 + u b_1(u)}{2}
\end{align}
%
Then, $r'$ simplifies to
%
\begin{align}
  \boxed{
    \begin{aligned}
      r' = u(2 b_0-1) + u^2 b_0(1-b_0) = u^2 (b_1 + b_0(1-b_0)) \label{eq:r-prime-large-reparametrized-simplified}
    \end{aligned}
  }
\end{align}
%
which avoids cancellation issues since $b_1 \to \frac{1}{4}$ and $b_0 \to \frac{1}{2}$ as $u \to 0$, thus $r' = \frac{1}{2} u^2 + \mathcal{O}(u^3)$.

Next, we consider $r''$:
%
\begin{align}
  r'' & = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r'                                                                                                        \\
      & = -u(u^2 (b_1 + b_0(1-b_0))) + u^2 (1 - u b_0) - 2 (1 - u b_0) (u^2 (b_1 + b_0(1-b_0)))                                                         \\
      & = u^2 (2 b_0^2 - 2 b_1 - (2 b_0 - 1)) + u^3 (b_1 (2 b_0 - 1) - 2 b_0 + 3b_0^2 - 2 b_0^3)                                                        \\
      & = u^2 (2 b_0^2 - 2 b_1) + u^3 (b_0 (- 2 + 3b_0 - 2 b_0^2) - b_1) + u^4 b_1^2                                                                    \\
      & = u^2 (2 b_0^2 - 2 b_1) + u^3 (b_0 (2(2 b_0 - 1) - b_0 (1 + 2 b_0)) - b_1) + u^4 b_1^2                                                          \\
      & = u^2 (2 b_0^2 - 2 b_1) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0) \label{eq:r-second-derivative-large-reparametrized-unsimplified}
\end{align}
%
Now, since $b_0 = \frac{1}{2} + \mathcal{O}(u)$ and $b_1 = \frac{1}{4} + \mathcal{O}(u)$, we have $b_0^2 - b_1 = \mathcal{O}(u)$, and thus the leading term is order $\mathcal{O}(u^3)$, as expected since $r = 1 + \mathcal{O}(1/z)$.
To avoid cancellation issues, we introduce one final change of variables:
%
\begin{align}
  b_2(u)             & = \frac{4 b_1(u) - 1}{u} = 1 + \mathcal{O}(u) \label{eq:b2-reparametrized} \\
  \Rightarrow b_1(u) & = \frac{1 + u b_2(u)}{4} \label{eq:b1-reparametrized}
\end{align}
%
Then, we have
%
\begin{align}
  2 b_0^2 - 2 b_1 & = 2(\frac{1 + u b_1}{2})^2 - 2 b_1 = \frac{1}{2} (1 - 4 b_1 + 2 u b_1 + u^2 b_1^2) \\
                  & = \frac{1}{2} u (-b_2 + 2 b_1) + \frac{1}{2} u^2 b_1^2
\end{align}
%
which we substitute into \cref{eq:r-second-derivative-large-reparametrized-unsimplified} and simplify to obtain
%
\begin{align}
  r'' & = u^2 (2 b_0^2 - 2 b_1) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0)                          \\
      & = \frac{1}{2} u^2 (u (-b_2 + 2 b_1) + u^2 b_1^2) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0) \\
      & = -u^3 (\frac{1}{2}b_2 + b_0^2 (1 + 2 b_0)) + u^4 b_1 (\frac{3}{2} b_1 + 2 b_0)
\end{align}
%
and finally
%
\begin{align}\label{eq:r-second-derivative-large-reparametrized}
  \boxed{r'' = -\frac{1}{2} u^3 (b_2 + 2 b_0^2 (1 + 2 b_0) - u b_1 (3 b_1 + 4 b_0))}
\end{align}
%
Then, $r$, $r'$, and $r''$ can be formed from $b_2$, $b_1 = \frac{1 + u b_2}{4}$, and $b_0 = \frac{1 + u b_1}{2}$ using \cref{eq:r-prime-large-reparametrized,eq:r-second-derivative-large-reparametrized} without catastrophic cancellation.
However, as we will see in \cref{sec:rician-third-derivatives}, a final reparameterization
%
\begin{align}
  b_3(u)             & = \frac{b_2(u)-1}{u} = \frac{25}{16} + \mathcal{O}(u) \\
  \Rightarrow b_2(u) & = 1 + u b_3(u)
\end{align}
%
will be required to stably compute the quantity $2r'+zr''$ needed for third derivatives of $f$ when $z \gg 1$.
Finally, we can stably compute all of $r$, $r'$, $r''$, and $2r'+zr''$ from one minimax polynomial fit to $b_3(u)$.

% \section{Intermediate-$z$ analysis}
%
% TODO?

\section{Further Rician derivative simplifications}

\subsection{First derivatives}\label{sec:rician-first-derivatives}

\paragraph{Small-$z$}

The algebraic simplification of \cref{eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu} leading to \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu} is sufficient to avoid catastrophic cancellation.
In particular, when $\nu \ll 1$, the problematic $\frac{1}{2\nu}$ and $\frac{-x}{2z} = \frac{-1}{2\nu}$ terms in \cref{eq:first-derivatives-unsimplified-nu} are cancelled analytically, avoiding subtraction of two $\mathcal{O}(1/\nu)$ terms.
%
\begin{align}\label{eq:first-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_x   & = x - \nu r - \frac{1}{x} \\
      f_\nu & = \nu - x r
    \end{aligned}
  }
\end{align}

\paragraph{Large-$z$}

For $z \gg 1$, we have $r = 1 - b_0(1/z) / z$ from \cref{eq:r-large-reparametrized}, resulting in
%
\begin{align}
  f_x   & = x - \nu (1 - \frac{b_0}{z}) - \frac{1}{x}
  = x - \nu - \frac{1 - b_0}{x}                       \\
  f_\nu & = \nu - x (1 - \frac{b_0}{z})
  = \nu - x + \frac{b_0}{\nu}
\end{align}
%
These forms of $f_x$ and $f_\nu$ are more numerically stable when $x \approx \nu \approx \sqrt{z}$.
The improved numerical stability results from the approximation error being scaled by $1/x$ and $1/\nu$ instead of by $\nu$ and $x$.
If $\hat{r}=r(1+\delta_r)$ and $\hat{b}_0=b_0(1+\delta_b)$, the naive forms incur absolute errors $\mathcal{O}(\nu|\delta_r|)$ in $f_x$ and $\mathcal{O}(x|\delta_r|)$ in $f_\nu$, whereas the simplified forms incur errors $\mathcal{O}(|\delta_b|/x)$ and $\mathcal{O}(|\delta_b|/\nu)$.
Since $z = x\nu \gg 1$ and the relative errors $|\delta_r|$ and $|\delta_b|$ are comparably small, this yields an absolute-error reduction approximately by a factor of $z$.
%
%
\begin{align}\label{eq:first-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_x   & = x - \nu - \frac{1 - b_0}{x} \\
      f_\nu & = \nu - x + \frac{b_0}{\nu}
    \end{aligned}
  }
\end{align}

\subsection{Second derivatives}\label{sec:rician-second-derivatives}

\paragraph{Small-$z$}

Similar to the first derivatives, the algebraically simplified expressions in \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu} are sufficient to avoid catastrophic cancellation.
%
%
\begin{align}\label{eq:second-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_{xx}     & = 1 + \frac{1}{x^2} - \nu^2 r' \\
      f_{x\nu}   & = -z (1 - r^2) = -(r+z r')     \\
      f_{\nu\nu} & = 1 - x^2 r'
    \end{aligned}
  }
\end{align}
%
Here $r= z a_0$ and $r' = 1-a_0 - z^2 a_0^2$, with $1-a_0=\frac{1}{2}+\mathcal{O}(z^2)$ and $z^2 a_0^2=\mathcal{O}(z^2)$, so no catastrophic cancellation occurs.
The cross term may use either $-z(1-r^2)$ or $-(r+z r')$:
since $r \approx z/2$ and $zr' \approx z/2$, the former subtracts terms of different magnitudes, and the latter adds terms of the same magnitude but whose leading terms do not suffer cancellation issues.

\paragraph{Large-$z$}

We start by rewriting the second derivatives expressions in \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu} for stability at large-$z$:
%
\begin{align}
  f_{xx}     & = 1 + \frac{1}{x^2} - \nu^2 r' = 1 + \frac{1}{x^2}(1 - z^2 r') \\
  f_{x\nu}   & = -z(1 - r^2) = -z (1 - r) (1 + r)                             \\
  f_{\nu\nu} & = 1 - x^2 r'
\end{align}
%
Recall that for large-$z$, we have from \cref{eq:r-large-reparametrized,eq:r-prime-large-reparametrized-simplified} that
\begin{align}
  r                      & = 1 - \frac{b_0}{z} = 1 - \frac{1}{2z} - \frac{1}{8z^2} + \mathcal{O}(1/z^3)                \\
  r'                     & = \frac{1}{z^2} (b_1 + b_0(1 - b_0)) = \frac{1}{2z^2} + \frac{1}{4z^3} + \mathcal{O}(1/z^4) \\
  \Rightarrow z(1 - r^2) & = r + z r' = 1 + \frac{1}{z} (b_1 - b_0^2) = 1 + \frac{1}{8z^2} + \mathcal{O}(1/z^3)
\end{align}
where $b_0 = \frac{1}{2} (1 + u b_1)$.
%
We make the following observations:
%
\begin{itemize}
  \item $f_{xx}$: since $z^2 r' \approx \frac{1}{2}$, $1 - z^2 r'$ has no cancellation issues and $f_{xx} \approx 1 + \frac{1}{2x^2}$.
  \item $f_{\nu\nu}$: we have that $x^2 r' \approx \frac{x^2}{2z^2} = \frac{1}{2\nu^2}$; this also has no cancellation issues, and $f_{\nu\nu} \approx 1 - \frac{1}{2\nu^2}$.
  \item $f_{x\nu}$: $1 + r$ is stable to compute, but we must rewrite $z(1 - r) = b_0$ to avoid cancellation, so $f_{x\nu} = -b_0(1 + r) \approx -1 - \frac{1}{8z^2}$.
\end{itemize}
%
and therefore the stable forms are:
%
\begin{align}\label{eq:second-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_{xx}     & = 1 + \frac{1}{x^2}(1 - z^2 r') \\
      f_{x\nu}   & = -b_0 (1 + r)                  \\
      f_{\nu\nu} & = 1 - x^2 r'
    \end{aligned}
  }
\end{align}

% \subsubsection{Error analysis}
%
% Following the large-$z$ analysis for the first derivatives, we substitute $r = 1 - \frac{b_0}{z}$ from \cref{eq:r-large-reparametrized} and $r' = \frac{1}{z^2} (b_1 + b_0(1 - b_0))$ from \cref{eq:r-prime-large-reparametrized-simplified} into the second derivatives expressions \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu}:
% %
% \begin{align}\label{eq:second-derivatives-large-z}
%   \boxed{
%     \begin{aligned}
%       f_{xx}     & = 1 + \frac{1 - (b_1 + b_0(1-b_0))}{x^2} \\
%       f_{x\nu}   & = -b_0 (2 - \frac{b_0}{z})               \\
%       f_{\nu\nu} & = 1 - \frac{b_1 + b_0(1-b_0)}{\nu^2}
%     \end{aligned}
%   }
% \end{align}
% %
% To analyze the error propagation, let $\hat{r}=r(1+\delta_r)$, $\hat{r}' = r'(1+\delta_{r'})$, $\hat{b}_0=b_0(1+\delta_{b_0})$, and $\hat{b}_1=b_1(1+\delta_{b_1})$ with comparable small relative errors.
% For the naive forms, we have
% %
% \begin{align}
%   |\hat{f}_{xx} - f_{xx}|         & = \nu^2 |\hat{r}' - r'| = \nu^2 |r'| |\delta_{r'}| \sim \nu^2 \frac{1}{z^2} |\delta_{r'}| = \frac{|\delta_{r'}|}{x^2}      \\
%   |\hat{f}_{\nu\nu} - f_{\nu\nu}| & = x^2 |\hat{r}' - r'| = x^2 |r'| |\delta_{r'}| \sim x^2 \frac{1}{z^2} |\delta_{r'}| = \frac{|\delta_{r'}|}{\nu^2}          \\
%   |\hat{f}_{x\nu} - f_{x\nu}|     & = |\hat{r} + z\hat{r}' - (r + zr')| \le |r| |\delta_r| + z |r'| |\delta_{r'}| \sim |\delta_r| + \frac{1}{z} |\delta_{r'}|.
% \end{align}
% %
% Next, the stable large-$z$ forms:
% Let $c = b_1 + b_0(1 - b_0)$ so that $f_{xx} = 1 + (1-c)/x^2$ and $f_{\nu\nu} = 1 - c/\nu^2$.
% Then, we have that:
% %
% \begin{align}
%   \hat{c} - c & = (\hat{b}_1 - b_1) + (\hat{b}_0 - b_0) - (\hat{b}_0^2 - b_0^2)             \\
%               & = b_1 \delta_{b_1} + b_0 \delta_{b_0} - b_0 \delta_{b_0} (\hat{b}_0 + b_0).
% \end{align}
% %
% This yields the first-order error bounds:
% %
% \begin{align}
%   |\hat{f}_{xx} - f_{xx}|         & = | \frac{c - \hat{c}}{x^2} | \lesssim \frac{|b_1| |\delta_{b_1}| + |b_0| (1 + |\hat{b}_0 + b_0|) |\delta_{b_0}|}{x^2}     \\
%   |\hat{f}_{\nu\nu} - f_{\nu\nu}| & = | \frac{c - \hat{c}}{\nu^2} | \lesssim \frac{|b_1| |\delta_{b_1}| + |b_0| (1 + |\hat{b}_0 + b_0|) |\delta_{b_0}|}{\nu^2} \\
%   |\hat{f}_{x\nu} - f_{x\nu}|     & = | 2 (b_0 - \hat{b}_0) + \frac{\hat{b}_0^2 - b_0^2}{z} | \le 2 (|b_0| + \frac{|b_0||\hat{b}_0 + b_0|}{z}) |\delta_{b_0}|.
% \end{align}

\subsection{Third derivatives}\label{sec:rician-third-derivatives}

\paragraph{Small-$z$}

We begin by noting that the third derivative expressions in \cref{eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-nu-nu-nu} depend only on $r'$ and $r''$.
Recall the small-$z$ parametrizations for $r'$ and $r''$ from \cref{eq:r-prime-reparametrized,eq:r-second-derivative-reparametrized}:
%
\begin{align}
  r'                      & = 1 - a_0 - z^2 a_0^2 = \frac{1}{2} - \frac{3}{16}z^2 + \mathcal{O}(z^4)                          \\
  r''                     & = z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3 = -\frac{3}{8}z + \mathcal{O}(z^3)                     \\
  \Rightarrow 2r' + z r'' & = 2 (1 - a_0) + (a_0 (a_0 - 2) + 2 a_1) z^2 + 2 a_0^3 z^4 = 1 - \frac{3}{4}z^2 + \mathcal{O}(z^3)
\end{align}
%
where $a_0 = \frac{r}{z}$ and $a_1 = \frac{1}{z^2} (a_0 - \frac{1}{2})$.
We see that for small-$z$, the original simplified expressions in \cref{eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-nu-nu-nu}
%
\begin{align}
  \boxed{
    \begin{aligned}
      f_{xxx}       & = -\frac{2}{x^3} - \nu^3 r'' \\
      f_{xx\nu}     & = -\nu(2r' + z r'')          \\
      f_{x\nu\nu}   & = -x(2r' + z r'')            \\
      f_{\nu\nu\nu} & = -x^3 r''
    \end{aligned}
  }
\end{align}
%
are stable to compute, since neither $r''$ nor $2r' + z r''$ suffer cancellation issues.

\paragraph{Large-$z$}

For large $z$, the expression for $r''$ in \cref{eq:r-second-derivative-large-reparametrized} is stable.
The term $2r'+zr''$ is more delicate.
We derive a stable form from the identity $2r' + zr'' = r' + r/z - 2zrr'$ which follows from \cref{eq:r-prime-large-reparametrized-simplified,eq:r-second-derivative-large-reparametrized}.
Substituting $r=1-ub_0$ and $r'=u^2(b_1+b_0(1-b_0))$ gives
%
\begin{align}
  2r' + zr'' & = r' + r/z - 2zrr'                                              \\
             & = u^2 (b_1+b_0(1-b_0)) + u(1-ub_0) - 2u(1-ub_0)(b_1+b_0(1-b_0)) \\
             & = u ( u(b_1+b_0-b_0^2) + (1-ub_0)(1 - 2(b_1+b_0-b_0^2)) )
\end{align}
%
The term $1-2(b_1+b_0-b_0^2) = -\frac{1}{2}u + \mathcal{O}(u^2)$ is prone to cancellation issues which can be resolved by substituting $b_0=(1+ub_1)/2$ followed by $b_1=(1+ub_2)/4$:
%
\begin{align}
  1-2(b_1+b_0-b_0^2) = \frac{1-4b_1+u^2 b_1^2}{2} = -\frac{1}{2}u(b_2 - u b_1^2)
\end{align}
%
Substituting this back yields
%
\begin{align}
  2r' + zr'' & = u^2 \left( (b_1+b_0-b_0^2) + (1-ub_0)\frac{u b_1^2 - b_2}{2} \right)                                          \\
             & = \frac{1}{2} u^2 \left( (1-b_2) + u (b_2 (\frac{1}{2}+b_0 ) + b_1^2 ) - u^2 b_1^2 (\frac{1}{2} + b_0 ) \right)
\end{align}
%
but now, as anticipated in \cref{sec:rician-large-z}, the term $1-b_2$ causes a cancellation issue since $b_2 = 1 + \mathcal{O}(u)$.
Therefore, we make the final change of variables $b_2 = 1 + u b_3$, giving the numerically stable expression:
%
\begin{align}
  2r' + zr'' & = \frac{1}{2} u^2 \left( -u b_3 + u (b_2 (\frac{1}{2}+b_0 ) + b_1^2 \right) - u^2 b_1^2 ( \frac{1}{2} + b_0 )                                     \\
             & = \frac{1}{2} u^3 \left( b_2 (\frac{1}{2}+b_0 ) + b_1^2 - b_3 - u b_1^2 (\frac{1}{2} + b_0) \right). \label{eq:two-rprime-z-r-primeprime-large-z}
\end{align}
%
This expression is $\mathcal{O}(u^3)$ with no remaining cancellation issues.
All terms are computed from $b_0, b_1, b_2$, and $b_3$, which depend on a single minimax approximation for $b_3(u)$.

With these stable forms, the third derivatives for large $z$ are computed as:
\begin{align}\label{eq:third-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_{xxx}       & = -\frac{2}{x^3} - \nu^3 r''                                                                      \\
      f_{xx\nu}     & = -\frac{1}{2 x z^2} \left( b_1^2 - b_3 + (b_2 - \frac{1}{z} b_1^2) (\frac{1}{2} + b_0) \right)   \\
      f_{x\nu\nu}   & = -\frac{1}{2 \nu z^2} \left( b_1^2 - b_3 + (b_2 - \frac{1}{z} b_1^2) (\frac{1}{2} + b_0) \right) \\
      f_{\nu\nu\nu} & = -x^3 r''
    \end{aligned}
  }
\end{align}

\section{The Quantized Rician distribution}

In magnetic resonance imaging (MRI), the magnitude of the signal in the presence of noise is often modeled with a Gaussian distribution for simplicity.
However, a more physically accurate model is the Rician distribution, which correctly accounts for the non-negativity of magnitude data.
Furthermore, MRI data is invariably quantized during acquisition, meaning the continuous signal is stored as discrete integer values.
A truly faithful statistical model must therefore account for this quantization step.
This leads us to the Quantized Rician distribution, which describes the probability of observing a signal within a specific quantization bin.

Given the Rician probability density function $p(y \mid \nu, \sigma)$ from \cref{eq:rician-pdf}, the probability of an observation $y$ falling into a bin $[x, x+\delta]$ is given by the integral of the Rician density over the interval:
%
\begin{align}\label{eq:quantized-rician-pdf}
  p(x \mid \nu, \sigma, \delta) = \int_x^{x+\delta} p(y \mid \nu, \sigma) dy.
\end{align}
%
This defines the probability mass function for the Quantized Rician distribution, which we denote $\mathrm{QRice}(\nu, \sigma, \delta)$.

\subsection{Numerical evaluation strategy}

We are interested in performing statistical inference procedures such as maximum likelihood estimation (MLE), maximum a posteriori (MAP) estimation, and Markov-chain Monte Carlo (MCMC) sampling under a Quantized Rician likelihood model.
These methods require many evaluations of the likelihood and its gradient, necessitating fast and accurate evaluation of the defining integral in \cref{eq:quantized-rician-pdf}.
While adaptive quadrature routines can compute the integral to arbitrary precision, they are generally too slow for this context, especially for GPU-based implementations.
Our goal is to use a fixed-order, non-adaptive quadrature scheme which is significantly faster.

The suitability of such a scheme depends on the integration regime.
Integrating an exponentially decaying function over a large interval in its tails, for example, can be numerically catastrophic for a fixed-order rule.
Fortunately, the nature of statistical inference itself ensures that we predominantly operate in a numerically favourable regime when the noise scale $\sigma$ is a nuisance parameter which is to be estimated jointly with other model parameters.
Crucially, the inference algorithms operate in high-likelihood regions:
%
\begin{itemize}
  \item MLE and MAP explicitly drive the parameters $(\nu,\sigma)$ toward the mode of the likelihood or posterior.
  \item MCMC methods spend the most time sampling from regions of high posterior density, visiting the tails only rarely.
\end{itemize}
%
This behavior is made concrete by considering the high signal-to-noise ratio (SNR) regime, where the Rician distribution is well-approximated by a Gaussian.
For a Gaussian model, a property of the MLE is that the noise variance estimate equals the mean squared error:
%
\begin{align}
  \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \nu_i)^2.
\end{align}
%
This implies that for a well-fit model the residuals satisfy $|x_i - \nu_i| = \mathcal{O}(\sigma)$, i.e., the normalized residual is on the order of one, $|x' - \nu'| = \mathcal{O}(1)$.
Thus, inference naturally results in evaluation of the density near the mode.
Pathological cases where $|x'-\nu'| \gg 1$ correspond to regions of low likelihood that are comparatively less important;
coarse estimates suffice there.

This operating regime is further constrained by practical considerations from signal acquisition.
The signal-to-quantization-noise ratio (SQNR) is engineered to exceed the physical SNR, ensuring quantization error is a smaller source of variance than the physical noise of the signal.
Empirically, the ratio of maximum signal magnitude to quantization bin width is large ($\sim\!500$--$1000$), and with 16-bit signal magnitude quantization (Int16) the dynamic range allows a theoretical upper bound of about $2^{15} \approx 30000$.
Consequently, the signal-to-bin-width ratio typically exceeds the SNR, i.e. $x/\delta \gtrsim x/\sigma$, which implies that the normalized integration width satisfies $\delta' = \delta/\sigma \lesssim 1$.
Thus, we have reasonable justification for the following two assumptions about the model parameters:
%
\begin{enumerate}
  \item The observed magnitude $x$ is close to the noncentrality parameter $\nu$, i.e., $|x - \nu| \lesssim \sigma \Rightarrow |x' - \nu'| \lesssim 1$.
  \item The quantization width $\delta$ is not larger than the noise level $\sigma$, i.e. $\delta' = \delta / \sigma \lesssim 1$.
\end{enumerate}
%
Combined, these factors create a best-case scenario for numerical integration:
we are integrating a smooth, well-behaved density over a short interval located near its mode.
This justifies the use of a fixed-order quadrature rule.
As a safeguard, on the rare occasions where a poor parameter choice leads to evaluating the likelihood in the tails (i.e., $|x'-\nu'| \gg 1$), one could temporarily increase the quadrature order or fall back to an adaptive routine;
under normal inference, these cases are exceptional.

\subsection{Gauss-Legendre quadrature}

We choose Gauss-Legendre quadrature to approximate the likelihood integral.
On the canonical interval $[-1,1]$, the $N$‑point Gauss–Legendre rule approximates
%
\begin{align}
  \int_{-1}^{1} g(x) \, dx \approx \sum_{i=1}^{N} w_i g(x_i),
\end{align}
%
where $\{x_i\}_{i=1}^N$ are the Legendre nodes, defined as the roots of the $N^\text{th}$ Legendre polynomial $P_N(x)$, and $\{w_i\}_{i=1}^N$ the associated positive weights.
This rule is extremely powerful despite its simplicity:
it is exact for all polynomials of degree up to $2N-1$, and for analytic functions the approximation error decreases exponentially fast as $N$ increases.
The rule's fixed nodes and weights are easy to precompute, and the positivity of the weights ensures that no additional cancellation issues are introduced.
Furthermore, the weighted sum structure is trivial to differentiate, making it straightforward to compute gradients and Hessians of the integral approximation.

To apply the rule on $[0,1]$, use the affine change of variables $t = (x+1)/2$, transforming the nodes and weights to $t_i = (x_i+1)/2$ and $w_i/2$, respectively:
%
\begin{align}
  \int_{0}^{1} g(t) \, dt \approx \sum_{i=1}^{N} \frac{w_i}{2} g(t_i).
\end{align}
%
To integrate over $[x, x+\delta]$, change variables to $y = x + \delta t$ with $t\in[0,1]$ and $dy = \delta dt$:
%
\begin{align}
  \int_{x}^{x+\delta} g(y) \, dy = \int_{0}^{1} g(x + \delta t) \, \delta dt
  \approx \delta \sum_{i=1}^{N} \frac{w_i}{2} g(x + \delta t_i).
\end{align}
%
In our regime, $\delta$ is small and $g$ is smooth, so moderate orders $N \approx 8\!-\!16$ achieve high accuracy.

\subsection{Log-likelihood formulation and simplification}

Before deriving expressions for the gradient and Hessian, we first simplify the problem by exploiting the scale-invariance of the Quantized Rician distribution.
If we change variables in the integrand to $y' = y / a$ for some $a > 0$ and similarly rescale $x$, $\nu$, $\sigma$, and $\delta$, the Rician pdf transforms as
%
\begin{align}
  p(a y' \mid a \nu', a \sigma')
  = \frac{a y'}{(a \sigma')^2} \exp\left(-\frac{(a y')^2+(a \nu')^2}{2(a \sigma')^2}\right) I_0\left(\frac{a y' (a \nu')}{(a \sigma')^2}\right)
  = \frac{1}{a} p(y' \mid \nu', \sigma').
\end{align}
%
Thus, the quantized likelihood integral is scale-invariant:
%
\begin{align}
  p(x \mid \nu, \sigma, \delta)
  = \int_{x/a}^{(x+\delta)/a} p(a y' \mid a\nu', a\sigma') (a dy') = \int_{x'}^{x'+\delta'} p(y' \mid \nu', \sigma') dy'
  = p(x' \mid \nu', \sigma', \delta').
\end{align}
%
Similar to the Rician distribution analysis, we choose $a = \sigma$ such that $\sigma'=1$;
for the remainder of this analysis, we will work with these normalized parameters and write $p(x \mid \nu, \delta)$ as shorthand for $p(x \mid \nu, \sigma=1, \delta)$, dropping the prime annotations.

In terms of $f$, the negative log-likelihood of the standard Rician distribution from \cref{eq:rician-neg-log-likelihood}, the Quantized Rician likelihood is given by
%
\begin{align}
  p(x \mid \nu, \delta) = \int_x^{x+\delta} \exp(-f(y, \nu)) dy \coloneqq I(\theta)
\end{align}
%
where we have introduced a shorthand notation for the normalized parameters $\theta = (x, \nu, \delta)$ and the integral $I(\theta)$.
Similarly, we introduce $\Omega(\theta) = -\log I(\theta)$ for the negative logarithm of the quantized likelihood:
%
\begin{align}
  -\log p(x \mid \nu, \delta) = -\log I(\theta) \coloneqq \Omega(\theta).
\end{align}
%
We now make the affine change variables $y = x + \delta t$ such that the integration bounds are independent of $\theta$, resulting in an integral over the unit interval $t \in [0, 1]$:
%
\begin{align}
  I(\theta)                  & = \int_0^1 \exp(-f(x + \delta t, \nu)) \cdot \delta \, dt                    \\
  \Rightarrow \Omega(\theta) & = -\log\left(\int_0^1 \exp(-f(x + \delta t, \nu)) \, dt\right) - \log\delta.
\end{align}
%
This expression has the form of a log-partition function;
denote the energy function as $\tilde{f}(t, \theta) = f(x + \delta t, \nu)$ and the partition function as
%
\begin{align}
  Z(\theta) = \int_0^1 \exp(-\tilde{f}(t, \theta)) \, dt.
\end{align}
%
Then, $\Omega(\theta) = -\log Z(\theta) - \log\delta$, and we can define a probability density over $t \in [0,1]$ as
%
\begin{align}
  P(t \mid \theta) = \frac{\exp(-\tilde{f}(t, \theta))}{Z(\theta)}.
\end{align}
%
This formulation will prove useful for computing gradients and higher derivatives of $\Omega$.

\subsection{The residual likelihood method}

Since the quadrature scheme approximates the partition function $Z(\theta)$ as a weighted sum of positive terms, $\log Z(\theta)$ can be stably computed using the log-sum-exp trick:
%
\begin{align}
  -\log Z(\theta)
  = -\log\left(\sum_i \frac{w_i}{2} \exp(-\tilde{f}(t_i, \theta))\right)
  = \tilde{f}_{\min}(\theta) - \log\left(\sum_i \frac{w_i}{2} \exp(\tilde{f}_{\min}(\theta) - \tilde{f}(t_i, \theta))\right)
\end{align}
%
where $\tilde{f}_{\min}(\theta) = \min_i \tilde{f}(t_i, \theta)$, avoiding naive overflow.
However, a more subtle numerical issue arises at high SNR ($x \approx \nu$).
In this regime, the Rician negative log-likelihood $f$ is very nearly Gaussian:
%
\begin{align}
  f(x, \nu) \approx f_G(x,\nu) = \frac{(x-\nu)^2}{2} + \frac{1}{2}\log(2\pi).
\end{align}
%
In other words, the residual energy $\tilde{r}(t,\theta) = \tilde{f}(t,\theta) - f_G(x,\nu) = f(x + \delta t, \nu) - f_G(x,\nu)$ is relatively small.
Since both integration and Gauss-Legendre quadrature are linear functionals, we can factor $\exp(-f_G(x,\nu))$ out of the $Z(\theta)$ integral, resulting in a better-conditioned quadrature problem via the residual energy $\tilde{r}$:
%
\begin{align}
  \log Z(\theta)
  = - \log\left(\int_0^1 \exp(-(\tilde{r}(t,\theta) + f_G(x,\nu))) \, dt \right)
  = f_G(x,\nu) - \log\left(\int_0^1 \exp(-\tilde{r}(t,\theta)) \, dt \right).
\end{align}
%
This transformation is most beneficial at large $z$ but -- as we will see -- introduces no numerical issues at small $z$, so we apply it universally for simplicity.

\subsubsection{Properties of the residual energy}

Since $f_G$ is quadratic in $x$ and $\nu$ and independent of $\delta$, it follows from $\tilde{r} = \tilde{f} - f_G$ that third and higher-order partial derivatives of $\tilde{r}$ with respect to $x$ and $\nu$ are equal to those of $\tilde{f}$.
Similarly, any partial derivative of $\tilde{r}$ with respect to $\delta$ equals that of $\tilde{f}$.
We therefore need only derive stable expressions for $\tilde{r}$ and its first and second partial derivatives with respect to $x$ and $\nu$.
From \cref{eq:rician-neg-log-likelihood-scaled}, we have:
%
\begin{align}
  \tilde{r}(t,\theta) = f(y, \nu) - f_G(x,\nu) & = \frac{(y-\nu)^2 - (x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{y}{\nu}\right) - \log \hat{I}_0(y \nu)                                               \\
                                               & = \delta t \left(x - \nu + \frac{\delta t}{2}\right) - \frac{1}{2}\log\left(\frac{y}{\nu}\right) - \log \hat{I}_0(y \nu) \label{eq:residual-energy}
\end{align}
%
where $y = x + \delta t$.
The first term in \cref{eq:residual-energy} is small when $|x-\nu| \lesssim 1$ and $\delta \lesssim 1$.
The size of the second and third terms $\tilde{c}(y,\nu) = \frac{1}{2}\log(y/\nu) + \log\hat{I}_0(y\nu)$ depends on the regime of $z=y\nu$.

\paragraph{Small $z=y\nu$}

Rewriting $\tilde{c}(y,\nu) = \log y + \frac{1}{2}\log(2\pi) + \log I_0(z) - z$ and Taylor expanding $\log I_0(z)$, we have
%
\begin{align}\label{eq:c-small-z}
  \tilde{c}(y,\nu) = \log y + \frac{1}{2}\log(2\pi) - z + \frac{z^2}{4} + \mathcal{O}(z^4).
\end{align}

\paragraph{Large $z=y\nu$}

Using the asymptotic expansion $\log \hat{I}_0(z) = \frac{1}{8z} + \mathcal{O}(z^{-2})$, we have
%
\begin{align}\label{eq:c-large-z}
  \tilde{c}(y,\nu) = \frac{1}{2}\log\left(\frac{y}{\nu}\right) + \frac{1}{8z} + \mathcal{O}(z^{-2}).
\end{align}

\paragraph{High-SNR $\nu \approx y \gg 1$}

Let $y = x + \delta t = \nu + \epsilon$ with $|\epsilon| = |x - \nu + \delta t| \lesssim 1$ and $\nu$ large.
Then, expanding \cref{eq:c-large-z} to first order in $\epsilon/\nu \ll 1$ gives:
%
\begin{align}
  \tilde{c}(\nu+\epsilon, \nu) & = \frac{1}{2}\log\left(1 + \frac{\epsilon}{\nu}\right) + \frac{1}{8\nu(\nu+\epsilon)} + \mathcal{O}(z^{-2})                                                                                                   \\
                               & = \frac{\epsilon}{2\nu} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right) + \frac{1}{8\nu^2}\left(1 - \frac{\epsilon}{\nu} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right)\right) + \mathcal{O}(\nu^{-4}) \\
                               & = \frac{\epsilon}{2\nu}\left(1 - \frac{1}{4\nu^2}\right) + \frac{1}{8\nu^2} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right).
\end{align}
%
Thus, $\tilde{c}$ is small in the high-SNR regime and therefore $\tilde{r} = \delta t (\epsilon - \frac{\delta t}{2}) - \tilde{c}$ is also small when $\delta \lesssim 1$.

\subsubsection{First derivatives of the residual}\label{sec:first-derivatives-residual}

The first partial derivatives of $\tilde{r}$ can be simplified using \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu}:
%
\begin{align}
  \tilde{r}_x   & = f_x(y,\nu) - (x-\nu) = (y - \nu r - \frac{1}{y}) - (x-\nu) = \delta t + \nu (1 - r) - \frac{1}{y} \\
  \tilde{r}_\nu & = f_\nu(y,\nu) - (\nu-x) = (\nu - y r) - (\nu-x) = -\delta t r + x (1 - r)
\end{align}
%
These forms are stable for small $z = y \nu$, but for large $z$, we substitute $1-r = \frac{b_0}{z}$ from \cref{eq:r-large-reparametrized}:
%
\begin{align}
  \tilde{r}_x   & = \delta t + \nu (1 - r) - \frac{1}{y} = \delta t - \frac{1 - b_0}{y}           \\
  \tilde{r}_\nu & = -\delta t r + x (1 - r) = -\delta t + y (1 - r) = -\delta t + \frac{b_0}{\nu}
\end{align}

\subsubsection{Second derivatives of the residual}\label{sec:second-derivatives-residual}

The second partial derivatives of the $\tilde{r}$ can be simplified using \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu}:
%
\begin{align}
  \tilde{r}_{xx}     & = f_{xx} - 1 = \frac{1}{y^2} - \nu^2 r'          \\
  \tilde{r}_{x\nu}   & = f_{x\nu} + 1 = 1 - (r + z r') = 1 - z(1 - r^2) \\
  \tilde{r}_{\nu\nu} & = f_{\nu\nu} - 1 = - y^2 r'
\end{align}
%
Similar to the small- and large $z$ expressions in \cref{eq:second-derivatives-small-z,eq:second-derivatives-large-z}, the above expressions for $\tilde{r}_{xx}$ and $\tilde{r}_{\nu\nu}$ are numerically stable.
However, for large $z$, the mixed-derivative $\tilde{r}_{x\nu}$ requires some care, since $f_{x\nu} = -1 + \mathcal{O}(\frac{1}{z^2})$ and so computing $\tilde{r}_{x\nu} = \mathcal{O}(\frac{1}{z^2})$ naively leads to catastrophic cancellation.
A stable form can be derived by substituting the large-$z$ reparametrizations $1 - r = u b_0$, $2 b_0 - 1 = u b_1$, and $4 b_1 - 1 = u b_2$ where $u = \frac{1}{z}$:
%
\begin{align}
  \tilde{r}_{x\nu} & = 1 - z(1 - r)(1 + r) = 1 - b_0 (1 + r) = 1 - b_0(2 - u b_0)                       \\
                   & = (1 - 2 b_0) + u b_0^2 = -u b_1 + u b_0^2 = u (b_0^2 - b_1)                       \\
                   & = u ((\frac{1 + u b_1}{2})^2 - b_1) = \frac{u}{4} (1 - 4b_1 + 2u b_1 + u^2 b_1^2)  \\
                   & = \frac{u}{4}(-u b_2 + 2u b_1 + u^2 b_1^2) = \frac{u^2}{4}(2 b_1 - b_2 + u b_1^2).
\end{align}
%
This final expression is stable to compute and $\mathcal{O}(\frac{1}{z^2})$ as expected.

\subsubsection{Expectations over $P(t \mid \theta)$}\label{sec:expectation-covariance}

Since $f_G(x,\nu)$ is constant with respect to $t$, we can factor $\exp(-f_G(x,\nu))$ out of the numerator and denominator of $P(t \mid \theta)$, resulting in the more numerically stable form
%
\begin{align}
  P(t \mid \theta)
  = \frac{\exp(-\tilde{f}(t, \theta))}{\int_0^1 \exp(-\tilde{f}(t', \theta)) \, dt'}
  = \frac{\exp(-\tilde{r}(t, \theta))}{\int_0^1 \exp(-\tilde{r}(t', \theta)) \, dt'}.
\end{align}
%
In the high SNR regime with $\delta$ small such that $\tilde{r}$ is small, we can compute $\log P$ in a more stable form:
%
\begin{align}
  \log P(t \mid \theta)
   & = -\log \left( \int_0^1 \exp(\tilde{r}(t, \theta) - \tilde{r}(t', \theta)) \, dt' \right)        \\
   & = -\log \left(1 + \int_0^1 \exp(\tilde{r}(t, \theta) - \tilde{r}(t', \theta)) - 1 \, dt' \right)
\end{align}
%
where $\log(1 + \epsilon)$ and $\exp(\epsilon - 1)$ are computed stably for small $\epsilon$ using the special-function routines \texttt{log1p($\epsilon$)} and \texttt{expm1($\epsilon$)}.

\subsubsection{Differentiating expectations over $P(t \mid \theta)$}\label{sec:jacobian-expectation-covariance}

Let $g : [0, 1] \times \mathbb{R}^3 \to \mathbb{R}^m$, and denote as $\mathbb{E}[g] \in \mathbb{R}^m$ the expectation of $g(t, \theta)$ with respect to the density $P(t \mid \theta)$.
The Jacobian $\nabla_\theta \mathbb{E}[g] \in \mathbb{R}^{m \times 3}$ is given by
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{f}) \label{eq:gradient-expectation-covariance}          \\
                              & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{r}) \label{eq:gradient-expectation-covariance-residual}
\end{align}
%
where, denoting $\mu_X = \mathbb{E}[X]$ and $\mu_Y = \mathbb{E}[Y]$, the covariance is defined as usual as
%
\begin{align}
  \mathrm{Cov}(X, Y) = \mathbb{E}[(X - \mu_X)(Y - \mu_Y)^T] = \mathbb{E}[X Y^T] - \mu_X \mu_Y^T.
\end{align}
%
\Cref{eq:gradient-expectation-covariance,eq:gradient-expectation-covariance-residual} follow from differentiating the definition of the expectation $\mathbb{E}[g] = \int_0^1 g(t, \theta) P(t \mid \theta) \, dt$.
Crucially, \cref{eq:gradient-expectation-covariance,eq:gradient-expectation-covariance-residual} also hold exactly when the expectation is computed via a fixed-node quadrature, $\mathbb{E}[g] \approx \sum_i \frac{w_i}{2} g(t_i, \theta) P(t_i \mid \theta)$, since both integration and quadrature are linear functionals of the integrand $g(\cdot,\theta) P(\cdot \mid \theta)$ and thus commute with differentiation.

\paragraph{Proof of \cref{eq:gradient-expectation-covariance}}

We start with the definition of the expectation and apply the product rule:
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \nabla_\theta \left( Z(\theta)^{-1} \int_0^1 g(t,\theta) \exp(-\tilde{r}(t,\theta)) \, dt \right)                                            \\
                              & = \left(\int_0^1 g e^{-\tilde{r}} \, dt\right) (\nabla_\theta Z^{-1})^T + Z^{-1} \nabla_\theta \left( \int_0^1 g e^{-\tilde{r}} \, dt \right).
\end{align}
%
The first term simplifies using the identity $\nabla_\theta Z^{-1} = -Z^{-1} \nabla_\theta \log Z$:
%
\begin{align}
  \left(Z \mathbb{E}[g]\right) \left(-Z^{-1} \nabla_\theta \log Z\right)^T = -\mathbb{E}[g] (\nabla_\theta \log Z)^T.
\end{align}
%
For the second term, we move the derivative inside the integral and apply the product rule again:
%
\begin{align}
  Z^{-1} \int_0^1 \nabla_\theta(g e^{-\tilde{r}}) \, dt & = Z^{-1} \int_0^1 \left( (\nabla_\theta g) e^{-\tilde{r}} - g (\nabla_\theta \tilde{r})^T e^{-\tilde{r}} \right) \, dt \\
                                                        & = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^T].
\end{align}
%
Combining these results gives
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^T] - \mathbb{E}[g](\nabla_\theta \log Z)^T.
\end{align}
%
To simplify the final term, we find the gradient of the log-partition function:
%
\begin{align}
  \nabla_\theta \log Z = \frac{\nabla_\theta Z}{Z} = Z^{-1} \int_0^1 \nabla_\theta e^{-\tilde{r}} \, dt = -Z^{-1} \int_0^1 \nabla_\theta \tilde{r} e^{-\tilde{r}} \, dt = -\mathbb{E}[\nabla_\theta \tilde{r}].
\end{align}
%
Substituting this back yields the final identity:
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^T] - \mathbb{E}[g] (-\mathbb{E}[\nabla_\theta \tilde{r}])^T \\
                              & = \mathbb{E}[\nabla_\theta g] - (\mathbb{E}[g (\nabla_\theta \tilde{r})^T] - \mathbb{E}[g] \mathbb{E}[\nabla_\theta \tilde{r}]^T)  \\
                              & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{r}).
\end{align}
%
This proof relies only on the linearity of differentiation and integration (or finite summation for quadrature), so the identity holds for both the continuous integral and its discrete quadrature approximation.

\subsubsection{Differentiating covariances with respect to $P(t \mid \theta)$}\label{sec:derivative-covariance}

For any two scalar functions $g(t,\theta)$ and $h(t,\theta)$, the derivative of their covariance with respect to a parameter $\gamma \in \{x, \nu, \delta\}$ is given by
%
\begin{align}\label{eq:derivative-covariance-identity}
  \partial_{\gamma} \mathrm{Cov}(g, h) = \mathrm{Cov}(\partial_{\gamma} g, h) + \mathrm{Cov}(g, \partial_{\gamma} h) - \mathrm{Cov3}(g, h, \partial_{\gamma} \tilde{r}),
\end{align}
%
where $\mathrm{Cov3}(X,Y,Z) = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])(Z-\mathbb{E}[Z])]$ is the third joint central moment.

\paragraph{Proof of \cref{eq:derivative-covariance-identity}}

We start from the definition of covariance and apply the product rule:
%
\begin{align}
  \partial_{\gamma} \mathrm{Cov}(g, h) & = \partial_{\gamma} \left( \mathbb{E}[g h] - \mathbb{E}[g]\mathbb{E}[h] \right) = \partial_{\gamma} \mathbb{E}[g h] - (\partial_{\gamma} \mathbb{E}[g]) \mathbb{E}[h] - \mathbb{E}[g] (\partial_{\gamma} \mathbb{E}[h]).
\end{align}
%
Next, we apply the gradient-of-expectation identity from \cref{eq:gradient-expectation-covariance-residual} to each of the three derivative terms:
%
\begin{align}
  \partial_{\gamma} \mathbb{E}[g h] & = \mathbb{E}[\partial_{\gamma}(g h)] - \mathrm{Cov}(g h, \partial_{\gamma} \tilde{r}) = \mathbb{E}[(\partial_{\gamma} g) h + g (\partial_{\gamma} h)] - \mathrm{Cov}(g h, \partial_{\gamma} \tilde{r}) \\
  \partial_{\gamma} \mathbb{E}[g]   & = \mathbb{E}[\partial_{\gamma} g] - \mathrm{Cov}(g, \partial_{\gamma} \tilde{r})                                                                                                                       \\
  \partial_{\gamma} \mathbb{E}[h]   & = \mathbb{E}[\partial_{\gamma} h] - \mathrm{Cov}(h, \partial_{\gamma} \tilde{r}).
\end{align}
%
Substituting these back into the main expression and grouping terms yields:
%
\begin{align}
  \partial_{\gamma} \mathrm{Cov}(g, h) & = \mathbb{E}[(\partial_{\gamma} g) h] - \mathbb{E}[\partial_{\gamma} g] \mathbb{E}[h] + \mathbb{E}[g (\partial_{\gamma} h)] - \mathbb{E}[g] \mathbb{E}[\partial_{\gamma} h]           \\
                                       & - \mathrm{Cov}(g h, \partial_{\gamma} \tilde{r}) + \mathrm{Cov}(g, \partial_{\gamma} \tilde{r}) \mathbb{E}[h] + \mathbb{E}[g] \mathrm{Cov}(h, \partial_{\gamma} \tilde{r}). \nonumber
\end{align}
%
The term in the parenthesis is the third joint central moment, or third-order covariance, of $(g, h, \partial_{\gamma} \tilde{r})$.
Expanding the definition of the third central moment confirms its equivalence to this term:
%
\begin{align}
  \mathrm{Cov3}(g, h, \partial_{\gamma} \tilde{r}) & = \mathbb{E}[(g-\mathbb{E}[g])(h-\mathbb{E}[h])(\partial_{\gamma} \tilde{r}-\mathbb{E}[\partial_{\gamma} \tilde{r}])]                                                                                                                                                                           \\
                                                   & = \mathbb{E}[(gh - g\mathbb{E}[h] - h\mathbb{E}[g] + \mathbb{E}[g]\mathbb{E}[h])(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])]                                                                                                                                        \\
                                                   & = \mathbb{E}[gh(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] - \mathbb{E}[h]\mathbb{E}[g(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] - \mathbb{E}[g]\mathbb{E}[h(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] \\
                                                   & = \mathrm{Cov}(gh, \partial_{\gamma} \tilde{r}) - \mathbb{E}[h]\mathrm{Cov}(g, \partial_{\gamma} \tilde{r}) - \mathbb{E}[g]\mathrm{Cov}(h, \partial_{\gamma} \tilde{r}).
\end{align}
%
Substituting this back gives the final expression for the derivative of the covariance.

\subsection{Residual gradient}\label{sec:qrician-residual-gradient}

We compute the gradient of $\Omega = -\log Z - \log\delta$ in terms of the gradient of $-\log Z$:
%
\begin{align}
  \nabla_{\theta} (-\log Z) = -\frac{\nabla_{\theta} Z}{Z} = -\frac{1}{Z}\int_0^1 -\nabla_{\theta} \tilde{f} \cdot \exp(-\tilde{f}) \, dt = \mathbb{E}[\nabla_{\theta} \tilde{f}].
\end{align}
%
Rewriting the expectation in terms of the residual energy $\tilde{r}$, we have
%
\begin{align}
  \mathbb{E}[\nabla_{\theta} \tilde{f}] = \mathbb{E}[\nabla_{\theta} (\tilde{r} + f_G)] = \mathbb{E}[\nabla_{\theta} \tilde{r}] + \nabla_{\theta} f_G
\end{align}
%
since $f_G$ is constant with respect to $t$.
Finally, the gradient $\nabla_{\theta} \Omega$ simplifies to
%
\begin{align}
  \nabla_{\theta} \Omega & = - \nabla_\theta \log Z - \nabla_\theta \log\delta = \mathbb{E}[\nabla_{\theta} \tilde{f}] - (0, 0, \delta^{-1})^T \\
                         & = \mathbb{E}\left[ \begin{pmatrix} \tilde{r}_x \\ \tilde{r}_\nu \\ t \tilde{f}_x \end{pmatrix} \right]
  + \begin{pmatrix} x - \nu \\ \nu - x \\ -\delta^{-1} \end{pmatrix}
\end{align}
%
where $\frac{\partial}{\partial x} f_G = x - \nu$, $\frac{\partial}{\partial \nu} f_G = \nu - x$, and $\tilde{r}_x$, $\tilde{r}_\nu$, and $f_x$ are evaluated at $y=x+\delta t$ as in \cref{sec:first-derivatives-residual}.

\subsection{Residual Hessian}\label{sec:qrician-residual-hessian}

We can compute the Hessian of $\Omega = -\log Z - \log\delta$ by differentiating $\nabla_{\theta} (-\log Z)$ from \cref{sec:qrician-residual-gradient} using the gradient of the expectation identity \cref{eq:gradient-expectation-covariance}:
%
\begin{align}
  \nabla_{\theta}^2 (-\log Z) = \nabla_\theta ( \mathbb{E}[\nabla_{\theta} \tilde{f}] ) = \mathbb{E}[\nabla_{\theta}^2 \tilde{f}] - \mathrm{Cov}(\nabla_{\theta} \tilde{f}, \nabla_{\theta} \tilde{f})
\end{align}
%
Substituting $\tilde{f} = \tilde{r} + f_G$ and recalling that $f_G$ is constant with respect to $t$, we have
%
\begin{align}
  \nabla_{\theta}^2 (-\log Z)                                                     & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r} + \nabla_{\theta}^2 f_G] - \mathrm{Cov}(\nabla_{\theta} \tilde{r} + \nabla_{\theta} f_G, \nabla_{\theta} \tilde{r} + \nabla_{\theta} f_G)  \\
                                                                                  & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r}] + \nabla_{\theta}^2 f_G - \mathrm{Cov}(\nabla_{\theta} \tilde{r}, \nabla_{\theta} \tilde{r}) \label{eq:residual-hessian-neglogz}          \\
  \Rightarrow \nabla_{\theta}^2 \Omega = \nabla_{\theta}^2 (-\log Z - \log\delta) & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r}] - \mathrm{Cov}(\nabla_{\theta} \tilde{r}, \nabla_{\theta} \tilde{r}) + \begin{pmatrix} 1 & -1 & 0 \\ -1 & 1 & 0 \\ 0 & 0 & \delta^{-2} \end{pmatrix} \label{eq:residual-hessian-qrician}
\end{align}
%
where $\frac{\partial^2}{\partial x^2} f_G = \frac{\partial^2}{\partial \nu^2} f_G = 1$, $\frac{\partial^2}{\partial x \partial \nu} f_G = -1$.
The Hessian of the residual energy $\nabla_{\theta}^2 \tilde{r}$ is
%
\begin{align}
  \nabla_{\theta}^2 \tilde{r} = \begin{pmatrix} \tilde{r}_{xx} & \tilde{r}_{x\nu} & t \tilde{f}_{xx} \\ \tilde{r}_{x\nu} & \tilde{r}_{\nu\nu} & t \tilde{f}_{x\nu} \\ t \tilde{f}_{xx} & t \tilde{f}_{x\nu} & t^2 f_{xx} \end{pmatrix}
\end{align}
%
where $\tilde{r}_{xx}$, $\tilde{r}_{x\nu}$, and $\tilde{r}_{\nu\nu}$ are evaluated at $y=x+\delta t$ as in \cref{sec:second-derivatives-residual}.

\section{Third-Order Derivatives and Vector-Jacobian Products}

For applications requiring differentiation through statistical inference procedures, it may be necessary to compute the derivatives of the gradient and Hessian of the log-likelihood.
We define a single vector containing these quantities:
%
\begin{align}
  \Phi(\theta) = \left( \nabla_\theta \Omega(\theta), \mathrm{vech}(\nabla_\theta^2 \Omega(\theta)) \right)^T \in \mathbb{R}^9,
\end{align}
%
where $\theta = (x, \nu, \delta)$ and $\mathrm{vech}$ stacks the lower–triangular elements of the symmetric Hessian.
This section details the computation of the Jacobian of $\Phi$ and the corresponding vector-Jacobian product (VJP).

\subsection{Third derivatives of $\Omega$}

The Jacobian of $\Phi$ contains the Hessian of $\Omega = -\log Z - \log\delta$ in its first three rows and the third-order partial derivatives of $\Omega$ in its last six rows.
We derive the third derivatives by differentiating the Hessian $\nabla_{\theta}^2 (-\log Z)$ from \cref{eq:residual-hessian-neglogz} with respect to a parameter $\gamma \in \{x, \nu, \delta\}$:
%
\begin{align}
  \partial_{\gamma}(\nabla_{\theta}^2 (-\log Z)) = \partial_{\gamma} \mathbb{E}[\nabla_\theta^2 \tilde{r}] - \partial_{\gamma}\mathrm{Cov}(\nabla_\theta \tilde{r}, \nabla_\theta \tilde{r}).
\end{align}
%
The components of the third-derivative tensor follow by applying \cref{eq:gradient-expectation-covariance-residual,eq:derivative-covariance-identity} to differentiate the expectation and covariance terms:
%
\begin{align}
  \partial_{\gamma} \mathbb{E}[\partial_{\alpha\beta} \tilde{r}]                         & = \mathbb{E}[\partial_{\alpha\beta\gamma} \tilde{r}] - \mathrm{Cov}(\partial_{\alpha\beta} \tilde{r}, \partial_{\gamma} \tilde{r})                                                                                                                                                                                                                                                                                                       \\
  \partial_{\gamma}\mathrm{Cov}(\partial_{\alpha} \tilde{r}, \partial_{\beta} \tilde{r}) & = \mathrm{Cov}(\partial_{\alpha\gamma} \tilde{r}, \partial_{\beta} \tilde{r}) + \mathrm{Cov}(\partial_{\alpha} \tilde{r}, \partial_{\beta\gamma} \tilde{r}) - \mathrm{Cov3}(\partial_{\alpha} \tilde{r}, \partial_{\beta} \tilde{r}, \partial_{\gamma} \tilde{r})                                                                                                                                                                        \\
  \Rightarrow \partial_{\alpha\beta\gamma} (-\log Z)                                     & = \mathbb{E}[\partial_{\alpha\beta\gamma} \tilde{r}] - \mathrm{Cov}(\partial_{\alpha\beta} \tilde{r}, \partial_{\gamma} \tilde{r}) - \mathrm{Cov}(\partial_{\alpha\gamma} \tilde{r}, \partial_{\beta} \tilde{r}) - \mathrm{Cov}(\partial_{\beta\gamma} \tilde{r}, \partial_{\alpha} \tilde{r}) + \mathrm{Cov3}(\partial_{\alpha} \tilde{r}, \partial_{\beta} \tilde{r}, \partial_{\gamma} \tilde{r}) \label{eq:neglogz-third-derivative} \\
  \Rightarrow \partial_{\alpha\beta\gamma} \Omega                                        & = \partial_{\alpha\beta\gamma} (-\log Z) - 2\delta^{-3} \mathbf{1}_{\{\alpha=\beta=\gamma=\delta\}}. \label{eq:omega-third-derivative-qrician}
\end{align}
%
We now discuss two strategies for the efficient evaluation of $\partial_{\alpha\beta\gamma}(-\log Z)$.

\paragraph{Raw-Moment Formulation}

We rewrite the covariances and third central moments in terms of raw moments.
Let $\mu_{\alpha} = \mathbb{E}[\partial_{\alpha} \tilde{r}]$, $\mu_{\alpha\beta} = \mathbb{E}[\partial_{\alpha} \tilde{r} \, \partial_{\beta} \tilde{r}]$, and $\mu_{\alpha\beta\gamma} = \mathbb{E}[\partial_{\alpha} \tilde{r} \, \partial_{\beta} \tilde{r} \, \partial_{\gamma} \tilde{r}]$.
Then,
%
\begin{align}
  \mathrm{Cov}(\partial_{\alpha\beta} \tilde{r}, \partial_{\gamma} \tilde{r})                         & = \mathbb{E}[\partial_{\alpha\beta} \tilde{r} \, \partial_{\gamma} \tilde{r}] - \mathbb{E}[\partial_{\alpha\beta} \tilde{r}] \, \mu_{\gamma}                                                                                                                                        \\
  \mathrm{Cov3}(\partial_{\alpha} \tilde{r}, \partial_{\beta} \tilde{r}, \partial_{\gamma} \tilde{r}) & = \mu_{\alpha\beta\gamma} - \mu_{\alpha} \, \mu_{\beta\gamma} - \mu_{\beta} \, \mu_{\alpha\gamma} - \mu_{\gamma} \, \mu_{\alpha\beta} + 2 \mu_{\alpha} \mu_{\beta} \mu_{\gamma}                                                                                                     \\
  \Rightarrow \partial_{\alpha\beta\gamma} (-\log Z)                                                  & = \mathbb{E}[\partial_{\alpha\beta\gamma} \tilde{r}] + \mu_{\alpha\beta\gamma} - \mu_{\alpha} \, \mu_{\beta\gamma} - \mu_{\beta} \, \mu_{\alpha\gamma} - \mu_{\gamma} \, \mu_{\alpha\beta} + 2 \mu_{\alpha} \mu_{\beta} \mu_{\gamma} \label{eq:neglogz-third-derivative-raw-moment} \\
                                                                                                      & - \mathbb{E}[\partial_{\alpha\beta} \tilde{r} \, \partial_{\gamma} \tilde{r}] - \mathbb{E}[\partial_{\alpha\gamma} \tilde{r} \, \partial_{\beta} \tilde{r}] - \mathbb{E}[\partial_{\beta\gamma} \tilde{r} \, \partial_{\alpha} \tilde{r}] \nonumber                                 \\
                                                                                                      & + \mathbb{E}[\partial_{\alpha\beta} \tilde{r}] \, \mu_{\gamma} + \mathbb{E}[\partial_{\alpha\gamma} \tilde{r}] \, \mu_{\beta} + \mathbb{E}[\partial_{\beta\gamma} \tilde{r}] \, \mu_{\alpha}. \nonumber
\end{align}
%
For a three-parameter system, a minimal basis of raw moments requires integrating 25 unique quantities.
The derivatives $\partial_{\alpha\beta\gamma} (-\log Z)$ are then assembled from sums and products of these raw moments.

\paragraph{Centred-Moment Formulation}

Let the centred first derivatives be given by $c_{\alpha} = \partial_{\alpha} \tilde{r} - \mu_{\alpha}$ where $\mu_{\alpha} = \mathbb{E}[\partial_{\alpha} \tilde{r}]$.
Then, $\partial_{\alpha\beta\gamma}(-\log Z)$ can be rewritten as an expectation:
%
\begin{align}
  \mathrm{Cov3}(\partial_{\alpha} \tilde{r}, \partial_{\beta} \tilde{r}, \partial_{\gamma} \tilde{r}) & = \mathbb{E}[c_{\alpha} c_{\beta} c_{\gamma}]                                                                                                                                                                                                                                            \\
  \mathrm{Cov}(\partial_{\alpha\beta} \tilde{r}, \partial_{\gamma} \tilde{r})                         & = \mathbb{E}[\partial_{\alpha\beta} \tilde{r} \, (\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] = \mathbb{E}[\partial_{\alpha\beta} \tilde{r} \, c_{\gamma}]                                                                                                   \\
  \Rightarrow \partial_{\alpha\beta\gamma} (-\log Z)                                                  & = \mathbb{E} [ \partial_{\alpha\beta\gamma} \tilde{r} - \partial_{\alpha\beta} \tilde{r} \, c_{\gamma} - \partial_{\alpha\gamma} \tilde{r} \, c_{\beta} - \partial_{\beta\gamma} \tilde{r} \, c_{\alpha} + c_{\alpha} c_{\beta} c_{\gamma} ]. \label{eq:neglogz-two-pass-centred-moment}
\end{align}
%
After computing the 3 means, this requires integrating 6 unique second-order and 10 unique third-order derivatives.
While this method requires evaluating first-derivatives twice, the linearity of the expectation in \cref{eq:neglogz-two-pass-centred-moment} can make for more efficient vector-Jacobian products than \cref{eq:neglogz-third-derivative-raw-moment} because contraction with a vector $v$ can be moved inside the expectation (i.e. $v^T \mathbb{E}[X] = \mathbb{E}[v^T X]$), resulting in fewer scalar integrands.

\subsection{Vector-Jacobian product of $\Phi$}

In the context of reverse-mode automatic differentiation, we require a function that computes the vector-Jacobian product (VJP) of $\Phi(\theta)$.
Given an incoming sensitivity vector $\Delta_\Phi \in \mathbb{R}^9$, the VJP is
%
\begin{align}
  (\nabla_\theta \Phi)^T \Delta_\Phi \in \mathbb{R}^3.
\end{align}
%
Let $\Delta_g = (\Delta_{\Phi,1}, \Delta_{\Phi,2}, \Delta_{\Phi,3})^T$ be the sensitivity for the gradient components of $\Phi$, and let $\Delta^H$ be the $3 \times 3$ symmetric matrix formed by un-vectorizing the remaining six elements of $\Delta_\Phi$.
The VJP can be split into two parts corresponding to the gradient and Hessian components of $\Phi$:
%
\begin{align}
  (\nabla_\theta \Phi)^T \Delta_\Phi = (\nabla_\theta^2 \Omega)^T \Delta_g + (\nabla_\theta (\mathrm{vech}(\nabla_\theta^2 \Omega)))^T \mathrm{vech}(\Delta^H).
\end{align}
%
The first term is simply $(\nabla_\theta^2 \Omega)^T \Delta_g = H \Delta_g$, since the Hessian $H$ is symmetric.
The second term is a contraction of the third-derivative tensor with the sensitivity matrix $\Delta^H$.
Its $k$-th component is given by the sum over the unique elements of the Hessian:
%
\begin{align}
  \left[ (\nabla_\theta (\mathrm{vech}(H)))^T \mathrm{vech}(\Delta^H) \right]_k
  = \sum_{1 \le i \le j \le 3} (\partial_k H_{ij}) (\Delta^H)_{ij}
  = \sum_{1 \le i \le j \le 3} \partial_{ijk}\Omega \cdot (\Delta^H)_{ij}.
\end{align}
%
Thus, the full VJP is given by
%
\begin{align}\label{eq:vjp-full}
  \left( (\nabla_\theta \Phi)^T \Delta_\Phi \right)_k = (H \Delta_g)_k + \sum_{1 \le i \le j \le 3} \partial_{ijk}\Omega \cdot (\Delta^H)_{ij},
\end{align}
%
which can be computed by first forming the Hessian and third-derivative tensor of $\Omega$ using quadrature and then performing the matrix-vector products and summations.

\section{Application: Differentiating Through Laplace's Approximation}

In Type-II maximum likelihood frameworks, hyperparameters are tuned by optimizing the model evidence \citep{mackay_bayesian_1992}.
Using the Laplace approximation (LA) of the evidence, this optimization requires backpropagating through the posterior's maximum a posteriori (MAP) estimate and its curvature; while this naively introduces third-order derivative tensors, the process can be made practical via efficient adjoint methods \citep{margossian_hamiltonian_2020}.
LA applications include Bayesian inference in latent Gaussian models, Variational Laplace Autoencoders, and end-to-end learning of data-augmentation invariances in deep networks \citep{margossian_hamiltonian_2020, park_variational_2019, immer_invariance_2022}.
For large-scale deep learning, tractability requires either replacing the full Hessian with approximations like Kronecker-factored structures or the generalized Gauss-Newton matrix \citep{ritter_scalable_2018, immer_improving_2021}, or applying the LA only to a subset of parameters.
A special case of this latter approach, \textit{last-layer} LA, applies the LA to a network's final linear layer \citep{kristiadi_being_2020, daxberger_laplace_2021}.
This motivates our setting: a small, two-dimensional LA over nuisance parameters is embedded within a larger variational posterior approximation.

\subsection{MRI Problem Formulation}

In a common MRI setting, magnitude data $x = (x_1, \dots, x_N)^T$ arise by taking the magnitude of complex measurements corrupted by Gaussian noise with variance $\sigma^2$, yielding Rician-distributed signals.
These signals are then quantized;
for example, stored as 16-bit integers with a scale factor $\delta$ to convert the compressed format back to physical units.
Suppose the measured signal $x_i$ is modeled as $s \cdot \mu_i(\lambda)$ where $\mu(\lambda)$ is a normalized signal model, $\lambda$ are the physical parameters of interest, and $s$ is a scaling factor.
The simplest such model is mono-exponential $T_2$ relaxation wherein $\lambda$ is the transverse relaxation time $T_2$, $\mu_i(\lambda) = \exp(-t_i/T_2)$ is the attenuation at echo time $t_i$, and $s$ is the initial signal amplitude.
Assuming the measurements are independent, this family of models results in the factorized likelihood:
%
\begin{align}\label{eq:likelihood}
  p(x \mid \lambda, \eta, \delta) = \prod_{i=1}^N p_{\mathrm{QRice}}(x_i \mid \nu = s \mu_i(\lambda), \sigma, \delta).
\end{align}

We consider a Bayesian setting where the goal is to infer the physical parameters $\lambda$ while marginalizing out the nuisance parameters $\eta = (\log s, \log \sigma)^T$.
Following \cref{eq:likelihood}, we write the negative log-likelihood as
%
\begin{align}\label{eq:negative-log-likelihood}
  L(\lambda, \eta; x, \delta)              & \coloneqq -\log p(x \mid \lambda, \eta, \delta) = \sum_{i=1}^N \ell(\xi^{(i)}(\lambda), \eta)                                                                      \\
  \text{where} \quad \ell(\xi^{(i)}, \eta) & \coloneqq -\log p_{\mathrm{QRice}}(x_i \mid \nu = s \mu_i, \sigma, \delta) = \Omega\left(\frac{x_i}{\sigma}, \frac{s \mu_i}{\sigma}, \frac{\delta}{\sigma}\right), \\
  \xi^{(i)}(\lambda)                       & \coloneqq (x_i, \mu_i(\lambda), \delta),
\end{align}
%
and $\Omega$ is the negative log-likelihood function for the quantized Rician distribution.
The joint density is then
%
\begin{align}
  p(x, \lambda, \eta \mid \delta) & = p(\lambda, \eta) p(x \mid \lambda, \eta, \delta) = e^{-U(\lambda, \eta; x, \delta)} \\
  \quad \text{where} \quad
  U(\lambda, \eta; x, \delta)     & \coloneqq L(\lambda, \eta; x, \delta) - \log p(\lambda, \eta)
\end{align}
%
where we assume a factorized prior $p(\lambda, \eta) = p(\lambda) p(\eta)$.

\subsection{Hybrid Variational Inference with Laplace's Approximation}

Suppose a hybrid variational inference scheme is employed wherein the posterior $p(\lambda, \eta \mid x, \delta)$ is modeled by a factorized distribution $q(\lambda, \eta) = q(\lambda) q(\eta \mid \lambda)$.
Let $q(\lambda)$ be a parametric approximation to the marginal posterior $p(\lambda \mid x, \delta)$ from a chosen variational family, and $q(\eta \mid \lambda)$ be a Gaussian surrogate for the conditional posterior $p(\eta \mid \lambda, x, \delta)$ computed using Laplace's method:
%
\begin{align}
  p(\eta \mid \lambda, x, \delta) \approx q(\eta \mid \lambda) & = \mathcal{N}(\eta \mid \hat{\eta}(\xi(\lambda)), \hat{H}(\xi(\lambda))^{-1}) \\
  \text{with} \quad
  \hat{\eta}(\xi)                                              & = \arg\min_{\eta} U(\xi, \eta) \label{eq:laplace-mode}                        \\
  \hat{H}(\xi)                                                 & = H(\xi, \hat{\eta}(\xi)) \label{eq:laplace-hessian}
\end{align}
%
where $\xi(\lambda) = (\mu(\lambda), x, \delta)$ groups all variables that the conditional posterior $p(\eta \mid \cdot)$ depends on, $\hat{\eta}(\xi)$ is the mode of $U(\xi, \cdot)$, and $H(\xi, \eta) \coloneqq \frac{\partial^2 U}{\partial \eta^2}$ is the Hessian of $U(\xi, \cdot)$.

The parameters $\psi$ of the variational family for $q(\lambda)$ are tuned via gradient-based optimization of an objective $J$.
This will require differentiating through the Laplace approximation since $J$ will be a function of $\hat{\eta}(\xi(\lambda))$ and $\hat{H}(\xi(\lambda))$.
For instance, if $J = \mathbb{E}_{q(\lambda, \eta)} \left[\log \frac{p(x, \lambda, \eta \mid \delta)}{q(\lambda, \eta)}\right]$ is the evidence lower bound, the gradient $\nabla_\psi$ flows through $\hat{\eta}$ and $\hat{H}$ via two mechanisms:
1) the entropy term $-\log q(\lambda, \eta)$ has a direct contribution from $-\log q(\eta \mid \lambda) = \tfrac{1}{2}\log\det(2\pi\hat{H}^{-1}) + \tfrac{1}{2} \lVert \hat{H}^{1/2} (\eta - \hat{\eta}) \rVert^2$;
and 2) if $q(\lambda)$ is reparameterizable (i.e. $\lambda \sim q(\lambda) \Leftrightarrow \lambda = g(\psi, \epsilon)$ where $\epsilon \sim p(\epsilon)$ with $p(\epsilon)$ fixed), then unbiased gradients of expectations of the form $\mathbb{E}_{q(\lambda, \eta)}[f(\lambda, \eta)]$ can be computed using the reparameterization trick: $\nabla_\psi \mathbb{E}_{q(\lambda, \eta)}[f(\lambda, \eta)] = \mathbb{E}_{\epsilon \sim p(\epsilon), z \sim \mathcal{N}(0, I)}[\nabla_\psi f(g(\psi, \epsilon), \hat{\eta} + \hat{H}^{-1/2} z)]$.
Therefore, to compute $\nabla_\psi J$ using reverse-mode automatic differentiation (AD) we must provide \textit{pullback} functions to pass gradients through \cref{eq:laplace-mode,eq:laplace-hessian}.

\subsection{Pullbacks for the Laplace Approximation}

Since $\hat{H}(\xi) = H(\xi, \hat{\eta}(\xi))$, we need only provide pullbacks for the implicit map $\xi \mapsto \hat{\eta}$ and the explicit map $(\xi, \eta) \mapsto H$; the pullback for $\hat{H}$ is constructed by the AD engine by composing the pullbacks for these two maps.
The pullback for $\xi \mapsto \hat{\eta}$ is derived from the optimality condition $\nabla_\eta U(\xi, \hat{\eta}(\xi)) = 0$.
Applying the implicit function theorem at $\eta = \hat{\eta}(\xi)$ yields:
%
\begin{align}
  \frac{\partial^2 U}{\partial \eta \partial \xi} + \left(\frac{\partial^2 U}{\partial \eta^2}\right) \left(\frac{\partial \hat{\eta}}{\partial \xi}\right) = 0
  \quad \Rightarrow \quad
  \frac{\partial \hat{\eta}}{\partial \xi} = -\left(\frac{\partial^2 U}{\partial \eta^2}\right)^{-1} \frac{\partial^2 U}{\partial \eta \partial \xi}.
\end{align}
%
Since $\hat{H}(\xi) = \left. \frac{\partial^2 U}{\partial \eta^2} \right|_{\eta=\hat{\eta}(\xi)}$, the pullback $\mathcal{B}_{\hat{\eta}}$ maps an upstream gradient $\Delta^{\hat{\eta}}$ to a gradient with respect to $\xi$:
%
\begin{align}\label{eq:pullback-eta-hat}
  \mathcal{B}_{\hat{\eta}} : \Delta^{\hat{\eta}} \to \left(\frac{\partial \hat{\eta}}{\partial \xi}\right)^{T} \Delta^{\hat{\eta}}
  = -\left(\frac{\partial^2 U}{\partial \eta \partial \xi}\right)^{T} \hat{H}(\xi)^{-1} \Delta^{\hat{\eta}}.
\end{align}
%
The map for the Hessian is explicit, $H(\xi, \eta) = \frac{\partial^2 U}{\partial \eta^2}$.
Its pullback $\mathcal{B}_H$ propagates an upstream sensitivity matrix $\Delta^H$ to its inputs $\xi$ and $\eta$, which requires third-order derivative tensors of $U$:
%
\begin{align}\label{eq:pullback-H}
  \mathcal{B}_H : \Delta^H \to \left( \left(\frac{\partial^3 U}{\partial \eta^2 \partial \xi}\right) \bullet \Delta^H, \left(\frac{\partial^3 U}{\partial \eta^3}\right) \bullet \Delta^H \right),
\end{align}
%
where the tensor contraction is defined as $[A \bullet B]_k = \sum_{i,j} A_{ij k} B_{ij}$.

Since $U = L - \log p(\lambda, \eta)$ and $L = \sum_i \ell(\xi^{(i)}, \eta)$, mixed partial derivatives of $U$ and $L$ are equal, $\frac{\partial^{k+1}}{\partial \eta^k \partial \xi} U = \frac{\partial^{k+1}}{\partial \eta^k \partial \xi} L$, and $\frac{\partial^k}{\partial \eta^k} U = \frac{\partial^k}{\partial \eta^k} L - \frac{\partial^k}{\partial \eta^k} \log p(\lambda, \eta)$.
The derivatives of $L$ can themselves be written in terms of the derivatives of $\ell_i = \ell(\xi^{(i)}, \eta) = \Omega(\frac{x_i}{\sigma}, \frac{s \mu_i}{\sigma}, \frac{\delta}{\sigma})$:
%
\begin{align}
  \frac{\partial^{k+1} L}{\partial \eta^k \partial x_i} = \frac{\partial^{k+1} \ell_i}{\partial \eta^k \partial x_i},
  \quad
  \frac{\partial^{k+1} L}{\partial \eta^k \partial \mu_i} = \frac{\partial^{k+1} \ell_i}{\partial \eta^k \partial \mu_i},
  \quad
  \frac{\partial^{k+1} L}{\partial \eta^k \partial \delta} = \sum_{i=1}^N \frac{\partial^{k+1} \ell_i}{\partial \eta^k \partial \delta},
  \quad
  \frac{\partial^{k} L}{\partial \eta^{k}} = \sum_{i=1}^N \frac{\partial^{k} \ell_i}{\partial \eta^{k}},
\end{align}
%
and so the VJPs in \cref{eq:pullback-eta-hat,eq:pullback-H} simplify to
%
\begin{align}
  \left(\frac{\partial^2 U}{\partial \eta \partial \xi}\right)^{T} \Delta^{\hat{H}} & = \left(
  \left(\frac{\partial^2 \ell_1}{\partial \eta \partial x_1}\right)^{T} \Delta^{\hat{H}}, \ldots,
  \left(\frac{\partial^2 \ell_1}{\partial \eta \partial \mu_1}\right)^{T} \Delta^{\hat{H}}, \ldots,
  \sum_{i=1}^N \left(\frac{\partial^2 \ell_i}{\partial \eta \partial \delta}\right)^{T} \Delta^{\hat{H}}
  \right), \label{eq:pullback-eta-hat-simplified}                                                                                                                                                                                                                                   \\
  \left(\frac{\partial^3 U}{\partial \eta^2 \partial \xi}\right) \bullet \Delta^H   & = \left(
  \left(\frac{\partial^3 \ell_1}{\partial \eta^2 \partial x_1}\right) \Delta^H, \ldots,
  \left(\frac{\partial^3 \ell_1}{\partial \eta^2 \partial \mu_1}\right) \Delta^H, \ldots,
  \sum_{i=1}^N \left(\frac{\partial^3 \ell_i}{\partial \eta^2 \partial \delta}\right) \Delta^H
  \right), \label{eq:pullback-H-Phi-eta-squared-simplified}                                                                                                                                                                                                                         \\
  \left(\frac{\partial^3 U}{\partial \eta^3}\right) \bullet \Delta^H                & = -\frac{\partial^3 \log p(\lambda, \eta)}{\partial \eta^3} + \sum_{i=1}^N \left(\frac{\partial^3 \ell_i}{\partial \eta^3}\right) \bullet \Delta^H \label{eq:pullback-H-eta-cubed-simplified}
\end{align}
%
where $\Delta^{\hat{H}} = \hat{H}(\xi)^{-1} \Delta^{\hat{\eta}}$.
Note that for a factorized prior $p(\lambda, \eta) = p(\lambda) p(\eta)$ with $p(\eta)$ Gaussian, $\frac{\partial^3}{\partial \eta^3} \log p(\lambda, \eta) = 0$.
Finally, differentiating through $\hat{\eta}$ and $\hat{H}$ reduces to computing the following derivative tensors:
%
\begin{align}\label{eq:ell-derivative-tensors}
  \ell_{\eta \xi^{(i)}} \coloneqq \frac{\partial^2 \ell_i}{\partial \eta \partial \xi^{(i)}},
  \quad
  \ell_{\eta \eta \xi^{(i)}} \coloneqq \frac{\partial^3 \ell_i}{\partial \eta^2 \partial \xi^{(i)}},
  \quad
  \ell_{\eta \eta \eta} \coloneqq \frac{\partial^3 \ell_i}{\partial \eta^3}.
\end{align}

\subsection{Derivatives of the Per-Observation Log-Likelihood}

The derivatives of $\ell_i$ required for the pullbacks in \cref{eq:ell-derivative-tensors} are found by applying the multivariate chain rule to the composition $\ell_i = \ell(\xi^{(i)}, \eta) = \Omega(\theta(\xi^{(i)}, \eta))$:
%
\begin{align}
  \theta(\xi^{(i)}, \eta) = (x', \nu', \delta')^T = \left(\frac{x_i}{\sigma}, \frac{s \mu_i}{\sigma}, \frac{\delta}{\sigma}\right)^T,
  \quad
  \xi^{(i)} = (x_i, \mu_i, \delta)^T,
  \quad
  \eta = (\log s, \log \sigma)^T. \label{eq:theta-transform}
\end{align}
%
In previous sections, we derived expressions for the numerically stable evaluation of the derivative tensors $\nabla_\theta \Omega$, $\nabla_\theta ^2\Omega$, and $\nabla_\theta ^3\Omega$.
We can express derivatives of $\ell_i$ in \cref{eq:ell-derivative-tensors} in terms of these derivatives of $\Omega$ via the multivariate chain rule;
dropping the index $i$ for brevity, let
%
\begin{align}
  \Theta_{\xi} \coloneqq \frac{\partial \theta}{\partial \xi} =
  \begin{bmatrix}
    \frac{1}{\sigma} & 0                & 0                \\
    0                & \frac{s}{\sigma} & 0                \\
    0                & 0                & \frac{1}{\sigma}
  \end{bmatrix}
  \qquad
  \Theta_{\eta} \coloneqq \frac{\partial \theta}{\partial \eta} =
  \begin{bmatrix}
    0    & -x'      \\
    \nu' & -\nu'    \\
    0    & -\delta'
  \end{bmatrix}
\end{align}
%
be the Jacobians of $\theta$ with respect to $\xi$ and $\eta$.
Similarly, let
$\Theta_{\eta\xi} \coloneqq \frac{\partial^2 \theta}{\partial \eta \partial \xi}$ and
$\Theta_{\eta\eta} \coloneqq \frac{\partial^2 \theta}{\partial \eta^2}$
be the second-derivative 3-tensors with components
%
\begin{align}
  \Theta_{\eta\xi}^{(x')}                                                                                                  & \coloneqq \frac{\partial^2 x'}{\partial \eta \partial \xi} = \begin{bsmallmatrix} 0 & 0 & 0 \\ -\frac{1}{\sigma} & 0 & 0 \end{bsmallmatrix} &   &
  \Theta_{\eta\xi}^{(\nu')} \coloneqq \frac{\partial^2 \nu'}{\partial \eta \partial \xi} = \begin{bsmallmatrix} 0 & \frac{s}{\sigma} & 0 \\ 0 & -\frac{s}{\sigma} & 0 \end{bsmallmatrix} &                                                                                              &
  \Theta_{\eta\xi}^{(\delta')} \coloneqq \frac{\partial^2 \delta'}{\partial \eta \partial \xi} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & -\frac{1}{\sigma} \end{bsmallmatrix}                                                                                                \\
  \Theta_{\eta\eta}^{(x')}                                                                                                 & \coloneqq \frac{\partial^2 x'}{\partial \eta^2} = \begin{bsmallmatrix} 0 & 0 \\ 0 & x' \end{bsmallmatrix}            &   &
  \Theta_{\eta\eta}^{(\nu')} \coloneqq \frac{\partial^2 \nu'}{\partial \eta^2} = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix}           &                                                                                              &
  \Theta_{\eta\eta}^{(\delta')} \coloneqq \frac{\partial^2 \delta'}{\partial \eta^2} = \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta' \end{bsmallmatrix}
\end{align}
%
and let $\Theta_{\eta\eta\xi}$ and $\Theta_{\eta\eta\eta}$ be the third-derivative 4-tensors with components
%
\begin{align}
  \Theta_{(\log s)\eta\xi}^{(x')}       & \coloneqq \tfrac{\partial \Theta_{\eta\xi}^{(x')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix}       &  & \Theta_{(\log s)\eta\xi}^{(\nu')} \coloneqq \tfrac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & \frac{s}{\sigma} & 0 \\ 0 & -\frac{s}{\sigma} & 0 \end{bsmallmatrix}             &  & \Theta_{(\log s)\eta\xi}^{(\delta')} \coloneqq \tfrac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix}              \\
  \Theta_{(\log \sigma)\eta\xi}^{(x')}  & \coloneqq \tfrac{\partial \Theta_{\eta\xi}^{(x')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & 0 & 0 \\ \frac{1}{\sigma} & 0 & 0 \end{bsmallmatrix}  &  & \Theta_{(\log \sigma)\eta\xi}^{(\nu')} \coloneqq \tfrac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & -\frac{s}{\sigma} & 0 \\ 0 & \frac{s}{\sigma} & 0 \end{bsmallmatrix}   &  & \Theta_{(\log \sigma)\eta\xi}^{(\delta')} \coloneqq \tfrac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & \frac{1}{\sigma} \end{bsmallmatrix}    \\
  \Theta_{(\log s)\eta\eta}^{(x')}      & \coloneqq \tfrac{\partial \Theta_{\eta\eta}^{(x')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix}      &  & \Theta_{(\log s)\eta\eta}^{(\nu')} \coloneqq \tfrac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial (\log s)} = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix}           &  & \Theta_{(\log s)\eta\eta}^{(\delta')} \coloneqq \tfrac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix}            \\
  \Theta_{(\log \sigma)\eta\eta}^{(x')} & \coloneqq \tfrac{\partial \Theta_{\eta\eta}^{(x')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -x' \end{bsmallmatrix} &  & \Theta_{(\log \sigma)\eta\eta}^{(\nu')} \coloneqq \tfrac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} -\nu' & \nu' \\ \nu' & -\nu' \end{bsmallmatrix} &  & \Theta_{(\log \sigma)\eta\eta}^{(\delta')} \coloneqq \tfrac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -\delta' \end{bsmallmatrix}.
\end{align}
%
Applying the chain rule, the first derivatives of $\ell$ are
%
\begin{align}
  \ell_{\xi} = \Theta_{\xi}^{T} \nabla_\theta \Omega,
  \qquad
  \ell_{\eta}   = \Theta_{\eta}^{T} \nabla_\theta \Omega.
\end{align}
%
The second derivatives are
%
\begin{align}
  \ell_{\eta \xi}  & = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\xi + \sum_{k} (\nabla_\theta \Omega)_{k} \, \Theta_{\eta\xi}^{(\theta_k)},   \\
  \ell_{\eta \eta} & = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\eta + \sum_{k} (\nabla_\theta \Omega)_{k} \, \Theta_{\eta\eta}^{(\theta_k)}.
\end{align}
%
The third-derivative tensors needed for \cref{eq:pullback-eta-hat-simplified,eq:pullback-H-Phi-eta-squared-simplified,eq:pullback-H-eta-cubed-simplified}
follow by differentiating the Hessian $\ell_{\eta\eta}$
%
\begin{align}
  (\ell_{\eta\eta})_{rs}
  = \frac{\partial^2 \ell}{\partial \eta_r \partial \eta_s}
  = \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial \theta_a}{\partial \eta_r} \frac{\partial \theta_b}{\partial \eta_s}
  + \frac{\partial \Omega}{\partial \theta_c} \frac{\partial^2 \theta_c}{\partial \eta_r \partial \eta_s}
\end{align}
%
where we use the Einstein summation convention.
Differentiating $(\ell_{\eta\eta})_{rs}$ and simplifying gives
%
\begin{align}
  \tfrac{\partial (\ell_{\eta\eta})_{rs}}{\partial \xi_p}
  = & \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}
  \tfrac{\partial \theta_a}{\partial \eta_r} \tfrac{\partial \theta_b}{\partial \eta_s} \tfrac{\partial \theta_c}{\partial \xi_p}
  + \tfrac{\partial \Omega}{\partial \theta_a}
  \tfrac{\partial^3 \theta_a}{\partial \eta_r \partial \eta_s \partial \xi_p} + \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b}
  \left(
  \tfrac{\partial^2 \theta_a}{\partial \eta_r \partial \xi_p} \tfrac{\partial \theta_b}{\partial \eta_s}
  + \tfrac{\partial^2 \theta_b}{\partial \eta_s \partial \xi_p} \tfrac{\partial \theta_a}{\partial \eta_r}
  + \tfrac{\partial^2 \theta_a}{\partial \eta_r \partial \eta_s} \tfrac{\partial \theta_b}{\partial \xi_p}
  \right),                                                                             \\
  \tfrac{\partial (\ell_{\eta\eta})_{rs}}{\partial \eta_t}
  = & \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}
  \tfrac{\partial \theta_a}{\partial \eta_r} \tfrac{\partial \theta_b}{\partial \eta_s} \tfrac{\partial \theta_c}{\partial \eta_t}
  + \tfrac{\partial \Omega}{\partial \theta_c}
  \tfrac{\partial^3 \theta_c}{\partial \eta_r \partial \eta_s \partial \eta_t} + \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b}
  \left(
  \tfrac{\partial^2 \theta_a}{\partial \eta_r \partial \eta_t} \tfrac{\partial \theta_b}{\partial \eta_s}
  + \tfrac{\partial^2 \theta_a}{\partial \eta_s \partial \eta_t} \tfrac{\partial \theta_b}{\partial \eta_r}
  + \tfrac{\partial^2 \theta_a}{\partial \eta_r \partial \eta_s} \tfrac{\partial \theta_b}{\partial \eta_t}
  \right).
\end{align}
%
The VJPs are then obtained by contracting with $\Delta^H_{rs}$ and simplifying using $\Delta^H_{rs} = \Delta^H_{sr}$:
%
\begin{align}
  \tfrac{\partial (\ell_{\eta\eta} \cdot \Delta^H)}{\partial \xi_p}  & = \left( \tfrac{\partial^3 \theta_a}{\partial \eta_r \partial \eta_s \partial \xi_p} \Delta^H_{rs} \right) \tfrac{\partial \Omega}{\partial \theta_a} + \left( 2 \tfrac{\partial^2 \theta_a}{\partial \eta_r \partial \xi_p} \tfrac{\partial \theta_b}{\partial \eta_s} \Delta^H_{rs} + \tfrac{\partial \theta_b}{\partial \xi_p} \tfrac{\partial^2 \theta_a}{\partial \eta_r \partial \eta_s} \Delta^H_{rs} \right) \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} + \left( \tfrac{\partial \theta_a}{\partial \eta_r} \tfrac{\partial \theta_b}{\partial \eta_s} \Delta^H_{rs} \right) \tfrac{\partial \theta_c}{\partial \xi_p} \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c},     \\
  \tfrac{\partial (\ell_{\eta\eta} \cdot \Delta^H)}{\partial \eta_t} & = \left( \tfrac{\partial^3 \theta_a}{\partial \eta_r \partial \eta_s \partial \eta_t} \Delta^H_{rs} \right) \tfrac{\partial \Omega}{\partial \theta_a} + \left( 2 \tfrac{\partial^2 \theta_a}{\partial \eta_r \partial \eta_t} \tfrac{\partial \theta_b}{\partial \eta_s} \Delta^H_{rs} + \tfrac{\partial \theta_b}{\partial \eta_t} \tfrac{\partial^2 \theta_a}{\partial \eta_r \partial \eta_s} \Delta^H_{rs} \right) \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} + \left( \tfrac{\partial \theta_a}{\partial \eta_r} \tfrac{\partial \theta_b}{\partial \eta_s} \Delta^H_{rs} \right) \tfrac{\partial \theta_c}{\partial \eta_t} \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}.
\end{align}

% Bibliography
\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}
