
@article{rice_mathematical_1944,
  title     = {Mathematical {Analysis} of {Random} {Noise}},
  volume    = {23},
  copyright = {{\textcopyright} 1944 The Bell System Technical Journal},
  issn      = {1538-7305},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1944.tb00874.x},
  doi       = {10.1002/j.1538-7305.1944.tb00874.x},
  language  = {en},
  number    = {3},
  urldate   = {2020-06-05},
  journal   = {Bell System Technical Journal},
  author    = {Rice, S. O.},
  year      = {1944},
  note      = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1944.tb00874.x},
  pages     = {282--332},
  file      = {Snapshot:/home/jdoucette/Zotero/storage/BWLD3BC5/j.1538-7305.1944.tb00874.html:text/html}
}

@article{rice_mathematical_1945,
  title     = {Mathematical {Analysis} of {Random} {Noise}},
  volume    = {24},
  copyright = {{\textcopyright} 1945 The Bell System Technical Journal},
  issn      = {1538-7305},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1945.tb00453.x},
  doi       = {10.1002/j.1538-7305.1945.tb00453.x},
  language  = {en},
  number    = {1},
  urldate   = {2020-06-05},
  journal   = {Bell System Technical Journal},
  author    = {Rice, S. O.},
  year      = {1945},
  note      = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1945.tb00453.x},
  pages     = {46--156},
  file      = {Snapshot:/home/jdoucette/Zotero/storage/DKXS9TCN/j.1538-7305.1945.tb00453.html:text/html}
}

@article{ball_half-range_2002,
  title    = {Half-{Range} {Generalized} {Hermite} {Polynomials} and the {Related} {Gaussian} {Quadratures}},
  volume   = {40},
  doi      = {10.1137/S0036142900370939},
  abstract = {A method is developed for calculating the recurrence coefficients for half-range generalized Hermite polynomials. These are orthogonal polynomials with measure x $\gamma$e -x2 on the interval (0, $\infty$). The recurrence coefficients can then be used to generate the weights and nodes of the related Gaussian quadratures. These quadratures allow efficient high accuracy evaluation of many Gaussian integrals encountered in probability functions, statistical mechanics, and quantum mechanics. The number of steps for an accurate numerical calculation of the recurrence coefficients is proportional to N, the number of coefficients obtained. Extended precision arithmetic is not needed in these calculations.},
  journal  = {SIAM J. Numerical Analysis},
  author   = {Ball, James},
  month    = dec,
  year     = {2002},
  pages    = {2311--2317},
  file     = {Ball - 2002 - Half-Range Generalized Hermite Polynomials and the.pdf:/home/jdoucette/Zotero/storage/ZNUHPJG4/Ball - 2002 - Half-Range Generalized Hermite Polynomials and the.pdf:application/pdf}
}

@article{galant_gauss_1969,
  title    = {Gauss {Quadrature} {Rules} for the {Evaluation} of 2$\pi$-1/2 ?$\infty$0 exp (-x2)f(x)dx},
  volume   = {23},
  issn     = {0025-5718},
  url      = {https://www.jstor.org/stable/2004397},
  abstract = {Gauss quadrature rules for evaluating integrals of the form 2$\pi$$^{\textrm{-1/2}}$ ?$^{\textrm{$\infty$}}$$_{\textrm{0}}$ exp (-x$^{\textrm{2}}$)f(x)dx have been calculated to 20S for one to twenty nodes. The coefficients for the three-term recurrence relation of the first twenty orthogonal polynomials associated with the weight function exp (-x$^{\textrm{2}}$) on the interval [ 0, $\infty$) are also tabulated to 20S.},
  number   = {107},
  urldate  = {2023-10-25},
  journal  = {Mathematics of Computation},
  author   = {Galant, David},
  year     = {1969},
  note     = {Publisher: American Mathematical Society},
  pages    = {674--s39},
  file     = {JSTOR Full Text PDF:/home/jdoucette/Zotero/storage/2CRDELRX/Galant - 1969 - Gauss Quadrature Rules for the Evaluation of 2$\pi$-1.pdf:application/pdf}
}

@article{shizgal_gaussian_1981,
  title    = {A {Gaussian} quadrature procedure for use in the solution of the {Boltzmann} equation and related problems},
  volume   = {41},
  issn     = {0021-9991},
  url      = {https://www.sciencedirect.com/science/article/pii/0021999181900991},
  doi      = {10.1016/0021-9991(81)90099-1},
  abstract = {A new Gaussian quadrature procedure is developed for integrals of the form ?0$\infty$ e-y2yp(y)dy for p = 0, 1 and 2. Recursion relations are derived for the coefficients in the general three term recurrence relation for the polynomials whose roots are the quadrature abscissae. A comparison with the Gauss-Laguerre quadrature procedure is presented. Solutions of the chemical kinetic Boltzmann equation are obtained with a discrete ordinate method based on this Gaussian quadrature procedure. The results are compared with previous solutions obtained with a polynomial expansion method.},
  number   = {2},
  urldate  = {2023-10-27},
  journal  = {Journal of Computational Physics},
  author   = {Shizgal, B},
  month    = jun,
  year     = {1981},
  pages    = {309--328},
  file     = {ScienceDirect Snapshot:/home/jdoucette/Zotero/storage/Z2PH8BUU/0021999181900991.html:text/html;Shizgal - 1981 - A Gaussian quadrature procedure for use in the sol.pdf:/home/jdoucette/Zotero/storage/AT39ISST/Shizgal - 1981 - A Gaussian quadrature procedure for use in the sol.pdf:application/pdf}
}

@article{golub_calculation_1969,
  title    = {Calculation of {Gauss} quadrature rules},
  volume   = {23},
  issn     = {0025-5718, 1088-6842},
  url      = {https://www.ams.org/mcom/1969-23-106/S0025-5718-69-99647-1/},
  doi      = {10.1090/S0025-5718-69-99647-1},
  language = {en},
  number   = {106},
  urldate  = {2025-08-28},
  journal  = {Mathematics of Computation},
  author   = {Golub, Gene H. and Welsch, John H.},
  year     = {1969},
  pages    = {221--230},
  file     = {Full Text PDF:/home/jdoucette/Zotero/storage/EF2S63TZ/Golub and Welsch - 1969 - Calculation of Gauss quadrature rules.pdf:application/pdf}
}

@inproceedings{immer_improving_2021,
  title     = {Improving predictions of {Bayesian} neural nets via local linearization},
  url       = {https://proceedings.mlr.press/v130/immer21a.html},
  abstract  = {The generalized Gauss-Newton (GGN) approximation is often used to make practical Bayesian deep learning approaches scalable by replacing a second order derivative with a product of first order derivatives. In this paper we argue that the GGN approximation should be understood as a local linearization of the underlying Bayesian neural network (BNN), which turns the BNN into a generalized linear model (GLM). Because we use this linearized model for posterior inference, we should also predict using this modified model instead of the original one. We refer to this modified predictive as "GLM predictive" and show that it effectively resolves common underfitting problems of the Laplace approximation. It extends previous results in this vein to general likelihoods and has an equivalent Gaussian process formulation, which enables alternative inference schemes for BNNs in function space. We demonstrate the effectiveness of our approach on several standard classification datasets as well as on out-of-distribution detection. We provide an implementation at https://github.com/AlexImmer/BNN-predictions.},
  language  = {en},
  urldate   = {2025-09-09},
  booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
  publisher = {PMLR},
  author    = {Immer, Alexander and Korzepa, Maciej and Bauer, Matthias},
  month     = mar,
  year      = {2021},
  note      = {ISSN: 2640-3498},
  pages     = {703--711},
  file      = {Full Text PDF:/home/jdoucette/Zotero/storage/C8DP6ILE/Immer et al. - 2021 - Improving predictions of Bayesian neural nets via local linearization.pdf:application/pdf;Supplementary PDF:/home/jdoucette/Zotero/storage/BZKH5E9H/Immer et al. - 2021 - Improving predictions of Bayesian neural nets via local linearization.pdf:application/pdf}
}

@article{mackay_bayesian_1992,
  title    = {Bayesian {Interpolation}},
  volume   = {4},
  issn     = {0899-7667},
  url      = {https://doi.org/10.1162/neco.1992.4.3.415},
  doi      = {10.1162/neco.1992.4.3.415},
  abstract = {Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. {\textquotedblleft}Occam's razor{\textquotedblright} is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.},
  number   = {3},
  urldate  = {2025-09-09},
  journal  = {Neural Computation},
  author   = {MacKay, David J. C.},
  month    = may,
  year     = {1992},
  pages    = {415--447},
  file     = {Accepted Version:/home/jdoucette/Zotero/storage/HVRFV5EH/MacKay - 1992 - Bayesian Interpolation.pdf:application/pdf;Snapshot:/home/jdoucette/Zotero/storage/IX2BT78W/neco.1992.4.3.html:text/html}
}

@inproceedings{margossian_hamiltonian_2020,
  title      = {Hamiltonian {Monte} {Carlo} using an adjoint-differentiated {Laplace} approximation: {Bayesian} inference for latent {Gaussian} models and beyond},
  volume     = {33},
  shorttitle = {Hamiltonian {Monte} {Carlo} using an adjoint-differentiated {Laplace} approximation},
  url        = {https://proceedings.neurips.cc/paper/2020/hash/673de96b04fa3adcae1aacda704217ef-Abstract.html},
  abstract   = {Gaussian latent variable models are a key class of Bayesian hierarchical models with applications in many fields. Performing Bayesian inference on such models can be challenging as Markov chain Monte Carlo algorithms struggle with the geometry of the resulting posterior distribution and can be prohibitively slow. An alternative is to use a Laplace approximation to marginalize out the latent Gaussian variables and then integrate out the remaining hyperparameters using dynamic Hamiltonian Monte Carlo, a gradient-based Markov chain Monte Carlo sampler. To implement this scheme efficiently, we derive a novel adjoint method that propagates the minimal information needed to construct the gradient of the approximate marginal likelihood. This strategy yields a scalable differentiation method that is orders of magnitude faster than state of the art differentiation techniques when the hyperparameters are high dimensional. We prototype the method in the probabilistic programming framework Stan and test the utility of the embedded Laplace approximation on several models, including one where the dimension of the hyperparameter is \~{}6,000. Depending on the cases, the benefits can include an alleviation of the geometric pathologies that frustrate Hamiltonian Monte Carlo and a dramatic speed-up.},
  urldate    = {2025-09-09},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher  = {Curran Associates, Inc.},
  author     = {Margossian, Charles and Vehtari, Aki and Simpson, Daniel and Agrawal, Raj},
  year       = {2020},
  pages      = {9086--9097},
  file       = {Full Text PDF:/home/jdoucette/Zotero/storage/CA5LBA3R/Margossian et al. - 2020 - Hamiltonian Monte Carlo using an adjoint-differentiated Laplace approximation Bayesian inference fo.pdf:application/pdf}
}

@inproceedings{park_variational_2019,
  title     = {Variational {Laplace} {Autoencoders}},
  url       = {https://proceedings.mlr.press/v97/park19a.html},
  abstract  = {Variational autoencoders employ an amortized inference model to approximate the posterior of latent variables. However, such amortized variational inference faces two challenges: (1) the limited posterior expressiveness of fully-factorized Gaussian assumption and (2) the amortization error of the inference model. We present a novel approach that addresses both challenges. First, we focus on ReLU networks with Gaussian output and illustrate their connection to probabilistic PCA. Building on this observation, we derive an iterative algorithm that finds the mode of the posterior and apply fullcovariance Gaussian posterior approximation centered on the mode. Subsequently, we present a general framework named Variational Laplace Autoencoders (VLAEs) for training deep generative models. Based on the Laplace approximation of the latent variable posterior, VLAEs enhance the expressiveness of the posterior while reducing the amortization error. Empirical results on MNIST, Omniglot, Fashion-MNIST, SVHN and CIFAR10 show that the proposed approach significantly outperforms other recent amortized or iterative methods on the ReLU networks.},
  language  = {en},
  urldate   = {2025-09-09},
  booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  author    = {Park, Yookoon and Kim, Chris and Kim, Gunhee},
  month     = may,
  year      = {2019},
  note      = {ISSN: 2640-3498},
  pages     = {5032--5041},
  file      = {Full Text PDF:/home/jdoucette/Zotero/storage/X9372BU4/Park et al. - 2019 - Variational Laplace Autoencoders.pdf:application/pdf;Supplementary PDF:/home/jdoucette/Zotero/storage/ATZWUQLE/Park et al. - 2019 - Variational Laplace Autoencoders.pdf:application/pdf}
}

@inproceedings{daxberger_laplace_2021,
  title     = {Laplace {Redux} - {Effortless} {Bayesian} {Deep} {Learning}},
  volume    = {34},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/a7c9585703d275249f30a088cebba0ad-Abstract.html},
  abstract  = {Bayesian formulations of deep learning have been shown to have compelling theoretical properties and offer practical functional benefits, such as improved predictive uncertainty quantification and model selection. The Laplace approximation (LA) is a classic, and arguably the simplest family of approximations for the intractable posteriors of deep neural networks. Yet, despite its simplicity, the LA is not as popular as alternatives like variational Bayes or deep ensembles. This may be due to assumptions that the LA is expensive due to the involved Hessian computation, that it is difficult to implement, or that it yields inferior results. In this work we show that these are misconceptions: we (i) review the range of variants of the LA including versions with minimal cost overhead; (ii) introduce "laplace", an easy-to-use software library for PyTorch offering user-friendly access to all major flavors of the LA; and (iii) demonstrate through extensive experiments that the LA is competitive with more popular alternatives in terms of performance, while excelling in terms of computational cost. We hope that this work will serve as a catalyst to a wider adoption of the LA in practical deep learning, including in domains where Bayesian approaches are not typically considered at the moment.},
  urldate   = {2025-09-09},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Daxberger, Erik and Kristiadi, Agustinus and Immer, Alexander and Eschenhagen, Runa and Bauer, Matthias and Hennig, Philipp},
  year      = {2021},
  pages     = {20089--20103},
  file      = {Full Text PDF:/home/jdoucette/Zotero/storage/Q449QKGC/Daxberger et al. - 2021 - Laplace Redux - Effortless Bayesian Deep Learning.pdf:application/pdf}
}

@article{immer_invariance_2022,
  title    = {Invariance {Learning} in {Deep} {Neural} {Networks} with {Differentiable} {Laplace} {Approximations}},
  volume   = {35},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/50d005f92a6c5c9646db4b761da676ba-Abstract-Conference.html},
  abstract = {Data augmentation is commonly applied to improve performance of deep learning by enforcing the knowledge that certain transformations on the input preserve the output. Currently, the data augmentation parameters are chosen by human effort and costly cross-validation, which makes it cumbersome to apply to new datasets. We develop a convenient gradient-based method for selecting the data augmentation without validation data during training of a deep neural network. Our approach relies on phrasing data augmentation as an invariance in the prior distribution on the functions of a neural network, which allows us to learn it using Bayesian model selection. This has been shown to work in Gaussian processes, but not yet for deep neural networks. We propose a differentiable Kronecker-factored Laplace approximation to the marginal likelihood as our objective, which can be optimised without human supervision or validation data. We show that our method can successfully recover invariances present in the data, and that this improves generalisation and data efficiency on image datasets.},
  language = {en},
  urldate  = {2025-09-09},
  journal  = {Advances in Neural Information Processing Systems},
  author   = {Immer, Alexander and van der Ouderaa, Tycho and R{\"a}tsch, Gunnar and Fortuin, Vincent and van der Wilk, Mark},
  month    = dec,
  year     = {2022},
  pages    = {12449--12463},
  file     = {Full Text PDF:/home/jdoucette/Zotero/storage/8M69URZA/Immer et al. - 2022 - Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations.pdf:application/pdf}
}

@inproceedings{ritter_scalable_2018,
  title     = {A scalable laplace approximation for neural networks},
  volume    = {6},
  abstract  = {We leverage recent insights from second-order optimisation for neural networks to construct a Kronecker factored Laplace approximation to the posterior over the weights of a trained network. Our approximation requires no modification of the training procedure, enabling practitioners to estimate the uncertainty of their models currently used in production without having to retrain them. We extensively compare our method to using Dropout and a diagonal Laplace approximation for estimating the uncertainty of a network. We demonstrate that our Kronecker factored method leads to better uncertainty estimates on out-of-distribution data and is more robust to simple adversarial attacks. Our approach only requires calculating two square curvature factor matrices for each layer. Their size is equal to the respective square of the input and output size of the layer, making the method efficient both computationally and in terms of memory usage. We illustrate its scalability by applying it to a state-of-the-art convolutional network architecture.},
  booktitle = {6th international conference on learning representations, {ICLR} 2018-conference track proceedings},
  publisher = {International Conference on Representation Learning},
  author    = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
  year      = {2018},
  file      = {A Scalable Laplace Approximation for Neural Networks - UCL Discovery:/home/jdoucette/Zotero/storage/53TB29SZ/10080902.html:text/html;PDF:/home/jdoucette/Zotero/storage/E7ALJR5Y/A Scalable Laplace Approximation for Neural Networks - UCL Discovery.pdf:application/pdf}
}

@inproceedings{kristiadi_being_2020,
  title     = {Being {Bayesian}, {Even} {Just} a {Bit}, {Fixes} {Overconfidence} in {ReLU} {Networks}},
  url       = {https://proceedings.mlr.press/v119/kristiadi20a.html},
  abstract  = {The point estimates of ReLU classification networks{\textemdash}arguably the most widely used neural network architecture{\textemdash}have been shown to yield arbitrarily high confidence far away from the training data. This architecture, in conjunction with a maximum a posteriori estimation scheme, is thus not calibrated nor robust. Approximate Bayesian inference has been empirically demonstrated to improve predictive uncertainty in neural networks, although the theoretical analysis of such Bayesian approximations is limited. We theoretically analyze approximate Gaussian distributions on the weights of ReLU networks and show that they fix the overconfidence problem. Furthermore, we show that even a simplistic, thus cheap, Bayesian approximation, also fixes these issues. This indicates that a sufficient condition for a calibrated uncertainty on a ReLU network is {\textquotedblleft}to be a bit Bayesian{\textquotedblright}. These theoretical results validate the usage of last-layer Bayesian approximation and motivate a range of a fidelity-cost trade-off. We further validate these findings empirically via various standard experiments using common deep ReLU networks and Laplace approximations.},
  language  = {en},
  urldate   = {2025-09-09},
  booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  author    = {Kristiadi, Agustinus and Hein, Matthias and Hennig, Philipp},
  month     = nov,
  year      = {2020},
  note      = {ISSN: 2640-3498},
  pages     = {5436--5446},
  file      = {Full Text PDF:/home/jdoucette/Zotero/storage/E2H2SLFX/Kristiadi et al. - 2020 - Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks.pdf:application/pdf;Supplementary PDF:/home/jdoucette/Zotero/storage/VBJKFQHQ/Kristiadi et al. - 2020 - Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks.pdf:application/pdf}
}
