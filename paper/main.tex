\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[sort,compress]{cleveref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{siunitx}

\graphicspath{{figures/output/}}
\sisetup{round-mode = figures, round-precision = 3}

\title{Stable, efficient evaluation of gradients and Hessians for the Rician log-likelihood}
\author{
  Jonathan Doucette \and
  Christian Kames \and
  Alexander Rauscher
}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}

We consider the Rician probability density for a positive observation $x>0$ with noncentrality parameter $\nu>0$ and scale $\sigma>0$.
The PDF is
%
\begin{align}
  p(x \mid \nu, \sigma) = \frac{x}{\sigma^2} \exp\left(-\frac{x^2+\nu^2}{2\sigma^2}\right) I_0\left(\frac{x\nu}{\sigma^2}\right) \label{eq:rician-pdf}
\end{align}
%
where $I_0$ is the modified Bessel function of the first kind of order zero.

We seek numerically stable, machine-precision formulas for the negative log-likelihood and its first, second, and third partial derivatives with respect to $x$ and $\nu$.
If we define
%
\begin{align}
  f(x, \nu) \coloneqq -\log p(x \mid \nu, \sigma = 1),
\end{align}
%
then the log of the likelihood of \cref{eq:rician-pdf} can be given in terms of $f$ via
%
\begin{align}
  \log p(x \mid \nu, \sigma) = -\log\sigma - f\left(\frac{x}{\sigma}, \frac{\nu}{\sigma}\right).
\end{align}
%
Similarly, the derivatives of $\log p$ with respect to $x$, $\nu$, and $\sigma$ are given in terms of the derivatives of $f$ evaluated at $(x', \nu') = (x / \sigma, \nu / \sigma)$:
%
\begin{align}
  \frac{\partial \log p}{\partial x}      = -\frac{1}{\sigma} \frac{\partial f}{\partial x'},                                                                                        \qquad
  \frac{\partial \log p}{\partial \nu}    = -\frac{1}{\sigma} \frac{\partial f}{\partial \nu'},                                                                                      \qquad
  \frac{\partial \log p}{\partial \sigma} = \frac{1}{\sigma} \left( x' \frac{\partial f}{\partial x'} + \nu' \frac{\partial f}{\partial \nu'} - 1 \right)
\end{align}
%
and likewise for higher-order derivatives.

Henceforth we take $\sigma=1$ and, by slight abuse of notation, write $x, \nu$ for the rescaled variables $x', \nu'$.
We will show that the key to numerically stable evaluation of $f$ and its derivatives is to carefully consider the limiting behavior in the regimes where $z = x\nu \ll 1$ and $z \gg 1 \Leftrightarrow u=1/z \ll 1$.
Naive evaluation of $f$ and its derivatives produces expressions with catastrophic cancellation in both regimes.
To evaluate these expressions in a numerically stable way, we derive algebraic simplifications in both the $z \ll 1$ and $z \gg 1$ regimes.

\section{Methods}

\subsection{The Rician log-likelihood and basic simplifications}\label{sec:rician-log-likelihood-and-basic-simplifications}

With $\sigma=1$, the Rician negative log-likelihood is given by
%
\begin{align}
  f(x, \nu) & = -\log p(x \mid \nu, \sigma=1)                                                                                                                                \\
            & = \frac{x^2 + \nu^2}{2} - \log x - \log I_0(x \nu) \label{eq:rician-neg-log-likelihood}                                                                        \\
            & = \frac{(x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{x}{\nu}\right) - \log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi). \label{eq:rician-neg-log-likelihood-scaled}
\end{align}
%
Here, $\hat{I}_0$ is the modified Bessel function of the first kind normalized such that $\lim_{z \mapsto \infty} \hat{I}_0(z) = 1$:
%
\begin{align}
  I_0(z) = \frac{e^z}{\sqrt{2\pi z}} \hat{I}_0(z) \quad \Leftrightarrow \quad \hat{I}_0(z) = \sqrt{2\pi z} e^{-z} I_0(z)
\end{align}
%
where $z = x\nu \ge 0$.
Next, we consider the numerical stability of these algebraically equivalent forms of $f$.

\subsubsection{Stability of the log-likelihood}

\paragraph{Small $z \ll 1$.}

For small $z$, we have that $\log I_0(z) = z^2/4 + \mathcal{O}(z^4)$, and thus \cref{eq:rician-neg-log-likelihood} becomes
%
\begin{align}
  f(x, \nu) = \frac{x^2 + \nu^2}{2} - \log x - \frac{z^2}{4} + \mathcal{O}(z^4).
\end{align}
%
The terms in this expression are generally not of the same order of magnitude, so there are no cancellation issues.
% This is slightly more numerically stable than using $\log \hat{I}_0(z) = \log I_0(z) - z + \frac{1}{2}\log(2\pi z) = \frac{1}{2}\log(2\pi z) - z + \mathcal{O}(z^2)$, which may suffer small cancellation issues when computing $-\frac{1}{2}\log(\frac{x}{\nu}) - \log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi) = -\log x + z + \mathcal{O}(z^2)$.
% For example, if $x \approx 1$ and $\nu \approx z$, then $\frac{1}{2}\log(\frac{x}{\nu}) \approx -\frac{1}{2}\log z$ and $-\log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi) \approx -\frac{1}{2}\log z$ but the result is $z \ll -\log z$.

\paragraph{Large $z \gg 1$.}

For large $z$, we have that $\log \hat{I}_0(z) = \frac{1}{8 z} + \mathcal{O}(z^{-2})$, and thus \cref{eq:rician-neg-log-likelihood-scaled} becomes
%
\begin{align}
  f(x, \nu) = \frac{(x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{x}{\nu}\right) + \frac{1}{2}\log(2\pi) - \frac{1}{8 z} + \mathcal{O}(z^{-2}).
\end{align}
%
The terms in this expression are generally not of the same order of magnitude -- unless $x \approx \nu$, which we address next -- so there are typically no cancellation issues.

\paragraph{High SNR $x \approx \nu \approx \sqrt{z} \gg 1$.}

A common case in practice is when the signal-to-noise ratio (SNR) is large:
observations are concentrated near the noncentrality parameter such that $x \approx \nu \pm 1$ (recall that the noise level is normalized to $\sigma=1$).
As was the case for large $z$, we should use the scaled form of \cref{eq:rician-neg-log-likelihood-scaled} but with a stable evaluation of $\log(x/\nu)$:
%
\begin{align}
  \log\left(\frac{x}{\nu}\right) = \begin{cases}
    \log\left(1 + \frac{x-\nu}{\nu}\right) & \text{if } x \ge \nu \\
    -\log\left(1 + \frac{\nu-x}{x}\right)  & \text{if } x < \nu
  \end{cases}
\end{align}
%
where $\log(1 + \epsilon) = \epsilon - \epsilon^2/2 + \mathcal{O}(\epsilon^3)$ is computed using the common special-function routine \texttt{log1p($\epsilon$)} which is designed to be accurate when $\epsilon$ is small.
With this modification, \cref{eq:rician-neg-log-likelihood-scaled} is accurate for all $z \gg 1$.

We note that \cref{eq:rician-neg-log-likelihood} is not stable in the high SNR regime, since $\log I_0(z) = \log \hat{I}_0(z) + z - \frac{1}{2}\log(2\pi z)$ has a large component $z$ which catastrophically cancels with $(x^2 + \nu^2) / 2 \approx z$.

\subsubsection{First derivatives}

Now, we differentiate $f$ with respect to $x$ and $\nu$ and simplify algebraically:
%
\begin{align}
  f_x   & = x-\nu -\frac{1}{2x} - \nu\frac{d}{dz}\log\hat{I}_0(z) \label{eq:first-derivatives-unsimplified-x}  \\
  f_\nu & = \nu-x +\frac{1}{2\nu} - x\frac{d}{dz}\log\hat{I}_0(z) \label{eq:first-derivatives-unsimplified-nu}
\end{align}
%
where
%
\begin{align}
  \frac{d}{dz}\log\hat{I}_0(z) & = r(z) - 1 + \frac{1}{2z} \label{eq:log-scaled-bessel-derivative} \\
  r(z)                         & = \frac{I_1(z)}{I_0(z)}. \label{eq:ratio-r}
\end{align}
%
Substituting \cref{eq:log-scaled-bessel-derivative} and $z=x\nu$ into \cref{eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu} gives
%
\begin{align}
  f_x   & = x - \nu r - \frac{1}{x} \label{eq:first-derivatives-simplified-x} \\
  f_\nu & = \nu - x r \label{eq:first-derivatives-simplified-nu}
\end{align}

\subsubsection{Second derivatives}

Next, we differentiate the simplified first derivatives in \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu} to obtain the second derivatives:
%
\begin{align}
  f_{xx}     & = \frac{\partial}{\partial x}\left(x - \nu r(z) - \frac{1}{x}\right) = 1 + \frac{1}{x^2} - \nu^2 r' \label{eq:second-derivatives-unsimplified-x-x} \\
  f_{x\nu}   & = \frac{\partial}{\partial \nu}\left(x - \nu r(z) - \frac{1}{x}\right) = -(r + z r') = -z(1 - r^2) \label{eq:second-derivatives-unsimplified-x-nu} \\
  f_{\nu\nu} & = \frac{\partial}{\partial \nu}\left(\nu - x r(z)\right) = 1 - x^2 r' \label{eq:second-derivatives-unsimplified-nu-nu}
\end{align}
%
where the last equality in \cref{eq:second-derivatives-unsimplified-x-nu} follows from the recurrence relation \cref{eq:r-prime-recurrence}.

\subsubsection{Third derivatives}

Differentiate once more to obtain third derivatives;
each depends only on $r'$ and $r''$:
%
\begin{align}
  f_{xxx}       & = \frac{\partial}{\partial x}\left(1+\frac{1}{x^2}-\nu^2 r'(z)\right) = -\frac{2}{x^3} - \nu^3 r'' \label{eq:third-derivatives-unsimplified-x-x-x}                                 \\
  f_{xx\nu}     & = \frac{\partial}{\partial \nu}\left(1+\frac{1}{x^2}-\nu^2 r'(z)\right) = -2\nu r' - x\nu^2 r'' = -\nu(2r' + z r'')              \label{eq:third-derivatives-unsimplified-x-x-nu}  \\
  f_{x\nu\nu}   & = \frac{\partial}{\partial x}\left(1 - x^2 r'(z)\right) = -2x r' - x^2\nu r'' = -x(2r' + z r'')                                  \label{eq:third-derivatives-unsimplified-x-nu-nu} \\
  f_{\nu\nu\nu} & = \frac{\partial}{\partial \nu}\left(1 - x^2 r'(z)\right) = -x^3 r''. \label{eq:third-derivatives-unsimplified-nu-nu-nu}
\end{align}
%
Therefore it suffices to compute $r$, $r'$, and $r''$ with high relative accuracy.

\subsubsection{Recurrence relations for derivatives of $r$}

To obtain $r$, $r'$, and $r''$, we differentiate $r=I_1/I_0$.
Using $I_0'=I_1$ and $I_1'=I_0 - I_1/z$, we have the recurrence relations
%
\begin{align}
  r'  & = \frac{I_1'}{I_0} - r\frac{I_0'}{I_0} = \frac{I_0 - \frac{1}{z} I_1}{I_0} - r^2 = 1 - \frac{r}{z} - r^2 \label{eq:r-prime-recurrence}                                           \\
  r'' & = -\left(\frac{r}{z}\right)' - 2 r r' = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r' = 2 r (r^2 - 1) + \frac{3 r^2 - 1}{z} + \frac{2 r}{z^2} \label{eq:r-second-derivative-recurrence}
\end{align}

These are exact identities but are numerically unstable as $z \mapsto 0$ and as $z \mapsto \infty$.
To address this, for each of $r$, $r'$, and $r''$, we split the domain $z>0$ into three regimes and approximate a single well-scaled quantity in each regime:
\begin{itemize}
  \item Small $z$:
        use a Taylor expansion, group terms by order in $z$, and fit a minimax polynomial to the residual.
  \item Large $z$:
        use the asymptotic expansion in $u = 1/z$, group terms by order in $u$, and fit a minimax polynomial to the residual.
  \item Intermediate $z$:
        cancellation is negligible;
        employ a rational minimax approximant.
\end{itemize}

\subsubsection{Small-$z$ reparametrizations for $r$, $r'$, $r''$}\label{sec:bessel-ratio-small-z}

For small $z$, $r \approx z/2$.
We define
%
\begin{align}\label{eq:a0-small-z}
  a_0(z) = \frac{r(z)}{z} = \frac{1}{2} - \frac{1}{16}z^2 + \mathcal{O}(z^4)
\end{align}
%
so that computing $r$ reduces to approximating $a_0$ to high precision:
%
\begin{align}\label{eq:r-small-reparametrized}
  \boxed{r(z) = z a_0(z).}
\end{align}
%
For $r'$, we substitute \cref{eq:r-small-reparametrized} into \cref{eq:r-prime-recurrence} to obtain
%
\begin{align}\label{eq:r-prime-small-reparametrized}
  \boxed{r'(z) = 1 - \frac{r}{z} - r^2 = 1 - a_0 - z^2 a_0^2}
\end{align}
%
which is numerically stable since $a_0 = \frac{1}{2} + \mathcal{O}(z^2)$, so both the constant coefficient $1 - a_0$ and the quadratic coefficient $-a_0^2$ do not suffer from cancellation in floating-point arithmetic.
However, the second derivative $r''$ requires more care.
Substituting \cref{eq:r-prime-small-reparametrized} into \cref{eq:r-second-derivative-recurrence} yields
%
\begin{align}
  r''(z) & = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r'                                             \\
         & = -\frac{1 - a_0 - z^2 a_0^2}{z} + \frac{z a_0}{z^2} -  2 z a_0(1 - a_0 - z^2 a_0^2) \\
         & = \frac{2 a_0 - 1}{z} + z a_0 (3 a_0 - 2) + 2 z^3 a_0^3.
\end{align}
%
The leading $\mathcal{O}(z^{-1})$ term has coefficient $2a_0 - 1$, but for small $z$, $a_0 \approx \frac{1}{2} - \frac{1}{16}z^2$, so computing $2a_0 - 1$ requires subtracting two $\mathcal{O}(1)$ terms to obtain a $\mathcal{O}(z^2)$ term.
We therefore reparametrize to
%
\begin{align}
  a_0                 & = \frac{1}{2} + z^2 a_1 \label{eq:a0-reparametrized}                                                          \\
  \Leftrightarrow a_1 & = \frac{1}{z^2} (a_0 - \frac{1}{2}) = \frac{1}{z^2} (\frac{r}{z} - \frac{1}{2}). \label{eq:a1-reparametrized}
\end{align}
%
Note that since $r = \frac{1}{2}z - \frac{1}{16}z^3 + \mathcal{O}(z^5)$, it follows that $a_1 = -\frac{1}{16} + \mathcal{O}(z^2)$.
Using $(2 a_0 - 1) / z = 2 z a_1$, we obtain a numerically stable form for the second derivative:
%
\begin{align}\label{eq:r-prime-prime-small-reparametrized}
  \boxed{r''(z) = z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3}
\end{align}
%
We also note that the quantity $2r' + z r''$ from \cref{eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu} does not suffer cancellation issues, since $2r' = 1 + \mathcal{O}(z^2)$ and $z r'' = \mathcal{O}(z^2)$.
Thus, we need only fit one minimax polynomial to \cref{eq:a1-reparametrized} to estimate $a_1(z)$, and then $r$, $r'$, and $r''$ can be formed from $a_1$ and $a_0 = \frac{1}{2} + z^2 a_1$ without catastrophic cancellation using \cref{eq:r-prime-small-reparametrized,eq:r-prime-prime-small-reparametrized}.

% We can then rewrite the first and second derivatives of $r$ as
% %
% \begin{align}
%   r'(z) & = 1 - a_0 - z^2 a_0^2 \\
%   & = 1 - (\frac{1}{2} + z^2 a_1) - z^2 (\frac{1}{2} + z^2 a_1)^2 \\
%   &= \frac{1}{2} - z^2 (a_1 + \frac{1}{4}) - z^4 a_1 - z^6 a_1^2
% \end{align}
% %
% \begin{align}
%   r''(z) & = \frac{2 a_0 - 1}{z} + z a_0 (3 a_0 - 2) + 2 z^3 a_0^3 \\
%   &= \frac{2(\frac{1}{2} + z^2 a_1)-1}{z} - 2z(\frac{1}{2} + z^2 a_1) + 2z(\frac{1}{2} + z^2 a_1)^{2} + 2z^3 (\frac{1}{2} + z^2 a_1)^3 \\
%          & = z (2a_1 - \frac{1}{2}) + \frac{1}{4} z^3 + z^5 a_1 (\frac{3}{2} + 2 a_1) + z^7 (3 a_1^2) + z^9 (2 a_1^3)
% \end{align}

% Finally, we see from \cref{eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu} that we should double-check the quantity $2r' + zr''$ for cancellation issues:
% %
% \begin{align}
%   2r' + zr'' & = 2(1 - a_0 - z^2 a_0^2) + z(z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3) \\
%              & = 2 - 2a_0 - 2z^2 a_0^2 + z^2 (2a_1 + a_0 (3 a_0 - 2)) + 2 z^4 a_0^3 \\
%              & = 2 a_0^3 z^4 + a_0^2 z^2 - 2 a_0 z^2 - 2 a_0 + 2 a_1 z^2 + 2 \\
%              & = 2 - 2a_0 + z^2 (2a_1 - a_0 (2 - a_0)) + 2 a_0^3 z^4
% \end{align}

\subsubsection{Large-$z$ reparametrizations for $r$, $r'$, and $r''$}\label{sec:bessel-ratio-large-z}

As $z \mapsto \infty$, $r \mapsto 1$.
To avoid cancellation issues analogous to the small-$z$ case, we work with $u=1/z$ and rescale the residual $1-r$ to obtain a well-scaled quantity:
%
\begin{align}
  \boxed{
    \begin{aligned}
      b_0(u)           & = \frac{1-r(z)}{u} = \frac{1}{2} + \frac{1}{8} u + \frac{1}{8} u^2 + \mathcal{O}(u^3) \\
      \Rightarrow r(z) & = 1 - u b_0(u) \label{eq:r-large-reparametrized}
    \end{aligned}
  }
\end{align}
%
Approximating $b_0$ with a minimax polynomial allows us to compute $r$ without catastrophic cancellation.
For the first derivative,
%
\begin{align}
  r'(z) & = 1 - \frac{r}{z} - r^2                                                     \\
        & = 1 - u(1 - u b_0) - (1 - u b_0)^2                                          \\
        & = u (2 b_0 - 1) + u^2 b_0 (1 - b_0) \label{eq:r-prime-large-reparametrized}
\end{align}
%
Analogous to the small-$z$ case, this will lead to cancellation issues since $2 b_0 - 1 = \mathcal{O}(u)$.
Thus, we reparametrize to
%
\begin{align}
  b_1(u)             & = \frac{2 b_0(u)-1}{u} = \frac{1}{4} + \frac{1}{4} u + \mathcal{O}(u^2) \\
  \Rightarrow b_0(u) & = \frac{1 + u b_1(u)}{2}
\end{align}
%
which simplifies $r'$ to
%
\begin{align}
  \boxed{
    \begin{aligned}
      r'(z) = u(2 b_0-1) + u^2 b_0(1-b_0) = u^2 (b_1 + b_0(1-b_0)) \label{eq:r-prime-large-reparametrized-simplified}
    \end{aligned}
  }
\end{align}
%
and avoids cancellation issues since $b_1 \mapsto \frac{1}{4}$ and $b_0 \mapsto \frac{1}{2}$ as $u \mapsto 0$, thus $r' = \frac{1}{2} u^2 + \mathcal{O}(u^3)$.
For the second derivative,
%
\begin{align}
  r'' & = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r'                                                                                                  \\
      & = -u(u^2 (b_1 + b_0(1-b_0))) + u^2 (1 - u b_0) - 2 (1 - u b_0) (u^2 (b_1 + b_0(1-b_0)))                                                   \\
      & = u^2 (2 b_0^2 - 2 b_1 - (2 b_0 - 1)) + u^3 (b_1 (2 b_0 - 1) - 2 b_0 + 3b_0^2 - 2 b_0^3)                                                  \\
      & = u^2 (2 b_0^2 - 2 b_1) + u^3 (b_0 (- 2 + 3b_0 - 2 b_0^2) - b_1) + u^4 b_1^2                                                              \\
      & = u^2 (2 b_0^2 - 2 b_1) + u^3 (b_0 (2(2 b_0 - 1) - b_0 (1 + 2 b_0)) - b_1) + u^4 b_1^2                                                    \\
      & = u^2 (2 b_0^2 - 2 b_1) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0) \label{eq:r-prime-prime-large-reparametrized-unsimplified}
\end{align}
%
Since $b_0 = \frac{1}{2} + \mathcal{O}(u)$ and $b_1 = \frac{1}{4} + \mathcal{O}(u)$, we have $b_0^2 - b_1 = \mathcal{O}(u)$, thus the leading term is order $\mathcal{O}(u^3)$, as expected.
To avoid cancellation issues, we introduce
%
\begin{align}
  b_2(u)             & = \frac{4 b_1(u) - 1}{u} = 1 + \mathcal{O}(u) \label{eq:b2-reparametrized} \\
  \Rightarrow b_1(u) & = \frac{1 + u b_2(u)}{4} \label{eq:b1-reparametrized}
\end{align}
%
Then,
%
\begin{align}
  2 b_0^2 - 2 b_1 & = 2(\frac{1 + u b_1}{2})^2 - 2 b_1 = \frac{1}{2} (1 - 4 b_1 + 2 u b_1 + u^2 b_1^2) \\
                  & = \frac{1}{2} u (-b_2 + 2 b_1) + \frac{1}{2} u^2 b_1^2
\end{align}
%
which we substitute into \cref{eq:r-prime-prime-large-reparametrized-unsimplified} and simplify:
%
\begin{align}
  r'' & = u^2 (2 b_0^2 - 2 b_1) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0)                          \\
      & = \frac{1}{2} u^2 (u (-b_2 + 2 b_1) + u^2 b_1^2) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0) \\
      & = -u^3 (\frac{1}{2}b_2 + b_0^2 (1 + 2 b_0)) + u^4 b_1 (\frac{3}{2} b_1 + 2 b_0)
\end{align}
%
yielding
%
\begin{align}\label{eq:r-prime-prime-large-reparametrized}
  \boxed{r''(z) = -\frac{1}{2} u^3 (b_2 + 2 b_0^2 (1 + 2 b_0) - u b_1 (3 b_1 + 4 b_0))}
\end{align}
%
Now $r$, $r'$, and $r''$ can be formed from $b_2$, $b_1 = \frac{1 + u b_2}{4}$, and $b_0 = \frac{1 + u b_1}{2}$ using \cref{eq:r-prime-large-reparametrized,eq:r-prime-prime-large-reparametrized} without catastrophic cancellation.
However, as we will see in \cref{sec:rician-third-derivatives}, a final reparameterization
%
\begin{align}
  b_3(u)             & = \frac{b_2(u)-1}{u} = \frac{25}{16} + \mathcal{O}(u) \\
  \Rightarrow b_2(u) & = 1 + u b_3(u)
\end{align}
%
will be required to stably compute the quantity $2r'+zr''$ needed for third derivatives of $f$ when $z \gg 1$.
Additionally, we will need to compute $1-r-zr'$ stably in \cref{sec:second-derivatives-residual}, which can be done using $b_2$ without requiring further reparameterizations.
Finally, we can stably compute all of $r$, $r'$, $r''$, $2r'+zr''$, and $1-r-zr'$ from one minimax polynomial fit to $b_3(u)$.

% \subsection{Intermediate-$z$ analysis} %TODO?

\subsection{Further Rician derivative simplifications}\label{sec:further-rician-derivative-simplifications}

\subsubsection{First derivatives}\label{sec:rician-first-derivatives}

\paragraph{Small-$z$}

The algebraic simplification of \cref{eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu} leading to \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu} is sufficient to avoid catastrophic cancellation.
In particular, when $\nu \ll 1$, the problematic $\frac{1}{2\nu}$ and $\frac{-x}{2z} = \frac{-1}{2\nu}$ terms in \cref{eq:first-derivatives-unsimplified-nu} are cancelled analytically, avoiding subtraction of two $\mathcal{O}(\nu^{-1})$ terms.
%
\begin{align}\label{eq:first-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_x   & = x - \nu r - \frac{1}{x} \\
      f_\nu & = \nu - x r
    \end{aligned}
  }
\end{align}

\paragraph{Large-$z$}

For $z \gg 1$, we have $r = 1 - b_0(1/z) / z$ from \cref{eq:r-large-reparametrized}, resulting in
%
\begin{align}
  f_x   & = x - \nu (1 - \frac{b_0}{z}) - \frac{1}{x}
  = x - \nu - \frac{1 - b_0}{x}                       \\
  f_\nu & = \nu - x (1 - \frac{b_0}{z})
  = \nu - x + \frac{b_0}{\nu}
\end{align}
%
These forms of $f_x$ and $f_\nu$ are more numerically stable when $x \approx \nu \approx \sqrt{z}$.
The improved numerical stability results from the approximation error being scaled by $1/x$ and $1/\nu$ instead of by $\nu$ and $x$.
If $\hat{r}=r(1+\delta_r)$ and $\hat{b}_0=b_0(1+\delta_b)$, the naive forms incur absolute errors $\mathcal{O}(\nu|\delta_r|)$ in $f_x$ and $\mathcal{O}(x|\delta_r|)$ in $f_\nu$, whereas the simplified forms incur errors $\mathcal{O}(|\delta_b|/x)$ and $\mathcal{O}(|\delta_b|/\nu)$.
Since $z = x\nu \gg 1$ and the relative errors $|\delta_r|$ and $|\delta_b|$ are comparably small, this yields an absolute-error reduction approximately by a factor of $z$.
%
%
\begin{align}\label{eq:first-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_x   & = x - \nu - \frac{1 - b_0}{x} \\
      f_\nu & = \nu - x + \frac{b_0}{\nu}
    \end{aligned}
  }
\end{align}

\subsubsection{Second derivatives}\label{sec:rician-second-derivatives}

\paragraph{Small-$z$}

Similar to the first derivatives, the algebraically simplified expressions in \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu} are sufficient to avoid catastrophic cancellation.
%
%
\begin{align}\label{eq:second-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_{xx}     & = 1 + \frac{1}{x^2} - \nu^2 r' \\
      f_{x\nu}   & = -z (1 - r^2) = -(r+z r')     \\
      f_{\nu\nu} & = 1 - x^2 r'
    \end{aligned}
  }
\end{align}
%
Here $r= z a_0$ and $r' = 1-a_0 - z^2 a_0^2$, with $1-a_0=\frac{1}{2}+\mathcal{O}(z^2)$ and $z^2 a_0^2=\mathcal{O}(z^2)$, so no catastrophic cancellation occurs.
The cross term may use either $-z(1-r^2)$ or $-(r+z r')$:
since $r \approx z/2$ and $zr' \approx z/2$, the former subtracts terms of different magnitudes, and the latter adds terms of the same magnitude but whose leading terms do not suffer cancellation issues.

\paragraph{Large-$z$}

We start by rewriting the second derivatives expressions in \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu} for stability at large-$z$:
%
\begin{align}
  f_{xx}     & = 1 + \frac{1}{x^2} - \nu^2 r' = 1 + \frac{1}{x^2}(1 - z^2 r') \\
  f_{x\nu}   & = -z(1 - r^2) = -z (1 - r) (1 + r)                             \\
  f_{\nu\nu} & = 1 - x^2 r'
\end{align}
%
Recall that for large-$z$, we have from \cref{eq:r-large-reparametrized,eq:r-prime-large-reparametrized-simplified} that
\begin{align}
  r                      & = 1 - \frac{b_0}{z} = 1 - \frac{1}{2z} - \frac{1}{8z^2} + \mathcal{O}(z^{-3})                \\
  r'                     & = \frac{1}{z^2} (b_1 + b_0(1 - b_0)) = \frac{1}{2z^2} + \frac{1}{4z^3} + \mathcal{O}(z^{-4}) \\
  \Rightarrow z(1 - r^2) & = r + z r' = 1 + \frac{1}{z} (b_1 - b_0^2) = 1 + \frac{1}{8z^2} + \mathcal{O}(z^{-3})
\end{align}
where $b_0 = \frac{1}{2} (1 + u b_1)$.
%
We make the following observations:
%
\begin{itemize}
  \item $f_{xx}$:
        since $z^2 r' \approx \frac{1}{2}$, $1 - z^2 r'$ has no cancellation issues and $f_{xx} \approx 1 + \frac{1}{2x^2}$.
  \item $f_{\nu\nu}$:
        we have that $x^2 r' \approx \frac{x^2}{2z^2} = \frac{1}{2\nu^2}$;
        this also has no cancellation issues, and $f_{\nu\nu} \approx 1 - \frac{1}{2\nu^2}$.
  \item $f_{x\nu}$:
        $1 + r$ is stable to compute, but we must rewrite $z(1 - r) = b_0$ to avoid cancellation, so $f_{x\nu} = -b_0(1 + r) \approx -1 - \frac{1}{8z^2}$.
\end{itemize}
%
and therefore the stable forms are:
%
\begin{align}\label{eq:second-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_{xx}     & = 1 + \frac{1}{x^2}(1 - z^2 r') \\
      f_{x\nu}   & = -b_0 (1 + r)                  \\
      f_{\nu\nu} & = 1 - x^2 r'
    \end{aligned}
  }
\end{align}

% \subsubsection{Error analysis}
%
% Following the large-$z$ analysis for the first derivatives, we substitute $r = 1 - \frac{b_0}{z}$ from \cref{eq:r-large-reparametrized} and $r' = \frac{1}{z^2} (b_1 + b_0(1 - b_0))$ from \cref{eq:r-prime-large-reparametrized-simplified} into the second derivatives expressions \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu}:
% %
% \begin{align}\label{eq:second-derivatives-large-z}
%   \boxed{
%     \begin{aligned}
%       f_{xx}     & = 1 + \frac{1 - (b_1 + b_0(1-b_0))}{x^2} \\
%       f_{x\nu}   & = -b_0 (2 - \frac{b_0}{z})               \\
%       f_{\nu\nu} & = 1 - \frac{b_1 + b_0(1-b_0)}{\nu^2}
%     \end{aligned}
%   }
% \end{align}
% %
% To analyze the error propagation, let $\hat{r}=r(1+\delta_r)$, $\hat{r}' = r'(1+\delta_{r'})$, $\hat{b}_0=b_0(1+\delta_{b_0})$, and $\hat{b}_1=b_1(1+\delta_{b_1})$ with comparable small relative errors.
% For the naive forms, we have
% %
% \begin{align}
%   |\hat{f}_{xx} - f_{xx}|         & = \nu^2 |\hat{r}' - r'| = \nu^2 |r'| |\delta_{r'}| \sim \nu^2 \frac{1}{z^2} |\delta_{r'}| = \frac{|\delta_{r'}|}{x^2}      \\
%   |\hat{f}_{\nu\nu} - f_{\nu\nu}| & = x^2 |\hat{r}' - r'| = x^2 |r'| |\delta_{r'}| \sim x^2 \frac{1}{z^2} |\delta_{r'}| = \frac{|\delta_{r'}|}{\nu^2}          \\
%   |\hat{f}_{x\nu} - f_{x\nu}|     & = |\hat{r} + z\hat{r}' - (r + zr')| \le |r| |\delta_r| + z |r'| |\delta_{r'}| \sim |\delta_r| + \frac{1}{z} |\delta_{r'}|.
% \end{align}
% %
% Next, the stable large-$z$ forms:
% Let $c = b_1 + b_0(1 - b_0)$ so that $f_{xx} = 1 + (1-c)/x^2$ and $f_{\nu\nu} = 1 - c/\nu^2$.
% Then, we have that:
% %
% \begin{align}
%   \hat{c} - c & = (\hat{b}_1 - b_1) + (\hat{b}_0 - b_0) - (\hat{b}_0^2 - b_0^2)             \\
%               & = b_1 \delta_{b_1} + b_0 \delta_{b_0} - b_0 \delta_{b_0} (\hat{b}_0 + b_0).
% \end{align}
% %
% This yields the first-order error bounds:
% %
% \begin{align}
%   |\hat{f}_{xx} - f_{xx}|         & = | \frac{c - \hat{c}}{x^2} | \lesssim \frac{|b_1| |\delta_{b_1}| + |b_0| (1 + |\hat{b}_0 + b_0|) |\delta_{b_0}|}{x^2}     \\
%   |\hat{f}_{\nu\nu} - f_{\nu\nu}| & = | \frac{c - \hat{c}}{\nu^2} | \lesssim \frac{|b_1| |\delta_{b_1}| + |b_0| (1 + |\hat{b}_0 + b_0|) |\delta_{b_0}|}{\nu^2} \\
%   |\hat{f}_{x\nu} - f_{x\nu}|     & = | 2 (b_0 - \hat{b}_0) + \frac{\hat{b}_0^2 - b_0^2}{z} | \le 2 (|b_0| + \frac{|b_0||\hat{b}_0 + b_0|}{z}) |\delta_{b_0}|.
% \end{align}

\subsubsection{Third derivatives}\label{sec:rician-third-derivatives}

\paragraph{Small-$z$}

We begin by noting that the third derivative expressions in \cref{eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-nu-nu-nu} depend only on $r'$ and $r''$.
Recall the small-$z$ parametrizations for $r'$ and $r''$ from \cref{eq:r-prime-small-reparametrized,eq:r-prime-prime-small-reparametrized}:
%
\begin{align}
  r'                      & = 1 - a_0 - z^2 a_0^2 = \frac{1}{2} - \frac{3}{16}z^2 + \mathcal{O}(z^4)                          \\
  r''                     & = z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3 = -\frac{3}{8}z + \mathcal{O}(z^3)                     \\
  \Rightarrow 2r' + z r'' & = 2 (1 - a_0) + (a_0 (a_0 - 2) + 2 a_1) z^2 + 2 a_0^3 z^4 = 1 - \frac{3}{4}z^2 + \mathcal{O}(z^3)
\end{align}
%
where $a_0 = \frac{r}{z}$ and $a_1 = \frac{1}{z^2} (a_0 - \frac{1}{2})$.
We see that for small-$z$, the original simplified expressions in \cref{eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-nu-nu-nu}
%
\begin{align}\label{eq:third-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_{xxx}       & = -\frac{2}{x^3} - \nu^3 r'' \\
      f_{xx\nu}     & = -\nu(2r' + z r'')          \\
      f_{x\nu\nu}   & = -x(2r' + z r'')            \\
      f_{\nu\nu\nu} & = -x^3 r''
    \end{aligned}
  }
\end{align}
%
are stable to compute, since neither $r''$ nor $2r' + z r''$ suffer cancellation issues.

\paragraph{Large-$z$}

For large $z$, the expression for $r''$ in \cref{eq:r-prime-prime-large-reparametrized} is stable.
The term $2r'+zr''$ is more delicate.
We derive a stable form from the identity $2r' + zr'' = r' + r/z - 2zrr'$ which follows from \cref{eq:r-prime-large-reparametrized-simplified,eq:r-prime-prime-large-reparametrized}.
Substituting $r=1-ub_0$ and $r'=u^2(b_1+b_0(1-b_0))$ gives
%
\begin{align}
  2r' + zr'' & = r' + r/z - 2zrr'                                              \\
             & = u^2 (b_1+b_0(1-b_0)) + u(1-ub_0) - 2u(1-ub_0)(b_1+b_0(1-b_0)) \\
             & = u ( u(b_1+b_0-b_0^2) + (1-ub_0)(1 - 2(b_1+b_0-b_0^2)) )
\end{align}
%
The term $1-2(b_1+b_0-b_0^2) = -\frac{1}{2}u + \mathcal{O}(u^2)$ is prone to cancellation issues which can be resolved by substituting $b_0=(1+ub_1)/2$ followed by $b_1=(1+ub_2)/4$:
%
\begin{align}
  1-2(b_1+b_0-b_0^2) = \frac{1-4b_1+u^2 b_1^2}{2} = -\frac{1}{2}u(b_2 - u b_1^2)
\end{align}
%
Substituting this back yields
%
\begin{align}
  2r' + zr'' & = u^2 \left( (b_1+b_0-b_0^2) + (1-ub_0)\frac{u b_1^2 - b_2}{2} \right)                                          \\
             & = \frac{1}{2} u^2 \left( (1-b_2) + u (b_2 (\frac{1}{2}+b_0 ) + b_1^2 ) - u^2 b_1^2 (\frac{1}{2} + b_0 ) \right)
\end{align}
%
but now, as anticipated in \cref{sec:bessel-ratio-large-z}, the term $1-b_2$ causes a cancellation issue since $b_2 = 1 + \mathcal{O}(u)$.
Therefore, we make the final change of variables $b_2 = 1 + u b_3$, giving the numerically stable expression:
%
\begin{align}
  2r' + zr'' & = \frac{1}{2} u^2 \left( -u b_3 + u (b_2 (\frac{1}{2}+b_0 ) + b_1^2 \right) - u^2 b_1^2 ( \frac{1}{2} + b_0 )                                     \\
             & = \frac{1}{2} u^3 \left( b_2 (\frac{1}{2}+b_0 ) + b_1^2 - b_3 - u b_1^2 (\frac{1}{2} + b_0) \right). \label{eq:two-rprime-z-r-primeprime-large-z}
\end{align}
%
This expression is $\mathcal{O}(u^3)$ with no remaining cancellation issues.
All terms are computed from $b_0, b_1, b_2$, and $b_3$, which depend on a single minimax approximation for $b_3(u)$.

With these stable forms, the third derivatives for large $z$ are computed as:
\begin{align}\label{eq:third-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_{xxx}       & = -\frac{2}{x^3} - \nu^3 r''                                                                      \\
      f_{xx\nu}     & = -\frac{1}{2 x z^2} \left( b_1^2 - b_3 + (b_2 - \frac{1}{z} b_1^2) (\frac{1}{2} + b_0) \right)   \\
      f_{x\nu\nu}   & = -\frac{1}{2 \nu z^2} \left( b_1^2 - b_3 + (b_2 - \frac{1}{z} b_1^2) (\frac{1}{2} + b_0) \right) \\
      f_{\nu\nu\nu} & = -x^3 r''
    \end{aligned}
  }
\end{align}

\subsection{The quantized Rician distribution}

In magnetic resonance imaging (MRI), the magnitude of the signal in the presence of noise is often modeled with a Gaussian distribution for simplicity.
However, a more physically accurate model is the Rician distribution, which correctly accounts for the non-negativity of magnitude data.
Furthermore, MRI data is invariably quantized during acquisition, meaning the continuous signal is stored as discrete integer values.
A truly faithful statistical model must therefore account for this quantization step.
This leads us to the quantized Rician distribution, which describes the probability of observing a signal within a specific quantization bin.

Given the Rician probability density function $p(y \mid \nu, \sigma)$ from \cref{eq:rician-pdf}, the probability of an observation $y$ falling into a bin $[x, x + \delta)$ is given by the integral of the Rician density over the interval:
%
\begin{align}\label{eq:quantized-rician-pmf}
  p(x \mid \nu, \sigma, \delta) = \int_x^{x + \delta} p(y \mid \nu, \sigma) dy.
\end{align}
%
This defines the probability mass function for the quantized Rician distribution, which we denote $\mathrm{QRice}(\nu, \sigma, \delta)$.

\subsubsection{Numerical evaluation strategy}

We are interested in performing statistical inference procedures such as maximum likelihood estimation (MLE), maximum a posteriori (MAP) estimation, and Markov-chain Monte Carlo (MCMC) sampling under a quantized Rician likelihood model.
These methods require many evaluations of the likelihood and its gradient, necessitating fast and accurate evaluation of the defining integral in \cref{eq:quantized-rician-pmf}.
While adaptive quadrature routines can compute the integral to arbitrary precision, they are generally too slow for this context, especially for GPU-based implementations.
Our goal is to use a fixed-order, non-adaptive quadrature scheme which is significantly faster.

The suitability of such a scheme depends on the integration regime.
Integrating an exponentially decaying function over a large interval in its tails, for example, can be numerically catastrophic for a fixed-order rule.
Fortunately, the nature of statistical inference itself ensures that we predominantly operate in a numerically favourable regime when the noise scale $\sigma$ is a nuisance parameter which is to be estimated jointly with other model parameters.
Crucially, the inference algorithms operate in high-likelihood regions:
%
\begin{itemize}
  \item MLE and MAP explicitly drive the parameters $(\nu,\sigma)$ toward the mode of the likelihood or posterior.
  \item MCMC methods spend the most time sampling from regions of high posterior density, visiting the tails only rarely.
\end{itemize}
%
This is made concrete by considering the high signal-to-noise ratio (SNR) regime, where the Rician distribution is well-approximated by a Gaussian.
For a Gaussian model, a property of the MLE is that the noise variance estimate equals the mean squared error:
%
\begin{align}
  \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \nu_i)^2.
\end{align}
%
This implies that for a well-fit model the residuals satisfy $|x_i - \nu_i| = \mathcal{O}(\sigma)$, i.e., the normalized residual is on the order of one, $|x' - \nu'| = \mathcal{O}(1)$.
Thus, inference naturally results in evaluation of the density near the mode.
Pathological cases where $|x'-\nu'| \gg 1$ correspond to regions of low likelihood that are comparatively less important;
coarse estimates suffice there.

This operating regime is further constrained by practical considerations from signal acquisition.
The signal-to-quantization-noise ratio (SQNR) is engineered to exceed the physical SNR, ensuring quantization error is a smaller source of variance than the physical noise of the signal.
Empirically, the ratio of maximum signal magnitude to quantization bin width is large ($\sim\!500\text{--}1000$), and with 16-bit signal magnitude quantization (Int16) the dynamic range allows a theoretical upper bound of about $2^{15} \approx 30000$.
Consequently, the signal-to-bin-width ratio typically exceeds the SNR, i.e. $x/\delta \gtrsim x/\sigma$, which implies that the normalized integration width satisfies $\delta' = \delta/\sigma \lesssim 1$.
Thus, we have reasonable justification for the following two assumptions about the model parameters:
%
\begin{enumerate}
  \item The observed magnitude $x$ is close to the noncentrality parameter $\nu$, i.e., $|x - \nu| \lesssim \sigma \Rightarrow |x' - \nu'| \lesssim 1$.
  \item The quantization width $\delta$ is not larger than the noise level $\sigma$, i.e. $\delta' = \delta / \sigma \lesssim 1$.
\end{enumerate}
%
Combined, these factors create a best-case scenario for numerical integration:
we are integrating a smooth, well-behaved density over a short interval near its mode.
This justifies the use of a fixed-order quadrature rule.
As a safeguard, when poor parameter choices lead to evaluating the likelihood in the tails (i.e., $|x'-\nu'| \gg 1$), one could increase the quadrature order or fall back to adaptive routines;
such cases are rare under normal inference, and so we leave adaptive quadrature as future work.

\subsubsection{Gauss--Legendre quadrature}

We choose Gauss--Legendre quadrature to approximate the likelihood integral.
On the canonical interval $[-1,1]$, the $N$-point Gauss--Legendre rule approximates
%
\begin{align}
  \int_{-1}^{1} g(x) \, dx \approx \sum_{i=1}^{N} w_i g(x_i),
\end{align}
%
where $\{x_i\}_{i=1}^N$ are the Legendre nodes, defined as the roots of the $N^\text{th}$ Legendre polynomial $P_N(x)$, and $\{w_i\}_{i=1}^N$ the associated positive weights.
This rule is extremely powerful despite its simplicity:
it is exact for all polynomials of degree up to $2N-1$, and for analytic functions the approximation error decreases exponentially fast as $N$ increases.
The rule's fixed nodes and weights are easy to precompute, and the positivity of the weights ensures that no additional cancellation issues are introduced.
Furthermore, the weighted sum structure is trivial to differentiate, making it straightforward to compute gradients and Hessians of the integral approximation.

To apply the rule on $[0,1]$, use the affine change of variables $t = (x+1)/2$, transforming the nodes and weights to $t_i = (x_i+1)/2$ and $w_i/2$, respectively:
%
\begin{align}
  \int_{0}^{1} g(t) \, dt \approx \sum_{i=1}^{N} \frac{w_i}{2} g(t_i).
\end{align}
%
To integrate over $[x, x + \delta)$, change variables to $y = x + \delta t$ with $t\in[0,1]$ and $dy = \delta dt$:
%
\begin{align}
  \int_{x}^{x + \delta} g(y) \, dy = \int_{0}^{1} g(x + \delta t) \, \delta dt
  \approx \delta \sum_{i=1}^{N} \frac{w_i}{2} g(x + \delta t_i).
\end{align}
%
In our regime, $\delta$ is small and $g$ is smooth, so moderate orders $N \approx 8\text{--}16$ achieve high accuracy.

\subsubsection{Log-likelihood formulation and simplification}

Before deriving expressions for the gradient and Hessian, we first simplify the problem by exploiting the scale-invariance of the quantized Rician distribution.
If we change variables in the integrand to $y' = y / a$ for some $a > 0$ and similarly rescale $x$, $\nu$, $\sigma$, and $\delta$, the Rician PDF transforms as
%
\begin{align}
  p(a y' \mid a \nu', a \sigma')
  = \frac{a y'}{(a \sigma')^2} \exp\left(-\frac{(a y')^2+(a \nu')^2}{2(a \sigma')^2}\right) I_0\left(\frac{a y' (a \nu')}{(a \sigma')^2}\right)
  = \frac{1}{a} p(y' \mid \nu', \sigma').
\end{align}
%
Thus, the quantized likelihood integral is scale-invariant:
%
\begin{align}
  p(x \mid \nu, \sigma, \delta)
  = \int_{x/a}^{(x + \delta)/a} p(a y' \mid a\nu', a\sigma') (a dy') = \int_{x'}^{x'+\delta'} p(y' \mid \nu', \sigma') dy'
  = p(x' \mid \nu', \sigma', \delta').
\end{align}
%
Similar to the Rician distribution analysis, we choose $a = \sigma$ such that $\sigma'=1$;
for the remainder of this analysis, we will work with these normalized parameters and write $p(x \mid \nu, \delta)$ as shorthand for $p(x \mid \nu, \sigma=1, \delta)$, dropping the prime annotations.

In terms of $f$, the negative log-likelihood of the standard Rician distribution from \cref{eq:rician-neg-log-likelihood}, the quantized Rician likelihood is given by
%
\begin{align}
  p(x \mid \nu, \delta) = \int_x^{x + \delta} \exp(-f(y, \nu)) dy \coloneqq I(\theta)
\end{align}
%
where we have introduced a shorthand notation for the normalized parameters $\theta = (x, \nu, \delta)$ and the integral $I(\theta)$.
Similarly, we introduce $\Omega(\theta) = -\log I(\theta)$ for the negative logarithm of the quantized likelihood:
%
\begin{align}
  -\log p(x \mid \nu, \delta) = -\log I(\theta) \coloneqq \Omega(\theta).
\end{align}
%
We now make the affine change of variables $y = x + \delta t$ such that the integration bounds are independent of $\theta$, resulting in an integral over the unit interval $t \in [0, 1]$:
%
\begin{align}
  I(\theta)                  & = \int_0^1 \exp(-f(x + \delta t, \nu)) \, \delta \, dt                       \\
  \Rightarrow \Omega(\theta) & = -\log\left(\int_0^1 \exp(-f(x + \delta t, \nu)) \, dt\right) - \log\delta.
\end{align}
%
This expression has the form of a log-partition function;
denote the energy function as $\tilde{f}(t, \theta) = f(x + \delta t, \nu)$ and the partition function as
%
\begin{align}
  Z(\theta) = \int_0^1 \exp(-\tilde{f}(t, \theta)) \, dt.
\end{align}
%
Then, $\Omega(\theta) = -\log Z(\theta) - \log\delta$, and we can define a probability density over $t \in [0,1]$ as
%
\begin{align}
  P(t \mid \theta) = \frac{\exp(-\tilde{f}(t, \theta))}{Z(\theta)}.
\end{align}
%
This formulation will prove useful for computing gradients and higher derivatives of $\Omega$.

\subsubsection{Numerical differentiation strategy}

We differentiate the fixed-node Gauss--Legendre approximation to $Z(\theta)$, and hence to $\Omega(\theta)$, rather than applying the Leibniz integral rule to the true integral.
This yields a coherent discretization: the gradients and higher derivatives are exact for the discretized integral.
In optimization, mixing a discretized integral objective with derivatives of the exact integral can produce minima with nonzero gradient and unreliable Hessian curvature tests.
For gradient-based MCMC, proposals use gradients while acceptance depends on likelihood ratios; inconsistency between likelihoods and gradients can undermine both correctness and efficiency.
Furthermore, exact endpoint formulas require differences such as $e^{-f(x+\delta,\nu)} - e^{-f(x,\nu)}$ and $f_x(x+\delta,\nu) - f_x(x,\nu)$, which catastrophically cancel when $\delta \ll 1$ and require careful handling at $x=0$ where $f$ has a logarithmic singularity; fixed-node Gauss--Legendre uses interior nodes and avoids this issue.
Differentiating the quadrature therefore provides stable averages of well-conditioned integrand derivatives, resulting in consistent $\Omega$ values and derivatives.

\subsubsection{The residual likelihood method}

Since the quadrature scheme approximates the partition function $Z(\theta)$ as a weighted sum of positive terms, $\log Z(\theta)$ can be stably computed using the log-sum-exp trick:
%
\begin{align}
  -\log Z(\theta)
  = -\log\left(\sum_i \frac{w_i}{2} \exp(-\tilde{f}(t_i, \theta))\right)
  = \tilde{f}_{\min}(\theta) - \log\left(\sum_i \frac{w_i}{2} \exp(\tilde{f}_{\min}(\theta) - \tilde{f}(t_i, \theta))\right)
\end{align}
%
where $\tilde{f}_{\min}(\theta) = \min_i \tilde{f}(t_i, \theta)$, avoiding naive overflow.
However, a more subtle numerical issue arises at high SNR ($x \approx \nu$).
In this regime, the Rician negative log-likelihood $f$ is very nearly Gaussian:
%
\begin{align}
  f(x, \nu) \approx f_G(x,\nu) = \frac{(x-\nu)^2}{2} + \frac{1}{2}\log(2\pi).
\end{align}
%
In other words, the residual energy $\tilde{r}(t,\theta) = \tilde{f}(t,\theta) - f_G(x,\nu) = f(x + \delta t, \nu) - f_G(x,\nu)$ is relatively small.
Since both integration and Gauss--Legendre quadrature are linear functionals, we can factor $\exp(-f_G(x,\nu))$ out of the $Z(\theta)$ integral, resulting in a better-conditioned quadrature problem via the residual energy $\tilde{r}$:
%
\begin{align}
  -\log Z(\theta)
  = - \log\left(\int_0^1 \exp(-(\tilde{r}(t,\theta) + f_G(x,\nu))) \, dt \right)
  = f_G(x,\nu) - \log\left(\int_0^1 \exp(-\tilde{r}(t,\theta)) \, dt \right). \label{eq:logz-residual-derivation}
\end{align}
%
This transformation is most beneficial at large $z$ but -- as we will see -- introduces no numerical issues at small $z$, so we apply it universally for simplicity.

\subsubsection{Properties and derivatives of the residual energy}\label{sec:residual-energy-properties-and-derivatives}

\paragraph{Properties of the residual energy}

Since $f_G$ is quadratic in $x$ and $\nu$ and independent of $\delta$, it follows from $\tilde{r} = \tilde{f} - f_G$ that third and higher-order partial derivatives of $\tilde{r}$ with respect to $x$ and $\nu$ are equal to those of $\tilde{f}$.
Similarly, any partial derivative of $\tilde{r}$ with respect to $\delta$ equals that of $\tilde{f}$.
We therefore need only derive stable expressions for $\tilde{r}$ and its first and second partial derivatives with respect to $x$ and $\nu$.
From \cref{eq:rician-neg-log-likelihood-scaled}, we have:
%
\begin{align}
  \tilde{r}(t,\theta) = f(y, \nu) - f_G(x,\nu) & = \frac{(y-\nu)^2 - (x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{y}{\nu}\right) - \log \hat{I}_0(y \nu)                                               \\
                                               & = \delta t \left(x - \nu + \frac{\delta t}{2}\right) - \frac{1}{2}\log\left(\frac{y}{\nu}\right) - \log \hat{I}_0(y \nu) \label{eq:residual-energy}
\end{align}
%
where $y = x + \delta t$.
The first term in \cref{eq:residual-energy} is small when $|x-\nu| \lesssim 1$ and $\delta \lesssim 1$.
The size of the second and third terms $\tilde{c}(y,\nu) = \frac{1}{2}\log(y/\nu) + \log\hat{I}_0(y\nu)$ depends on the regime of $z=y\nu$.

\paragraph{Small $z=y\nu$}

Rewriting $\tilde{c}(y,\nu) = \log y + \frac{1}{2}\log(2\pi) + \log I_0(z) - z$ and Taylor expanding $\log I_0(z)$, we have
%
\begin{align}\label{eq:c-small-z}
  \tilde{c}(y,\nu) = \log y + \frac{1}{2}\log(2\pi) - z + \frac{z^2}{4} + \mathcal{O}(z^4).
\end{align}

\paragraph{Large $z=y\nu$}

Using the asymptotic expansion $\log \hat{I}_0(z) = \frac{1}{8z} + \mathcal{O}(z^{-2})$, we have
%
\begin{align}\label{eq:c-large-z}
  \tilde{c}(y,\nu) = \frac{1}{2}\log\left(\frac{y}{\nu}\right) + \frac{1}{8z} + \mathcal{O}(z^{-2}).
\end{align}

\paragraph{High-SNR $\nu \approx y \gg 1$}

Let $y = x + \delta t = \nu + \epsilon$ with $|\epsilon| = |x - \nu + \delta t| \lesssim 1$ and $\nu$ large.
Then, expanding \cref{eq:c-large-z} to first order in $\epsilon/\nu \ll 1$ gives:
%
\begin{align}
  \tilde{c}(\nu+\epsilon, \nu) & = \frac{1}{2}\log\left(1 + \frac{\epsilon}{\nu}\right) + \frac{1}{8\nu(\nu+\epsilon)} + \mathcal{O}(z^{-2})                                                                                                   \\
                               & = \frac{\epsilon}{2\nu} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right) + \frac{1}{8\nu^2}\left(1 - \frac{\epsilon}{\nu} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right)\right) + \mathcal{O}(\nu^{-4}) \\
                               & = \frac{\epsilon}{2\nu}\left(1 - \frac{1}{4\nu^2}\right) + \frac{1}{8\nu^2} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right).
\end{align}
%
Thus, $\tilde{c}$ is small in the high-SNR regime and therefore $\tilde{r} = \delta t (\epsilon - \frac{\delta t}{2}) - \tilde{c}$ is also small when $\delta \lesssim 1$.

\paragraph{First derivatives of the residual}\label{sec:first-derivatives-residual}

The first partial derivatives of $\tilde{r}$ can be simplified using \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu}:
%
\begin{align}
  \tilde{r}_x   & = f_x(y,\nu) - (x-\nu) = (y - \nu r - \frac{1}{y}) - (x-\nu) = \delta t + \nu (1 - r) - \frac{1}{y} \\
  \tilde{r}_\nu & = f_\nu(y,\nu) - (\nu-x) = (\nu - y r) - (\nu-x) = -\delta t r + x (1 - r)
\end{align}
%
These forms are stable for small $z = y \nu$, but for large $z$, we substitute $1-r = \frac{b_0}{z}$ from \cref{eq:r-large-reparametrized}:
%
\begin{align}
  \tilde{r}_x   & = \delta t + \nu (1 - r) - \frac{1}{y} = \delta t - \frac{1 - b_0}{y}           \\
  \tilde{r}_\nu & = -\delta t r + x (1 - r) = -\delta t + y (1 - r) = -\delta t + \frac{b_0}{\nu}
\end{align}

\paragraph{Second derivatives of the residual}\label{sec:second-derivatives-residual}

The second partial derivatives of $\tilde{r}$ can be simplified using \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu}:
%
\begin{align}
  \tilde{r}_{xx}     & = f_{xx} - 1 = \frac{1}{y^2} - \nu^2 r'          \\
  \tilde{r}_{x\nu}   & = f_{x\nu} + 1 = 1 - (r + z r') = 1 - z(1 - r^2) \\
  \tilde{r}_{\nu\nu} & = f_{\nu\nu} - 1 = - y^2 r'
\end{align}
%
Similar to the small- and large $z$ expressions in \cref{eq:second-derivatives-small-z,eq:second-derivatives-large-z}, the above expressions for $\tilde{r}_{xx}$ and $\tilde{r}_{\nu\nu}$ are numerically stable.
However, for large $z$, the mixed-derivative $\tilde{r}_{x\nu}$ requires some care, since $f_{x\nu} = -1 + \mathcal{O}(z^{-2})$ and so computing $\tilde{r}_{x\nu} = \mathcal{O}(z^{-2})$ naively leads to catastrophic cancellation.
A stable form can be derived by substituting the large-$z$ reparametrizations $1 - r = u b_0$, $2 b_0 - 1 = u b_1$, and $4 b_1 - 1 = u b_2$ where $u = \frac{1}{z}$:
%
\begin{align}
  1 - r - z r' & = 1 - z(1 - r^2) = 1 - z(1 - r)(1 + r)                                                                                            \\
               & = 1 - b_0 (1 + r) = 1 - b_0(2 - u b_0) = (1 - 2 b_0) + u b_0^2 = -u b_1 + u b_0^2                                                 \\
               & = u (b_0^2 - b_1) = u ((\frac{1 + u b_1}{2})^2 - b_1) = \frac{u}{4} (1 - 4b_1 + 2u b_1 + u^2 b_1^2)                               \\
               & = \frac{u}{4}(-u b_2 + 2u b_1 + u^2 b_1^2) = \frac{u^2}{4}(2 b_1 - b_2 + u b_1^2). \label{eq:one-minus-r-minus-z-r-prime-large-z}
\end{align}
%
This final expression is stable to compute and $\mathcal{O}(z^{-2})$ as expected.

\subsubsection{Expectations and derivatives under $P(t \mid \theta)$}

\paragraph{Expectations over $P(t \mid \theta)$}\label{sec:expectation-covariance}

Since $f_G(x,\nu)$ is constant with respect to $t$, we can factor $\exp(-f_G(x,\nu))$ out of the numerator and denominator of $P(t \mid \theta)$, resulting in the more numerically stable form
%
\begin{align}
  P(t \mid \theta)
  = \frac{\exp(-\tilde{f}(t, \theta))}{\int_0^1 \exp(-\tilde{f}(t', \theta)) \, dt'}
  = \frac{\exp(-\tilde{r}(t, \theta))}{\int_0^1 \exp(-\tilde{r}(t', \theta)) \, dt'}.
\end{align}
%
In the high SNR regime with $\delta$ small such that $\tilde{r}$ is small, we can compute $\log P$ in a more stable form:
%
\begin{align}
  \log P(t \mid \theta)
   & = -\log \left( \int_0^1 \exp(\tilde{r}(t, \theta) - \tilde{r}(t', \theta)) \, dt' \right)        \\
   & = -\log \left(1 + \int_0^1 \exp(\tilde{r}(t, \theta) - \tilde{r}(t', \theta)) - 1 \, dt' \right)
\end{align}
%
where $\log(1 + \epsilon)$ and $\exp(\epsilon - 1)$ are computed stably for small $\epsilon$ using the special-function routines \texttt{log1p($\epsilon$)} and \texttt{expm1($\epsilon$)}.

\paragraph{Differentiating expectations over $P(t \mid \theta)$}\label{sec:jacobian-expectation-covariance}

Let $g : [0, 1] \times \mathbb{R}^3 \mapsto \mathbb{R}^m$, and denote as $\mathbb{E}[g] \in \mathbb{R}^m$ the expectation of $g(t, \theta)$ with respect to the density $P(t \mid \theta)$.
The Jacobian $\nabla_\theta \mathbb{E}[g] \in \mathbb{R}^{m \times 3}$ is given by
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{f}) \label{eq:jacobian-expectation}          \\
                              & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{r}) \label{eq:jacobian-expectation-residual}
\end{align}
%
where, denoting $\mu_X = \mathbb{E}[X]$ and $\mu_Y = \mathbb{E}[Y]$, the covariance is defined as usual as
%
\begin{align}
  \mathrm{Cov}(X, Y) = \mathbb{E}[(X - \mu_X)(Y - \mu_Y)^T] = \mathbb{E}[X Y^T] - \mu_X \mu_Y^T.
\end{align}
%
\Cref{eq:jacobian-expectation,eq:jacobian-expectation-residual} follow from differentiating the definition of the expectation $\mathbb{E}[g] = \int_0^1 g(t, \theta) P(t \mid \theta) \, dt$;
a complete proof is provided in \Cref{app:proof-jacobian-expectation}.
Crucially, \cref{eq:jacobian-expectation,eq:jacobian-expectation-residual} also hold exactly when the expectation is computed via a fixed-node quadrature, $\mathbb{E}[g] \approx \sum_i \frac{w_i}{2} g(t_i, \theta) P(t_i \mid \theta)$, since both integration and quadrature are linear functionals of the integrand $t \mapsto g(t,\theta) P(t \mid \theta)$ and thus commute with differentiation.

\paragraph{Differentiating covariances with respect to $P(t \mid \theta)$}\label{sec:derivative-covariance}

For any two scalar functions $g(t,\theta)$ and $h(t,\theta)$, the derivative of their covariance with respect to a parameter $\gamma \in \{x, \nu, \delta\}$ is given by
%
\begin{align}\label{eq:derivative-covariance-identity}
  \partial_{\gamma} \mathrm{Cov}(g, h) = \mathrm{Cov}(\partial_{\gamma} g, h) + \mathrm{Cov}(g, \partial_{\gamma} h) - \mathrm{Cov3}(g, h, \partial_{\gamma} \tilde{r}),
\end{align}
%
where $\mathrm{Cov3}(X,Y,Z) = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])(Z-\mathbb{E}[Z])]$ is the third joint central moment;
a complete proof is provided in \Cref{app:proof-covariance-derivative}.

\subsubsection{First derivatives}\label{sec:qrician-first-derivatives}

We compute the gradient of $\Omega = -\log Z - \log\delta$ in terms of the gradient of $-\log Z$:
%
\begin{align}
  \nabla_{\theta} (-\log Z) = -\frac{\nabla_{\theta} Z}{Z} = -\frac{1}{Z}\int_0^1 -\nabla_{\theta} \tilde{f} \, \exp(-\tilde{f}) \, dt = \mathbb{E}[\nabla_{\theta} \tilde{f}].
\end{align}
%
Rewriting the expectation in terms of the residual energy $\tilde{r}$, we have
%
\begin{align}
  \mathbb{E}[\nabla_{\theta} \tilde{f}] = \mathbb{E}[\nabla_{\theta} (\tilde{r} + f_G)] = \mathbb{E}[\nabla_{\theta} \tilde{r}] + \nabla_{\theta} f_G
\end{align}
%
since $f_G$ is constant with respect to $t$.
Finally, the gradient $\nabla_{\theta} \Omega$ simplifies to
%
\begin{align}
  \nabla_{\theta} \Omega & = - \nabla_\theta \log Z - \nabla_\theta \log\delta = \mathbb{E}[\nabla_{\theta} \tilde{f}] - (0, 0, \delta^{-1})^T \\
                         & = \mathbb{E}\left[ \begin{pmatrix} \tilde{r}_x \\ \tilde{r}_\nu \\ t \tilde{f}_x \end{pmatrix} \right]
  + \begin{pmatrix} x - \nu \\ \nu - x \\ -\delta^{-1} \end{pmatrix}
\end{align}
%
where $\frac{\partial}{\partial x} f_G = x - \nu$, $\frac{\partial}{\partial \nu} f_G = \nu - x$, and $\tilde{r}_x$, $\tilde{r}_\nu$, and $f_x$ are evaluated at $y=x + \delta t$ as in \cref{sec:first-derivatives-residual}.

\subsubsection{Second derivatives}\label{sec:qrician-second-derivatives}

We can compute the Hessian of $\Omega = -\log Z - \log\delta$ by differentiating $\nabla_{\theta} (-\log Z)$ from \cref{sec:qrician-first-derivatives} using the gradient of the expectation identity \cref{eq:jacobian-expectation}:
%
\begin{align}
  \nabla_{\theta}^2 (-\log Z) = \nabla_\theta ( \mathbb{E}[\nabla_{\theta} \tilde{f}] ) = \mathbb{E}[\nabla_{\theta}^2 \tilde{f}] - \mathrm{Cov}(\nabla_{\theta} \tilde{f}, \nabla_{\theta} \tilde{f})
\end{align}
%
Substituting $\tilde{f} = \tilde{r} + f_G$ and recalling that $f_G$ is constant with respect to $t$, we have
%
\begin{align}
  \nabla_{\theta}^2 (-\log Z)                                                     & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r} + \nabla_{\theta}^2 f_G] - \mathrm{Cov}(\nabla_{\theta} \tilde{r} + \nabla_{\theta} f_G, \nabla_{\theta} \tilde{r} + \nabla_{\theta} f_G)  \\
                                                                                  & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r}] + \nabla_{\theta}^2 f_G - \mathrm{Cov}(\nabla_{\theta} \tilde{r}, \nabla_{\theta} \tilde{r}) \label{eq:residual-hessian-neglogz}          \\
  \Rightarrow \nabla_{\theta}^2 \Omega = \nabla_{\theta}^2 (-\log Z - \log\delta) & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r}] - \mathrm{Cov}(\nabla_{\theta} \tilde{r}, \nabla_{\theta} \tilde{r}) + \begin{pmatrix} 1 & -1 & 0 \\ -1 & 1 & 0 \\ 0 & 0 & \delta^{-2} \end{pmatrix} \label{eq:residual-hessian-qrician}
\end{align}
%
where $\frac{\partial^2}{\partial x^2} f_G = \frac{\partial^2}{\partial \nu^2} f_G = 1$, $\frac{\partial^2}{\partial x \partial \nu} f_G = -1$.
The Hessian of the residual energy $\nabla_{\theta}^2 \tilde{r}$ is
%
\begin{align}
  \nabla_{\theta}^2 \tilde{r} = \begin{pmatrix} \tilde{r}_{xx} & \tilde{r}_{x\nu} & t \tilde{f}_{xx} \\ \tilde{r}_{x\nu} & \tilde{r}_{\nu\nu} & t \tilde{f}_{x\nu} \\ t \tilde{f}_{xx} & t \tilde{f}_{x\nu} & t^2 f_{xx} \end{pmatrix}
\end{align}
%
where $\tilde{r}_{xx}$, $\tilde{r}_{x\nu}$, and $\tilde{r}_{\nu\nu}$ are evaluated at $y=x + \delta t$ as in \cref{sec:second-derivatives-residual}.

\subsubsection{Third derivatives and vector-Jacobian products}

Differentiating through statistical inference procedures, such as Laplace's approximation, requires gradients of scalar functions of the log-likelihood Hessian, $\mathcal{L}(\nabla_\theta^2 \Omega(\theta))$.
This necessitates third-order derivatives of $\Omega(\theta)$.
We derive the components of the 3-tensor $\nabla_\theta^3 \Omega(\theta)$ and the vector-Jacobian product (VJP) for the map $\theta \mapsto \nabla_\theta^2 \Omega(\theta)$ required for backpropagation.

\paragraph{Third derivatives of $\Omega$}

The components of the VJP are linear combinations of the third-order partial derivatives of $\Omega = -\log Z - \log\delta$.
We use the shorthand $\partial_i \coloneqq \frac{\partial}{\partial \theta_i}$ where $\theta = (x, \nu, \delta)^T$, and similarly $\partial_{ij} \coloneqq \frac{\partial^2}{\partial \theta_i \partial \theta_j}$ and $\partial_{ijk} \coloneqq \frac{\partial^3}{\partial \theta_i \partial \theta_j \partial \theta_k}$, and derive the third derivatives by differentiating the Hessian $\nabla_{\theta}^2 (-\log Z)$ from \cref{eq:residual-hessian-neglogz} with respect to a parameter $\theta_k$:
%
\begin{align}
  \partial_{k}(\nabla_{\theta}^2 (-\log Z)) = \partial_{k} \mathbb{E}[\nabla_\theta^2 \tilde{r}] - \partial_{k}\mathrm{Cov}(\nabla_\theta \tilde{r}, \nabla_\theta \tilde{r}).
\end{align}
%
The components of the third-derivative tensor follow by applying \cref{eq:jacobian-expectation-residual,eq:derivative-covariance-identity} to differentiate the expectation and covariance terms:
%
\begin{align}
  \partial_{k} \mathbb{E}[\partial_{ij} \tilde{r}]                         & = \mathbb{E}[\partial_{ijk} \tilde{r}] - \mathrm{Cov}(\partial_{ij} \tilde{r}, \partial_{k} \tilde{r})                                                                                                                                                                                                                                                             \\
  \partial_{k}\mathrm{Cov}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}) & = \mathrm{Cov}(\partial_{ik} \tilde{r}, \partial_{j} \tilde{r}) + \mathrm{Cov}(\partial_{i} \tilde{r}, \partial_{jk} \tilde{r}) - \mathrm{Cov3}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}, \partial_{k} \tilde{r})                                                                                                                                            \\
  \Rightarrow \partial_{ijk} (-\log Z)                                     & = \mathbb{E}[\partial_{ijk} \tilde{r}] - \mathrm{Cov}(\partial_{ij} \tilde{r}, \partial_{k} \tilde{r}) - \mathrm{Cov}(\partial_{ik} \tilde{r}, \partial_{j} \tilde{r}) - \mathrm{Cov}(\partial_{jk} \tilde{r}, \partial_{i} \tilde{r}) + \mathrm{Cov3}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}, \partial_{k} \tilde{r}) \label{eq:neglogz-third-derivative} \\
  \Rightarrow \partial_{ijk} \Omega                                        & = \partial_{ijk} (-\log Z) - 2\delta^{-3} \mathbf{1}_{\{\theta_i=\theta_j=\theta_k=\delta\}}. \label{eq:omega-third-derivative-qrician}
\end{align}
%
We now discuss two strategies for the efficient evaluation of $\partial_{ijk}(-\log Z)$.

\paragraph{Raw-moment formulation}

We rewrite the covariances and third central moments in terms of raw moments.
Let $\mu_{i} = \mathbb{E}[\partial_{i} \tilde{r}]$, $\mu_{ij} = \mathbb{E}[\partial_{i} \tilde{r} \partial_{j} \tilde{r}]$, and $\mu_{ijk} = \mathbb{E}[\partial_{i} \tilde{r} \partial_{j} \tilde{r} \partial_{k} \tilde{r}]$.
Then,
%
\begin{align}
  \mathrm{Cov}(\partial_{ij} \tilde{r}, \partial_{k} \tilde{r})                         & = \mathbb{E}[\partial_{ij} \tilde{r} \partial_{k} \tilde{r}] - \mathbb{E}[\partial_{ij} \tilde{r}] \mu_{k}                                                                                       \\
  \mathrm{Cov3}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}, \partial_{k} \tilde{r}) & = \mu_{ijk} - \mu_{i} \mu_{jk} - \mu_{j} \mu_{ik} - \mu_{k} \mu_{ij} + 2 \mu_{i} \mu_{j} \mu_{k}                                                                                                 \\
  \Rightarrow \partial_{ijk} (-\log Z)                                                  & = \mathbb{E}[\partial_{ijk} \tilde{r}] + \mu_{ijk} - \mu_{i} \mu_{jk} - \mu_{j} \mu_{ik} - \mu_{k} \mu_{ij} + 2 \mu_{i} \mu_{j} \mu_{k} \label{eq:neglogz-third-derivative-raw-moment}           \\
                                                                                        & - \mathbb{E}[\partial_{ij} \tilde{r} \partial_{k} \tilde{r}] - \mathbb{E}[\partial_{ik} \tilde{r} \partial_{j} \tilde{r}] - \mathbb{E}[\partial_{jk} \tilde{r} \partial_{i} \tilde{r}] \nonumber \\
                                                                                        & + \mathbb{E}[\partial_{ij} \tilde{r}] \mu_{k} + \mathbb{E}[\partial_{ik} \tilde{r}] \mu_{j} + \mathbb{E}[\partial_{jk} \tilde{r}] \mu_{i}. \nonumber
\end{align}
%
For a three-parameter system, a minimal basis of raw moments requires integrating 25 unique quantities.
The derivatives $\partial_{ijk} (-\log Z)$ are then assembled from sums and products of these raw moments.

\paragraph{Central moment formulation}

Let the centred first derivatives be given by $c_{i} = \partial_{i} \tilde{r} - \mu_{i}$ where $\mu_{i} = \mathbb{E}[\partial_{i} \tilde{r}]$.
Then, $\partial_{ijk}(-\log Z)$ can be rewritten as an expectation:
%
\begin{align}
  \mathrm{Cov3}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}, \partial_{k} \tilde{r}) & = \mathbb{E}[c_{i} c_{j} c_{k}]                                                                                                                                                                           \\
  \mathrm{Cov}(\partial_{ij} \tilde{r}, \partial_{k} \tilde{r})                         & = \mathbb{E}[\partial_{ij} \tilde{r} (\partial_{k} \tilde{r} - \mathbb{E}[\partial_{k} \tilde{r}])] = \mathbb{E}[\partial_{ij} \tilde{r} c_{k}]                                                           \\
  \Rightarrow \partial_{ijk} (-\log Z)                                                  & = \mathbb{E} [ \partial_{ijk} \tilde{r} - \partial_{ij} \tilde{r} c_{k} - \partial_{ik} \tilde{r} c_{j} - \partial_{jk} \tilde{r} c_{i} + c_{i} c_{j} c_{k} ]. \label{eq:neglogz-two-pass-central-moment}
\end{align}
%
After computing the 3 means, this requires integrating 6 unique second-order and 10 unique third-order derivatives.
While this method requires evaluating first-derivatives twice, the linearity of the expectation in \cref{eq:neglogz-two-pass-central-moment} can make for more efficient vector-Jacobian products than \cref{eq:neglogz-third-derivative-raw-moment} since contraction with a cotangent $\Delta$ can move inside the expectation (i.e. $\Delta \bullet \mathbb{E}[X] = \mathbb{E}[\Delta \bullet X]$), resulting in fewer scalar integrands.

\paragraph{Vector-Jacobian product of $\nabla_\theta^2 \Omega$}

The vector-Jacobian product (VJP) is the core operation in reverse-mode automatic differentiation.
We define the VJP in terms of the tensor contraction
%
\begin{align}\label{eq:general-bullet-contraction}
  (\Delta \bullet J)_{j_1, \ldots, j_q} \coloneqq \sum_{i_1, \ldots, i_p} \Delta_{i_1, \ldots, i_p} \, J_{i_1, \ldots, i_p, j_1, \ldots, j_q}
\end{align}
%
where $\Delta \in \mathbb{R}^{m_1 \times \cdots \times m_p}$ is a rank $p$ \textit{cotangent} tensor and $J \in \mathbb{R}^{m_1 \times \cdots \times m_p \times n_1 \times \cdots \times n_q}$ is a rank $p+q$ Jacobian tensor.
As a concrete example, for an $\mathbb{R}^n \to \mathbb{R}^m$ function with Jacobian $J \in \mathbb{R}^{m \times n}$ and cotangent $\Delta \in \mathbb{R}^m$,
%
\begin{align}
  (\Delta \bullet J)_j = \sum_{i=1}^m \Delta_i J_{i j} = (\Delta^{T} J)_j
\end{align}
%
which recovers the traditional VJP.
Now, let $\mathcal{L}$ be a scalar function depending on the Hessian $\nabla_\theta^2 \Omega$.
The pullback $\mathcal{B}$ for the map $\theta \mapsto \nabla_\theta^2 \Omega(\theta)$ transforms the symmetric cotangent matrix $\Delta$ with components $\Delta_{ij} = \frac{\partial \mathcal{L}}{\partial (\partial_{ij} \Omega)}$ to the input cotangent vector $\frac{\partial \mathcal{L}}{\partial \theta}$:
%
\begin{align}
  \mathcal{B}(\Delta)_k \coloneqq \frac{\partial \mathcal{L}}{\partial \theta_k} = \sum_{i,j} \frac{\partial \mathcal{L}}{\partial (\partial_{ij} \Omega)} \frac{\partial (\partial_{ij} \Omega)}{\partial \theta_k} = \sum_{i,j} \Delta_{ij} \partial_{ijk} \Omega = (\Delta \bullet \nabla_\theta^3 \Omega)_k. \label{eq:vjp-omega-hessian}
\end{align}
%
% We note that, by linearity, $(\Delta \bullet \nabla_\theta^3 \Omega)_k = \partial_k (\Delta \bullet \nabla_\theta^2 \Omega)$, which may be more computationally efficient in some cases.
Using the third-derivative results from \cref{eq:omega-third-derivative-qrician,eq:neglogz-two-pass-central-moment}, the pullback \cref{eq:vjp-omega-hessian} evaluates to
%
\begin{align}
  \mathcal{B}(\Delta)_k = \mathbb{E} \bigg[ \sum_{i,j} \Delta_{ij} (\partial_{ijk} \tilde{r} - \partial_{ij} \tilde{r} c_k - \partial_{ik} \tilde{r} c_j - \partial_{jk} \tilde{r} c_i + c_i c_j c_k) \bigg] - 2 \delta^{-3} \Delta_{33} \mathbf{1}_{\{\theta_k=\delta\}}
\end{align}
%
where the sum over $i,j$ has been moved inside the expectation.

% Figure: Bessel ratio accuracy
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{bessel_ratio_accuracy_figure.pdf}
  \caption{Error envelopes (maxima over sliding windows) for naive vs.\ proposed evaluation of $r(z) = I_1(z)/I_0(z)$, $r'(z)$, $r''(z)$, $2 r'(z) + z r''(z)$, and $1 - r(z) - z r'(z)$ over 1001 points $z \in [10^{-6}, 10^{6}]$. Rows (top to bottom) show $e_{\mathrm{abs}} = |\hat{y} - y|$ and $e_{\mathrm{rel}} = |\hat{y} - y| / |y|$ for single precision, then for double precision. Horizontal reference lines indicate machine epsilon for single (dotted) and double precision (dashed).}
  \label{fig:bessel-accuracy}
\end{figure}

% Table: Bessel ratio accuracy
\begin{table}[t]
  \centering
  \input{figures/output/bessel_ratio_accuracy_table.tex}
  \caption{Maximum error $e_{\min} = \min(|\hat{y} - y|, |\hat{y} - y| / |y|)$ over $z \in [10^{-6}, 10^{6}]$ for $r(z)$, $r'(z)$, $r''(z)$, $2 r'(z) + z r''(z)$, and $1 - r(z) - z r'(z)$, comparing naive vs.\ proposed implementations in single and double precision.}
  \label{fig:bessel-table}
\end{table}

% Table: NLL accuracy table
\begin{table}[t]
  \centering
  \resizebox{\textwidth}{!}{\input{figures/output/nll_accuracy_table.tex}}
  \caption{Maximum error $e_{\min} = \min(|\hat{y} - y|, |\hat{y} - y| / |y|)$ over $(x, \nu)$ pairs with $(\log_{10} x, \log_{10} \nu) \in [-3, 3]^2$ for $f$ and its first through third derivatives. Columns ``Bessels.jl'' and ``SF.jl'' use AD-based baselines using Bessels.jl and SpecialFunctions.jl, respectively; ``Naive'' uses numerically unstable expressions equivalent to manual AD; ``Proposed'' uses the derived stable expressions.}
  \label{fig:nll-table}
\end{table}

% Figure: QRice accuracy vs order
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{qrice_accuracy.pdf}
  \caption{Error vs.\ Gauss--Legendre order $N$ for $\Omega$, $\nabla\Omega$, $\nabla^2\Omega$, and $\nabla^3\Omega$. For each precision and each $(\delta, N)$, we take the maximum $e_{\min} = \min(|\hat{y} - y|, |\hat{y} - y| / |y|)$ over 1024 samples $\nu$ with $\log_{10}\nu \in [-3, 3]$ and 10 samples $x \sim \mathrm{QRice}(\nu, 1, \delta)$ per $\nu$; for tensor quantities we report the maximum entrywise error.}
  \label{fig:qrice-accuracy}
\end{figure}

% Table: Rician performance
\begin{table}[h]
  \centering
  \caption{Runtime for $f$, $\nabla f$, $\nabla^2 f$, and $\nabla^3 f$, comparing the proposed implementation against AD-based baselines using Bessels.jl and SpecialFunctions.jl. CPU timings are measured at random $(x,\nu)$ with $(\log_{10} x, \log_{10} \nu) \in [-3, 3]^2$; GPU timings measure parallel evaluation over $1024 \times 1024$ grids of $(x,\nu)$ pairs.}
  \label{tab:rician-performance}
  \resizebox{\textwidth}{!}{\input{figures/output/rician_performance_table.tex}}
\end{table}

% Table: Quantized Rician performance
\begin{table}[h]
  \centering
  \caption{Runtime vs.\ Gauss--Legendre order $N$ for $\Omega$, $\nabla\Omega$, $\nabla^2\Omega$, and $\nabla^3\Omega$ using the proposed method. CPU timings are measured with $\delta = 1$ at random ``high-SNR'' samples with $\log_{10} \nu \in [-3, 3]$, $x \sim \mathrm{Rice}(\nu,1)$; GPU timings measure parallel evaluation over $1024 \times 1024 \times 1$ grids of $(x, \nu, \delta)$ triples.}
  \label{tab:qrician-performance}
  \input{figures/output/qrician_performance_table.tex}
\end{table}

\section{Results}

Numerical accuracy is evaluated against 500-bit extended-precision reference expressions computed with ArbNumerics.jl \citep{johansson_arb_2014}.
CPU measurements use an AMD Ryzen 9 3950X with 16 cores and 32 threads;
GPU measurements use an NVIDIA GeForce RTX 3080 with 10 GB of memory.
We report errors using the absolute error $e_{\mathrm{abs}} = |\hat{y} - y|$, the relative error $e_{\mathrm{rel}} = |\hat{y} - y| / |y|$, and the minimum error $e_{\min} = \min(e_{\mathrm{abs}}, e_{\mathrm{rel}})$.
Unless stated otherwise, figures and tables report maxima of $e_{\min}$ over the stated domains.

\paragraph{Bessel ratio and its derivatives}

\Cref{fig:bessel-accuracy} shows error envelopes computed as maxima over local sliding windows for $r(z) = I_1(z) / I_0(z)$, $r'(z)$, $r''(z)$, $2 r'(z) + z r''(z)$, and $1 - r(z) - z r'(z)$ over 1001 points $z \in [10^{-6}, 10^{6}]$.
The naive method evaluates the recurrences \cref{eq:r-prime-recurrence,eq:r-second-derivative-recurrence} directly.
The proposed method follows the small-$z$ formulations in \cref{sec:bessel-ratio-small-z}, computing $r$, $r'$, and $r''$ from the reparametrized quantity $a_1$ via \cref{eq:r-small-reparametrized,eq:r-prime-small-reparametrized,eq:r-prime-prime-small-reparametrized}, and the large-$z$ formulations in \cref{sec:bessel-ratio-large-z}, computing $r$, $r'$, $r''$, $2r'+zr''$, and $1-r-zr'$ from the reparametrized quantity $b_3$ via \cref{eq:r-large-reparametrized,eq:r-prime-large-reparametrized-simplified,eq:r-prime-prime-large-reparametrized,eq:two-rprime-z-r-primeprime-large-z,eq:one-minus-r-minus-z-r-prime-large-z}, and uses rational polynomial approximants in the mid-$z$ regime.
The proposed piecewise evaluation remains at or near machine precision in both single and double precision, whereas the naive recurrence degrades severely, especially for $r''$, $2r'+zr''$, and $1-r-zr'$.
\Cref{fig:bessel-table} confirms these trends using the $e_{\min}$ metric.

\paragraph{Rician negative log-likelihood and derivatives}

\Cref{fig:nll-table} reports the maximum of $e_{\min}$ over a Cartesian grid with $(\log_{10} x, \log_{10} \nu) \in [-3, 3]^2$ for $f$ and its first through third derivatives.
Baselines labeled ``Bessels.jl'' and ``SF.jl'' compute derivatives of the Rician log-likelihood using forward-mode automatic differentiation (AD) via ForwardDiff.jl, where AD is applied to expressions using $I_0$ implementations from Bessels.jl and SpecialFunctions.jl, respectively.
``Naive'' implements the unsimplified relations from the \Cref{sec:rician-log-likelihood-and-basic-simplifications}, namely \cref{eq:rician-neg-log-likelihood,eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu,eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu,eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu,eq:third-derivatives-unsimplified-nu-nu-nu}.
``Proposed'' implements the stable forms from the \Cref{sec:further-rician-derivative-simplifications}, namely \cref{eq:first-derivatives-small-z,eq:first-derivatives-large-z,eq:second-derivatives-small-z,eq:second-derivatives-large-z,eq:third-derivatives-small-z,eq:third-derivatives-large-z}.
The proposed method achieves near-machine-precision accuracy for all quantities in both single and double precision.
The naive evaluation is numerically unstable; the AD-based baselines degrade as the derivative order increases, particularly in single precision.

\paragraph{Quantized Rician accuracy vs.\ quadrature order}

\Cref{fig:qrice-accuracy} presents the maximum error versus Gauss--Legendre order $N$ for $\Omega$, $\nabla\Omega$, $\nabla^2\Omega$, and $\nabla^3\Omega$.
For each precision and for each pair $(\delta, N)$, we take the maximum over 1024 samples $\nu$ with $\log_{10} \nu \in [-3, 3]$ and 10 samples $x \sim \mathrm{QRice}(\nu, 1, \delta)$ per $\nu$.
The error metric is $e_{\min}$, with the maximum entrywise error reported for vector and tensor quantities.
Errors decrease rapidly with $N$ but increase with $\delta$; for $\delta \le 1$, $N=6$ is sufficient to reach the minimum error for single precision and $N=10$ for double precision.

\paragraph{Performance}

\Cref{tab:rician-performance} summarizes per-evaluation runtime for $f$, $\nabla f$, $\nabla^2 f$, and $\nabla^3 f$.
The proposed implementation is consistently faster than AD-based baselines on both the CPU and GPU, with speedups increasing with derivative order.
\Cref{tab:qrician-performance} reports runtime versus $N$ for $\Omega$, $\nabla\Omega$, $\nabla^2\Omega$, and $\nabla^3\Omega$.
Runtime grows with $N$ and derivative order.

\section{Application: differentiating through Laplace's approximation}

In Type-II maximum likelihood frameworks, hyperparameters are tuned by optimizing the model evidence \citep{mackay_bayesian_1992}.
Using the Laplace approximation (LA) of the evidence, this optimization requires backpropagating through the posterior's maximum a posteriori (MAP) estimate and its curvature;
while this naively introduces third-order derivative tensors, the process can be made practical via efficient adjoint methods \citep{margossian_hamiltonian_2020}.
LA applications include Bayesian inference in latent Gaussian models, Variational Laplace Autoencoders, and end-to-end learning of data-augmentation invariances in deep networks \citep{margossian_hamiltonian_2020, park_variational_2019, immer_invariance_2022}.
For large-scale deep learning, tractability requires either replacing the full Hessian with approximations like Kronecker-factored structures or the generalized Gauss--Newton matrix \citep{ritter_scalable_2018, immer_improving_2021}, or applying the LA only to a subset of parameters.
A special case of this latter approach, \textit{last-layer} LA, applies the LA to a network's final linear layer \citep{kristiadi_being_2020, daxberger_laplace_2021}.
This motivates our setting:
a small, two-dimensional LA over nuisance parameters is embedded within a larger variational posterior approximation.

\subsection{MRI problem formulation}

In a common MRI setting, magnitude data $x = (x_1, \ldots, x_N)^T$ arise by taking the magnitude of complex measurements corrupted by Gaussian noise with variance $\sigma^2$, yielding Rician-distributed signals.
These signals are then quantized;
for example, stored as 16-bit integers with a scale factor $\delta$ to convert the compressed format back to physical units.
Suppose the measured signal $x_i$ is modeled as $s \cdot \mu_i(\lambda)$ where $\mu(\lambda)$ is a normalized signal model, $\lambda$ are the physical parameters of interest, and $s$ is a scaling factor.
The simplest such model is mono-exponential $T_2$ relaxation wherein $\lambda$ is the transverse relaxation time $T_2$, $\mu_i(\lambda) = \exp(-t_i/T_2)$ is the attenuation at echo time $t_i$, and $s$ is the initial signal amplitude.
Assuming the measurements are independent, this family of models results in the factorized likelihood:
%
\begin{align}\label{eq:likelihood}
  p(x \mid \lambda, \eta, \delta) = \prod_{i=1}^N p_{\mathrm{QRice}}(x_i \mid \nu = s \mu_i(\lambda), \sigma, \delta).
\end{align}

We consider a Bayesian setting where the goal is to infer the physical parameters $\lambda$ while marginalizing out the nuisance parameters $\eta = (\log s, \log \sigma)^T$.
Following \cref{eq:likelihood}, we write the negative log-likelihood as
%
\begin{align}\label{eq:negative-log-likelihood}
  L(\lambda, \eta; x, \delta)              & \coloneqq -\log p(x \mid \lambda, \eta, \delta) = \sum_{i=1}^N \ell(\xi^{(i)}(\lambda), \eta)                                                                      \\
  \text{where} \quad \ell(\xi^{(i)}, \eta) & \coloneqq -\log p_{\mathrm{QRice}}(x_i \mid \nu = s \mu_i, \sigma, \delta) = \Omega\left(\frac{x_i}{\sigma}, \frac{s \mu_i}{\sigma}, \frac{\delta}{\sigma}\right), \\
  \xi^{(i)}(\lambda)                       & \coloneqq (x_i, \mu_i(\lambda), \delta),
\end{align}
%
and $\Omega$ is the negative log-likelihood function for the quantized Rician distribution.
The joint density is then
%
\begin{align}
  p(x, \lambda, \eta \mid \delta) & = p(x \mid \lambda, \eta, \delta) p(\lambda, \eta) = e^{-U(\lambda, \eta; x, \delta)} \\
  \quad \text{where} \quad
  U(\lambda, \eta; x, \delta)     & \coloneqq L(\lambda, \eta; x, \delta) - \log p(\lambda, \eta)
\end{align}
%
where we assume a factorized prior $p(\lambda, \eta) = p(\lambda) p(\eta)$.

\subsection{Hybrid variational inference with Laplace's approximation}

Suppose a hybrid variational inference scheme is employed wherein the posterior $p(\lambda, \eta \mid x, \delta)$ is modeled by a factorized distribution $q(\lambda, \eta) = q(\lambda) q(\eta \mid \lambda)$.
Let $q(\lambda)$ be a parametric approximation to the marginal posterior $p(\lambda \mid x, \delta)$ from a chosen variational family, and $q(\eta \mid \lambda)$ be a Gaussian surrogate for the conditional posterior $p(\eta \mid \lambda, x, \delta)$ computed using Laplace's method:
%
\begin{align}
  p(\eta \mid \lambda, x, \delta) \approx q(\eta \mid \lambda) & = \mathcal{N}(\eta \mid \hat{\eta}(\xi(\lambda)), \hat{H}(\xi(\lambda))^{-1}) \\
  \text{with} \quad
  \hat{\eta}(\xi)                                              & = \arg\min_{\eta} U(\xi, \eta) \label{eq:laplace-mode}                        \\
  \hat{H}(\xi)                                                 & = H(\xi, \hat{\eta}(\xi)) \label{eq:laplace-hessian}
\end{align}
%
where $\xi(\lambda) = (\mu(\lambda), x, \delta)$ groups all variables that the conditional posterior $p(\eta \mid \cdot)$ depends on, $\hat{\eta}(\xi)$ is the mode of $U(\xi, \cdot)$, and $H(\xi, \eta) \coloneqq \frac{\partial^2 U}{\partial \eta^2}$ is the Hessian of $U(\xi, \cdot)$.

The parameters $\psi$ of the variational family for $q(\lambda)$ are tuned by optimizing an objective $\mathcal{L}$ which depends on the Laplace approximation's mode $\hat{\eta}(\xi(\lambda))$ and Hessian $\hat{H}(\xi(\lambda))$.
For instance, if $\mathcal{L} = \mathbb{E}_{q(\lambda, \eta)} \left[\log \frac{p(x, \lambda, \eta \mid \delta)}{q(\lambda, \eta)}\right]$ is the evidence lower bound, the gradient $\nabla_\psi$ flows through $\hat{\eta}$ and $\hat{H}$ via two mechanisms:
1) the entropy term $-\log q(\lambda, \eta)$ has a direct contribution from $-\log q(\eta \mid \lambda) = \tfrac{1}{2}\log\det(2\pi\hat{H}^{-1}) + \tfrac{1}{2} \lVert \hat{H}^{1/2} (\eta - \hat{\eta}) \rVert^2$;
and 2) if $q(\lambda)$ is reparameterizable (i.e. $\lambda \sim q(\lambda) \Leftrightarrow \lambda = g(\psi, \epsilon)$ where $\epsilon \sim p(\epsilon)$ with $p(\epsilon)$ fixed), then unbiased gradients of expectations $\mathbb{E}_{q(\lambda, \eta)}[f(\lambda, \eta)]$ follow from the reparameterization trick:
$\nabla_\psi \mathbb{E}_{q(\lambda, \eta)}[f(\lambda, \eta)] = \mathbb{E}_{\epsilon \sim p(\epsilon), z \sim \mathcal{N}(0, I)}[\nabla_\psi f(g(\psi, \epsilon), \hat{\eta} + \hat{H}^{-1/2} z)]$.
Therefore, backpropagating through $\mathcal{L}$ requires the implementation of \textit{pullback} functions to pass gradients through \cref{eq:laplace-mode,eq:laplace-hessian}.

\subsection{Pullbacks for the Laplace approximation}

Since $\hat{H}(\xi) = H(\xi, \hat{\eta}(\xi))$, we need only provide pullbacks for the implicit map $\xi \mapsto \hat{\eta}$ and the explicit map $(\xi, \eta) \mapsto H$;
the pullback for $\hat{H}$ is constructed by the automatic differentiation engine by composing the pullbacks for these two maps.
The pullback for $\xi \mapsto \hat{\eta}$ is derived from the optimality condition $\nabla_\eta U(\xi, \hat{\eta}(\xi)) = 0$.
Applying the implicit function theorem at $\eta = \hat{\eta}(\xi)$ yields:
%
\begin{align}
  \frac{\partial^2 U}{\partial \eta \partial \xi} + \left(\frac{\partial^2 U}{\partial \eta^2}\right) \left(\frac{\partial \hat{\eta}}{\partial \xi}\right) = 0
  \quad \Rightarrow \quad
  \frac{\partial \hat{\eta}}{\partial \xi} = -\left(\frac{\partial^2 U}{\partial \eta^2}\right)^{-1} \frac{\partial^2 U}{\partial \eta \partial \xi}.
\end{align}
%
Since $\hat{H}(\xi) = \left. \frac{\partial^2 U}{\partial \eta^2} \right|_{\eta=\hat{\eta}(\xi)}$, the pullback $\mathcal{B}_{\hat{\eta}}$ mapping the cotangent $\Delta^{\hat{\eta}} = \frac{\partial \mathcal{L}}{\partial \hat{\eta}}$ to $\frac{\partial \mathcal{L}}{\partial \xi}$ is given by
%
\begin{align}\label{eq:pullback-eta-hat}
  \mathcal{B}_{\hat{\eta}}(\Delta^{\hat{\eta}})
  = \Delta^{\hat{\eta}} \bullet \frac{\partial \hat{\eta}}{\partial \xi}
  = -\left( \hat{H}(\xi)^{-1} \Delta^{\hat{\eta}} \right)^{T} \frac{\partial^2 U}{\partial \eta \partial \xi}.
\end{align}
%
The map for the Hessian is explicit, $H(\xi, \eta) = \frac{\partial^2 U}{\partial \eta^2}$.
Its pullback $\mathcal{B}_H$ mapping the cotangent $\Delta^H = \frac{\partial \mathcal{L}}{\partial H}$ to $\frac{\partial \mathcal{L}}{\partial \xi}$ and $\frac{\partial \mathcal{L}}{\partial \eta}$ thus requires third-order derivative tensors of $U$:
%
\begin{align}\label{eq:pullback-H}
  \mathcal{B}_H(\Delta^H) = \left( \Delta^H \bullet \frac{\partial^3 U}{\partial \eta^2 \partial \xi}, \, \Delta^H \bullet \frac{\partial^3 U}{\partial \eta^3} \right).
\end{align}

Since $U = L - \log p(\lambda, \eta)$ and $L = \sum_i \ell(\xi^{(i)}, \eta)$, mixed partial derivatives of $U$ and $L$ are equal, $\frac{\partial^{k+1}}{\partial \eta^k \partial \xi} U = \frac{\partial^{k+1}}{\partial \eta^k \partial \xi} L$, and $\frac{\partial^k}{\partial \eta^k} U = \frac{\partial^k}{\partial \eta^k} L - \frac{\partial^k}{\partial \eta^k} \log p(\lambda, \eta)$.
The derivatives of $L$ can themselves be written in terms of the derivatives of $\ell_i = \ell(\xi^{(i)}, \eta) = \Omega(\frac{x_i}{\sigma}, \frac{s \mu_i}{\sigma}, \frac{\delta}{\sigma})$:
%
\begin{align}
  \frac{\partial^{k+1} L}{\partial \eta^k \partial x_i} = \frac{\partial^{k+1} \ell_i}{\partial \eta^k \partial x_i},
  \quad
  \frac{\partial^{k+1} L}{\partial \eta^k \partial \mu_i} = \frac{\partial^{k+1} \ell_i}{\partial \eta^k \partial \mu_i},
  \quad
  \frac{\partial^{k+1} L}{\partial \eta^k \partial \delta} = \sum_{i=1}^N \frac{\partial^{k+1} \ell_i}{\partial \eta^k \partial \delta},
  \quad
  \frac{\partial^{k} L}{\partial \eta^{k}} = \sum_{i=1}^N \frac{\partial^{k} \ell_i}{\partial \eta^{k}},
\end{align}
%
and so the VJPs in \cref{eq:pullback-eta-hat,eq:pullback-H} simplify to
%
\begin{align}
  \Delta^{\hat{\eta}'} \bullet \frac{\partial^2 U}{\partial \eta \partial \xi} & = \left( \Delta^{\hat{\eta}'} \bullet \frac{\partial^2 \ell_1}{\partial \eta \partial x_1}, \ldots, \Delta^{\hat{\eta}'} \bullet \frac{\partial^2 \ell_1}{\partial \eta \partial \mu_1}, \ldots, \sum_{i=1}^N \Delta^{\hat{\eta}'} \bullet \frac{\partial^2 \ell_i}{\partial \eta \partial \delta} \right), \label{eq:pullback-eta-hat-simplified} \\
  \Delta^H \bullet \frac{\partial^3 U}{\partial \eta^2 \partial \xi}           & = \left( \Delta^H \bullet \frac{\partial^3 \ell_1}{\partial \eta^2 \partial x_1}, \ldots, \Delta^H \bullet \frac{\partial^3 \ell_1}{\partial \eta^2 \partial \mu_1}, \ldots, \sum_{i=1}^N \Delta^H \bullet \frac{\partial^3 \ell_i}{\partial \eta^2 \partial \delta} \right), \label{eq:pullback-H-Phi-eta-squared-simplified}                     \\
  \Delta^H \bullet \frac{\partial^3 U}{\partial \eta^3}                        & = -\Delta^H \bullet \frac{\partial^3 \log p(\lambda, \eta)}{\partial \eta^3} + \sum_{i=1}^N \Delta^H \bullet \frac{\partial^3 \ell_i}{\partial \eta^3} \label{eq:pullback-H-eta-cubed-simplified}
\end{align}
%
where $\Delta^{\hat{\eta}'} = \hat{H}^{-1} \Delta^{\hat{\eta}}$.
Note that for a factorized prior $p(\lambda, \eta) = p(\lambda) p(\eta)$ with $p(\eta)$ Gaussian, $\frac{\partial^3}{\partial \eta^3} \log p(\lambda, \eta) = 0$.
Finally, differentiating through $\hat{\eta}$ and $\hat{H}$ reduces to computing the following derivative tensors:
%
\begin{align}\label{eq:ell-derivative-tensors}
  \ell_{\eta \xi^{(i)}} \coloneqq \frac{\partial^2 \ell_i}{\partial \eta \partial \xi^{(i)}},
  \quad
  \ell_{\eta \eta \xi^{(i)}} \coloneqq \frac{\partial^3 \ell_i}{\partial \eta^2 \partial \xi^{(i)}},
  \quad
  \ell_{\eta \eta \eta} \coloneqq \frac{\partial^3 \ell_i}{\partial \eta^3}.
\end{align}

\subsection{Derivatives of the per-observation log-likelihood}

The derivatives of $\ell_i$ required for the pullbacks in \cref{eq:ell-derivative-tensors} are found by applying the multivariate chain rule to the composition $\ell_i = \ell(\xi^{(i)}, \eta) = \Omega(\theta(\xi^{(i)}, \eta))$, where
%
\begin{align}
  \theta(\xi^{(i)}, \eta) = (x', \nu', \delta')^T = \left(\frac{x_i}{\sigma}, \frac{s \mu_i}{\sigma}, \frac{\delta}{\sigma}\right)^T,
  \quad
  \xi^{(i)} = (x_i, \mu_i, \delta)^T,
  \quad
  \eta = (\log s, \log \sigma)^T.
\end{align}
%
In previous sections, we derived expressions for the numerically stable evaluation of the derivative tensors $\nabla_\theta \Omega$, $\nabla_\theta^2\Omega$, and $\nabla_\theta^3\Omega$.
We can express derivatives of $\ell_i$ in \cref{eq:ell-derivative-tensors} in terms of these derivatives of $\Omega$ via the multivariate chain rule.
Dropping the index $i$ for brevity, let $\Theta_{\xi} \coloneqq \frac{\partial \theta}{\partial \xi}$ and $\Theta_{\eta} \coloneqq \frac{\partial \theta}{\partial \eta}$ be the Jacobians of $\theta$ with respect to $\xi$ and $\eta$.
The gradients of $\ell$ are
%
\begin{align}
  \ell_{\xi} = \Theta_{\xi}^{T} \nabla_\theta \Omega,
  \qquad
  \ell_{\eta}   = \Theta_{\eta}^{T} \nabla_\theta \Omega.
\end{align}
%
The second derivative matrices are found by applying the chain rule again:
%
\begin{align}
  \ell_{\eta \xi}  & = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\xi + \sum_{k} (\nabla_\theta \Omega)_{k} \Theta_{\eta\xi}^{(\theta_k)},   \\
  \ell_{\eta \eta} & = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\eta + \sum_{k} (\nabla_\theta \Omega)_{k} \Theta_{\eta\eta}^{(\theta_k)}.
\end{align}
%
where $\Theta_{\eta\xi}$ and $\Theta_{\eta\eta}$ are 3-tensors containing the second derivatives of $\theta$, with components $\Theta_{\eta\xi}^{(\theta_k)} \coloneqq \frac{\partial^2 \theta_k}{\partial \eta \partial \xi}$ and $\Theta_{\eta\eta}^{(\theta_k)} \coloneqq \frac{\partial^2 \theta_k}{\partial \eta^2}$.
The third-derivative tensors required by \cref{eq:pullback-eta-hat-simplified,eq:pullback-H-Phi-eta-squared-simplified,eq:pullback-H-eta-cubed-simplified}
are obtained by differentiating the Hessian $\ell_{\eta\eta}$.
Using the Einstein summation convention, the $ij^\text{th}$ component of the Hessian is
%
\begin{align}
  (\ell_{\eta\eta})_{ij}
  = \frac{\partial^2 \ell}{\partial \eta_i \partial \eta_j}
  = \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j}
  + \frac{\partial \Omega}{\partial \theta_c} \frac{\partial^2 \theta_c}{\partial \eta_i \partial \eta_j}.
\end{align}
%
Differentiating $(\ell_{\eta\eta})_{ij}$ with respect to $\xi_k$ and $\eta_l$ yields
%
\begin{align}
  \tfrac{\partial (\ell_{\eta\eta})_{ij}}{\partial \xi_k}  & = \tfrac{\partial \Omega}{\partial \theta_a} \tfrac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k} + \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \left( \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \tfrac{\partial \theta_b}{\partial \eta_j} + \tfrac{\partial^2 \theta_a}{\partial \eta_j \partial \xi_k} \tfrac{\partial \theta_b}{\partial \eta_i} + \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \tfrac{\partial \theta_b}{\partial \xi_k} \right) + \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} \tfrac{\partial \theta_a}{\partial \eta_i} \tfrac{\partial \theta_b}{\partial \eta_j} \tfrac{\partial \theta_c}{\partial \xi_k}, \label{eq:third-derivative-ell-eta-eta-xi}       \\
  \tfrac{\partial (\ell_{\eta\eta})_{ij}}{\partial \eta_l} & = \tfrac{\partial \Omega}{\partial \theta_c} \tfrac{\partial^3 \theta_c}{\partial \eta_i \partial \eta_j \partial \eta_l} + \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \left( \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_l} \tfrac{\partial \theta_b}{\partial \eta_j} + \tfrac{\partial^2 \theta_a}{\partial \eta_j \partial \eta_l} \tfrac{\partial \theta_b}{\partial \eta_i} + \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \tfrac{\partial \theta_b}{\partial \eta_l} \right) + \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} \tfrac{\partial \theta_a}{\partial \eta_i} \tfrac{\partial \theta_b}{\partial \eta_j} \tfrac{\partial \theta_c}{\partial \eta_l}. \label{eq:third-derivative-ell-eta-eta-eta}
\end{align}
%
The elements of the required VJPs are then obtained by contracting with the symmetric matrix $\Delta^H$:
%
\begin{align}
  (\Delta^H \bullet \ell_{\eta\eta\xi})_k  & = \left( \Delta^H_{ij} \tfrac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k} \right) \tfrac{\partial \Omega}{\partial \theta_a} + \Delta^H_{ij} \left( 2 \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \tfrac{\partial \theta_b}{\partial \eta_j} + \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \tfrac{\partial \theta_b}{\partial \xi_k} \right) \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} + \left( \Delta^H_{ij} \tfrac{\partial \theta_a}{\partial \eta_i} \tfrac{\partial \theta_b}{\partial \eta_j} \right) \tfrac{\partial \theta_c}{\partial \xi_k} \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}, \label{eq:third-derivative-ell-eta-eta-xi-delta-h}      \\
  (\Delta^H \bullet \ell_{\eta\eta\eta})_l & = \left( \Delta^H_{ij} \tfrac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \eta_l} \right) \tfrac{\partial \Omega}{\partial \theta_a} + \Delta^H_{ij} \left( 2 \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_l} \tfrac{\partial \theta_b}{\partial \eta_j} + \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \tfrac{\partial \theta_b}{\partial \eta_l} \right) \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} + \left( \Delta^H_{ij} \tfrac{\partial \theta_a}{\partial \eta_i} \tfrac{\partial \theta_b}{\partial \eta_j} \right) \tfrac{\partial \theta_c}{\partial \eta_l} \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}. \label{eq:third-derivative-ell-eta-eta-eta-delta-h}
\end{align}
%
Explicit formulas for the tensors $\Theta_{\xi}$, $\Theta_{\eta}$, $\Theta_{\eta\xi}$, $\Theta_{\eta\eta}$, $\Theta_{\eta\eta\xi}$, and $\Theta_{\eta\eta\eta}$ are collected in \Cref{app:explicit-change-of-vars-derivatives}.

% Bibliography
\bibliographystyle{plainnat}
\bibliography{bibliography}

% Appendix
\appendix

\section{Proofs of expectation and covariance identities}\label{app:proofs-expectation-covariance}

We provide complete proofs of the expectation and covariance identities deferred from the main text.
These results follow from the linearity of differentiation and integration, and apply equally to both continuous integrals and fixed-node quadrature.

\subsection{Jacobian-of-expectation identity (\Cref{eq:jacobian-expectation})}\label{app:proof-jacobian-expectation}

Recall that $g : [0, 1] \times \mathbb{R}^3 \mapsto \mathbb{R}^m$ and $\mathbb{E}[g] \in \mathbb{R}^m$ denotes the expectation of $g(t, \theta)$ with respect to the density $P(t \mid \theta) = e^{-\tilde{r}(t, \theta)} / Z(\theta)$ where $Z(\theta) = \int_0^1 \exp(-\tilde{r}(t, \theta)) \, dt$ is the normalizing constant.
Starting from the definition of the expectation and applying the product rule yields
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \nabla_\theta \left( Z(\theta)^{-1} \int_0^1 g(t,\theta) \exp(-\tilde{r}(t,\theta)) \, dt \right)                                              \\
                              & = \left(\int_0^1 g e^{-\tilde{r}} \, dt\right) (\nabla_\theta Z^{-1})^{T} + Z^{-1} \nabla_\theta \left( \int_0^1 g e^{-\tilde{r}} \, dt \right).
\end{align}
%
Using the identity $\nabla_\theta Z^{-1} = -Z^{-1} \nabla_\theta \log Z$, the first term simplifies to
%
\begin{align}
  \left(Z \mathbb{E}[g]\right) \left(-Z^{-1} \nabla_\theta \log Z\right)^{T} = - \mathbb{E}[g] \left(\nabla_\theta \log Z\right)^{T}
\end{align}
%
where the gradient of the log-partition function is given by
%
\begin{align}
  \nabla_\theta \log Z = \frac{\nabla_\theta Z}{Z} = Z^{-1} \int_0^1 \nabla_\theta e^{-\tilde{r}} \, dt = -Z^{-1} \int_0^1 \nabla_\theta \tilde{r} e^{-\tilde{r}} \, dt = -\mathbb{E}[\nabla_\theta \tilde{r}].
\end{align}
%
For the second term, we move the derivative inside the integral and apply the product rule:
%
\begin{align}
  Z^{-1} \int_0^1 \nabla_\theta(g e^{-\tilde{r}}) \, dt & = Z^{-1} \int_0^1 \left( (\nabla_\theta g) e^{-\tilde{r}} - g (\nabla_\theta \tilde{r})^{T} e^{-\tilde{r}} \right) \, dt \\
                                                        & = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^{T}].
\end{align}
%
Combining these results and recognizing the definition of the covariance yields
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = - \mathbb{E}[g] (\nabla_\theta \log Z)^{T} + \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^{T}]                         \\
                              & = \mathbb{E}[\nabla_\theta g] - \left(\mathbb{E}[g (\nabla_\theta \tilde{r})^{T}] - \mathbb{E}[g] \mathbb{E}[\nabla_\theta \tilde{r}]^{T}\right) \\
                              & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{r}).
\end{align}
%
Since this derivation relies only on the linearity of the integral operator and its commutativity with differentiation, the identity holds for both the continuous integral $\int_0^1 (\cdot) \, dt$ and for fixed-node Gauss--Legendre quadrature $\sum_{i=1}^N \frac{w_i}{2} (\cdot)$, which shares these properties.

\subsection{Covariance-derivative identity (\Cref{eq:derivative-covariance-identity})}\label{app:proof-covariance-derivative}

From the definition of covariance and the product rule, we have
%
\begin{align}
  \partial_{\gamma} \mathrm{Cov}(g, h) & = \partial_{\gamma} \left( \mathbb{E}[g h] - \mathbb{E}[g]\mathbb{E}[h] \right) = \partial_{\gamma} \mathbb{E}[g h] - (\partial_{\gamma} \mathbb{E}[g]) \mathbb{E}[h] - \mathbb{E}[g] (\partial_{\gamma} \mathbb{E}[h]).
\end{align}
%
Applying the Jacobian-of-expectation identity from \cref{eq:jacobian-expectation-residual} to each derivative term gives
%
\begin{align}
  \partial_{\gamma} \mathbb{E}[g h] & = \mathbb{E}[\partial_{\gamma}(g h)] - \mathrm{Cov}(g h, \partial_{\gamma} \tilde{r}) = \mathbb{E}[(\partial_{\gamma} g) h + g (\partial_{\gamma} h)] - \mathrm{Cov}(g h, \partial_{\gamma} \tilde{r}) \\
  \partial_{\gamma} \mathbb{E}[g]   & = \mathbb{E}[\partial_{\gamma} g] - \mathrm{Cov}(g, \partial_{\gamma} \tilde{r})                                                                                                                       \\
  \partial_{\gamma} \mathbb{E}[h]   & = \mathbb{E}[\partial_{\gamma} h] - \mathrm{Cov}(h, \partial_{\gamma} \tilde{r}).
\end{align}
%
Substituting in these expressions and collecting terms yields
%
\begin{align}
  \partial_{\gamma} \mathrm{Cov}(g, h) & = \mathbb{E}[(\partial_{\gamma} g) h] - \mathbb{E}[\partial_{\gamma} g] \mathbb{E}[h] + \mathbb{E}[g (\partial_{\gamma} h)] - \mathbb{E}[g] \mathbb{E}[\partial_{\gamma} h]           \\
                                       & - \mathrm{Cov}(g h, \partial_{\gamma} \tilde{r}) + \mathrm{Cov}(g, \partial_{\gamma} \tilde{r}) \mathbb{E}[h] + \mathbb{E}[g] \mathrm{Cov}(h, \partial_{\gamma} \tilde{r}). \nonumber
\end{align}
%
The first four terms are recognized as $\mathrm{Cov}(\partial_{\gamma} g, h) + \mathrm{Cov}(g, \partial_{\gamma} h)$.
The remaining three terms can be shown to equal the negative third central moment:
%
\begin{align}
  \mathrm{Cov3}(g, h, \partial_{\gamma} \tilde{r}) & = \mathbb{E}[(g-\mathbb{E}[g])(h-\mathbb{E}[h])(\partial_{\gamma} \tilde{r}-\mathbb{E}[\partial_{\gamma} \tilde{r}])]                                                                                                                                                                           \\
                                                   & = \mathbb{E}[(gh - g\mathbb{E}[h] - h\mathbb{E}[g] + \mathbb{E}[g]\mathbb{E}[h])(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])]                                                                                                                                        \\
                                                   & = \mathbb{E}[gh(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] - \mathbb{E}[h]\mathbb{E}[g(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] - \mathbb{E}[g]\mathbb{E}[h(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] \\
                                                   & = \mathrm{Cov}(gh, \partial_{\gamma} \tilde{r}) - \mathbb{E}[h]\mathrm{Cov}(g, \partial_{\gamma} \tilde{r}) - \mathbb{E}[g]\mathrm{Cov}(h, \partial_{\gamma} \tilde{r}).
\end{align}
%
We then recover the desired expression in \cref{eq:derivative-covariance-identity}:
%
\begin{align}
  \partial_{\gamma} \mathrm{Cov}(g, h) = \mathrm{Cov}(\partial_{\gamma} g, h) + \mathrm{Cov}(g, \partial_{\gamma} h) - \mathrm{Cov3}(g, h, \partial_{\gamma} \tilde{r}).
\end{align}
%
As with the Jacobian-of-expectation identity, this derivation only uses linearity of expectation and differentiation, and thus holds when integration is replaced by fixed-node Gauss--Legendre quadrature.

\section{Coordinate-transformation derivatives for $\theta(\xi,\eta)$}\label{app:theta-derivatives}

We derive the first- (Jacobian matrices), second- (3-tensors), and third-order (4-tensors) derivatives of the coordinate map $\theta(\xi,\eta)$ used in $\ell(\xi,\eta) = \Omega(\theta(\xi,\eta))$.
These tensors enter the pullback formulas for Laplace's approximation via the chain rule; see \Cref{eq:pullback-eta-hat,eq:pullback-H}.
After establishing the variable definitions and coordinate transformation, we compute Jacobians and higher-order derivative tensors for $\theta$.
A compact summary of the explicit tensors appears in \Cref{app:explicit-change-of-vars-derivatives}, and derivatives of $\ell$ with their associated VJPs are developed in \Cref{app:ell-derivatives}.

\subsection{Variable definitions and coordinate transformation}

The per-observation log-likelihood is a composition $\ell(\xi, \eta) = \Omega(\theta(\xi, \eta))$, where
%
\begin{align}
  \theta & = (\theta_1, \theta_2, \theta_3)^T = (x', \nu', \delta')^T = \left(\frac{x}{\sigma}, \frac{s \mu}{\sigma}, \frac{\delta}{\sigma}\right)^T, \\
  \xi    & = (\xi_1, \xi_2, \xi_3)^T = (x, \mu, \delta)^T,                                                                                            \\
  \eta   & = (\eta_1, \eta_2)^T = (\log s, \log \sigma)^T.
\end{align}
%
Here, $\theta, \xi \in \mathbb{R}^3$ and $\eta \in \mathbb{R}^2$.
%
The log-transformations imply $s = e^{\eta_1}$ and $\sigma = e^{\eta_2}$, from which the components of $\theta$ are
%
\begin{align}
  \theta_1 = x'(\xi, \eta)      & = x e^{-\eta_2},           \\
  \theta_2 = \nu'(\xi, \eta)    & = \mu e^{\eta_1 - \eta_2}, \\
  \theta_3 = \delta'(\xi, \eta) & = \delta e^{-\eta_2}.
\end{align}
%
To compute derivatives of $\ell$ with respect to $\xi$ and $\eta$ via the multivariate chain rule, we require the first, second, and third derivatives of this coordinate transformation.

\subsection{First derivatives of the transformation}\label{app:first-derivatives-of-transformation}

We compute the Jacobians of $\theta$ with respect to $\xi$ and $\eta$.

\paragraph{Jacobian with respect to $\xi$.}

The Jacobian $\Theta_\xi \coloneqq \frac{\partial \theta}{\partial \xi}$ is a $3 \times 3$ matrix with entries $(\Theta_\xi)_{ij} = \frac{\partial \theta_i}{\partial \xi_j}$:
%
\begin{align}
  \frac{\partial \theta_1}{\partial \xi_1} & = \frac{\partial x'}{\partial x} = \frac{1}{\sigma}, &  & \frac{\partial \theta_1}{\partial \xi_2} = \frac{\partial x'}{\partial \mu} = 0,                  &  & \frac{\partial \theta_1}{\partial \xi_3} = \frac{\partial x'}{\partial \delta} = 0,                     \\
  \frac{\partial \theta_2}{\partial \xi_1} & = \frac{\partial \nu'}{\partial x} = 0,              &  & \frac{\partial \theta_2}{\partial \xi_2} = \frac{\partial \nu'}{\partial \mu} = \frac{s}{\sigma}, &  & \frac{\partial \theta_2}{\partial \xi_3} = \frac{\partial \nu'}{\partial \delta} = 0,                   \\
  \frac{\partial \theta_3}{\partial \xi_1} & = \frac{\partial \delta'}{\partial x} = 0,           &  & \frac{\partial \theta_3}{\partial \xi_2} = \frac{\partial \delta'}{\partial \mu} = 0,             &  & \frac{\partial \theta_3}{\partial \xi_3} = \frac{\partial \delta'}{\partial \delta} = \frac{1}{\sigma}.
\end{align}
%
Assembling these components yields
%
\begin{align}\label{eq:Theta-xi}
  \Theta_{\xi} =
  \begin{bmatrix}
    \frac{1}{\sigma} & 0                & 0                \\
    0                & \frac{s}{\sigma} & 0                \\
    0                & 0                & \frac{1}{\sigma}
  \end{bmatrix}.
\end{align}

\paragraph{Jacobian with respect to $\eta$.}

The Jacobian $\Theta_\eta \coloneqq \frac{\partial \theta}{\partial \eta}$ is a $3 \times 2$ matrix with entries $(\Theta_\eta)_{ij} = \frac{\partial \theta_i}{\partial \eta_j}$:
%
\begin{align}
  \frac{\partial \theta_1}{\partial \eta_1} & = \frac{\partial (x e^{-\eta_2})}{\partial \eta_1} = 0,                                        &  & \frac{\partial \theta_1}{\partial \eta_2} = \frac{\partial (x e^{-\eta_2})}{\partial \eta_2} = -x e^{-\eta_2} = -x',                       \\
  \frac{\partial \theta_2}{\partial \eta_1} & = \frac{\partial (\mu e^{\eta_1 - \eta_2})}{\partial \eta_1} = \mu e^{\eta_1 - \eta_2} = \nu', &  & \frac{\partial \theta_2}{\partial \eta_2} = \frac{\partial (\mu e^{\eta_1 - \eta_2})}{\partial \eta_2} = -\mu e^{\eta_1 - \eta_2} = -\nu', \\
  \frac{\partial \theta_3}{\partial \eta_1} & = \frac{\partial (\delta e^{-\eta_2})}{\partial \eta_1} = 0,                                   &  & \frac{\partial \theta_3}{\partial \eta_2} = \frac{\partial (\delta e^{-\eta_2})}{\partial \eta_2} = -\delta e^{-\eta_2} = -\delta'.
\end{align}
%
This gives
%
\begin{align}\label{eq:Theta-eta}
  \Theta_{\eta} =
  \begin{bmatrix}
    0    & -x'      \\
    \nu' & -\nu'    \\
    0    & -\delta'
  \end{bmatrix}.
\end{align}

\subsection{Second derivatives of the transformation}

The second derivatives form 3-tensors: $\Theta_{\eta\xi} \coloneqq \frac{\partial^2 \theta}{\partial \eta \partial \xi}$ and $\Theta_{\eta\eta} \coloneqq \frac{\partial^2 \theta}{\partial \eta^2}$.
We present these as collections of matrices, one for each component of $\theta$.

\paragraph{Mixed second derivative $\Theta_{\eta\xi}$.}

The components $(\Theta_{\eta\xi})_{kij} = \frac{\partial^2 \theta_k}{\partial \eta_i \partial \xi_j}$ are obtained by differentiating $\Theta_\xi$ with respect to each component of $\eta$:
%
\begin{align}
  \frac{\partial^2 x'}{\partial \eta_1 \partial \xi}      & = \left( \frac{\partial}{\partial \eta_1}\frac{\partial x'}{\partial x}, \frac{\partial}{\partial \eta_1}\frac{\partial x'}{\partial \mu}, \frac{\partial}{\partial \eta_1}\frac{\partial x'}{\partial \delta} \right) = \left( \frac{\partial (1/\sigma)}{\partial \eta_1}, 0, 0 \right) = (0, 0, 0),                                   \\
  \frac{\partial^2 x'}{\partial \eta_2 \partial \xi}      & = \left( \frac{\partial}{\partial \eta_2}\frac{\partial x'}{\partial x}, \frac{\partial}{\partial \eta_2}\frac{\partial x'}{\partial \mu}, \frac{\partial}{\partial \eta_2}\frac{\partial x'}{\partial \delta} \right) = \left( \frac{\partial (e^{-\eta_2})}{\partial \eta_2}, 0, 0 \right) = (-e^{-\eta_2}, 0, 0) = (-1/\sigma, 0, 0), \\
  \frac{\partial^2 \nu'}{\partial \eta_1 \partial \xi}    & = \left( 0, \frac{\partial (s/\sigma)}{\partial \eta_1}, 0 \right) = \left( 0, \frac{\partial (e^{\eta_1}/\sigma)}{\partial \eta_1}, 0 \right) = (0, s/\sigma, 0),                                                                                                                                                                       \\
  \frac{\partial^2 \nu'}{\partial \eta_2 \partial \xi}    & = \left( 0, \frac{\partial (s/\sigma)}{\partial \eta_2}, 0 \right) = \left( 0, \frac{\partial (s e^{-\eta_2})}{\partial \eta_2}, 0 \right) = (0, -s/\sigma, 0),                                                                                                                                                                          \\
  \frac{\partial^2 \delta'}{\partial \eta_1 \partial \xi} & = \left( 0, 0, \frac{\partial (1/\sigma)}{\partial \eta_1} \right) = (0, 0, 0),                                                                                                                                                                                                                                                          \\
  \frac{\partial^2 \delta'}{\partial \eta_2 \partial \xi} & = \left( 0, 0, \frac{\partial (1/\sigma)}{\partial \eta_2} \right) = \left( 0, 0, \frac{\partial (e^{-\eta_2})}{\partial \eta_2} \right) = (0, 0, -1/\sigma).
\end{align}
%
The component matrices $\Theta_{\eta\xi}^{(x')} = \frac{\partial^2 x'}{\partial \eta \partial \xi}$, $\Theta_{\eta\xi}^{(\nu')} = \frac{\partial^2 \nu'}{\partial \eta \partial \xi}$, and $\Theta_{\eta\xi}^{(\delta')} = \frac{\partial^2 \delta'}{\partial \eta \partial \xi}$ are
%
\begin{align}
  \Theta_{\eta\xi}^{(x')}      & = \begin{bsmallmatrix} 0 & 0 & 0 \\ -\frac{1}{\sigma} & 0 & 0 \end{bsmallmatrix}, &
  \Theta_{\eta\xi}^{(\nu')}    & = \begin{bsmallmatrix} 0 & \frac{s}{\sigma} & 0 \\ 0 & -\frac{s}{\sigma} & 0 \end{bsmallmatrix}, &
  \Theta_{\eta\xi}^{(\delta')} & = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & -\frac{1}{\sigma} \end{bsmallmatrix}.
\end{align}

\paragraph{Second derivative $\Theta_{\eta\eta}$.}

The components $\frac{\partial}{\partial \eta_j} \left( \frac{\partial \theta_k}{\partial \eta_i} \right)$ are obtained by differentiating $\Theta_\eta$ with respect to each component of $\eta$:
%
\begin{align}
  \frac{\partial^2 x'}{\partial \eta_1^2}                 & = \frac{\partial}{\partial \eta_1}(0)   = 0, \qquad \frac{\partial^2 x'}{\partial \eta_2 \partial \eta_1} = \frac{\partial}{\partial \eta_2}(0) = 0,    \\
  \frac{\partial^2 x'}{\partial \eta_2^2}                 & = \frac{\partial}{\partial \eta_2}(-x') = \frac{\partial}{\partial \eta_2}(-x e^{-\eta_2}) = x e^{-\eta_2} = x',                                        \\
  \frac{\partial^2 \nu'}{\partial \eta_1^2}               & = \frac{\partial}{\partial \eta_1}(\nu') = \frac{\partial}{\partial \eta_1}(\mu e^{\eta_1 - \eta_2}) = \nu',                                            \\
  \frac{\partial^2 \nu'}{\partial \eta_2 \partial \eta_1} & = \frac{\partial}{\partial \eta_2}(\nu') = \frac{\partial}{\partial \eta_2}(\mu e^{\eta_1 - \eta_2}) = -\nu',                                           \\
  \frac{\partial^2 \nu'}{\partial \eta_2^2}               & = \frac{\partial}{\partial \eta_2}(-\nu') = \frac{\partial}{\partial \eta_2}(-\mu e^{\eta_1 - \eta_2}) = \nu',                                          \\
  \frac{\partial^2 \delta'}{\partial \eta_1^2}            & = \frac{\partial}{\partial \eta_1}(0) = 0, \qquad \frac{\partial^2 \delta'}{\partial \eta_2 \partial \eta_1} = \frac{\partial}{\partial \eta_2}(0) = 0, \\
  \frac{\partial^2 \delta'}{\partial \eta_2^2}            & = \frac{\partial}{\partial \eta_2}(-\delta') = \frac{\partial}{\partial \eta_2}(-\delta e^{-\eta_2}) = \delta e^{-\eta_2} = \delta'.
\end{align}
%
The component matrices $\Theta_{\eta\eta}^{(x')} = \frac{\partial^2 x'}{\partial \eta^2}$, $\Theta_{\eta\eta}^{(\nu')} = \frac{\partial^2 \nu'}{\partial \eta^2}$, and $\Theta_{\eta\eta}^{(\delta')} = \frac{\partial^2 \delta'}{\partial \eta^2}$ are
%
\begin{align}
  \Theta_{\eta\eta}^{(x')}      & = \begin{bsmallmatrix} 0 & 0 \\ 0 & x' \end{bsmallmatrix}, &
  \Theta_{\eta\eta}^{(\nu')}    & = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix}, &
  \Theta_{\eta\eta}^{(\delta')} & = \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta' \end{bsmallmatrix}.
\end{align}

\subsection{Third derivatives of the transformation}

The third derivatives are 4-tensors: $\Theta_{\eta\eta\xi} \coloneqq \frac{\partial^3 \theta}{\partial \eta^2 \partial \xi}$ and $\Theta_{\eta\eta\eta} \coloneqq \frac{\partial^3 \theta}{\partial \eta^3}$.
Their components are, for $k \in \{1,2,3\}$, $i,j \in \{1,2\}$, and $\ell \in \{1,2,3\}$:
%
\begin{align}
  (\Theta_{\eta\eta\xi})_{k\ell ij} = \frac{\partial^3 \theta_k}{\partial \eta_i \partial \eta_j \partial \xi_\ell}, \quad (\Theta_{\eta\eta\eta})_{kij\ell} = \frac{\partial^3 \theta_k}{\partial \eta_i \partial \eta_j \partial \eta_\ell}.
\end{align}
%
These are obtained by differentiating the second-derivative component matrices.

\paragraph{Mixed third derivative $\Theta_{\eta\eta\xi}$.}

We differentiate $\Theta_{\eta\xi}^{(x')}, \Theta_{\eta\xi}^{(\nu')}, \Theta_{\eta\xi}^{(\delta')}$ with respect to $\eta_1 = \log s$ and $\eta_2 = \log \sigma$:
%
\begin{align}
  \frac{\partial \Theta_{\eta\xi}^{(x')}}{\partial \eta_1}      & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & 0 & 0 \\ -1/\sigma & 0 & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\xi}^{(x')}}{\partial \eta_2}      & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & 0 & 0 \\ -e^{-\eta_2} & 0 & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ e^{-\eta_2} & 0 & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 1/\sigma & 0 & 0 \end{bsmallmatrix}, \\
  \frac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial \eta_1}    & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & s/\sigma & 0 \\ 0 & -s/\sigma & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & s/\sigma & 0 \\ 0 & -s/\sigma & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial \eta_2}    & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & s e^{-\eta_2} & 0 \\ 0 & -s e^{-\eta_2} & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & -s e^{-\eta_2} & 0 \\ 0 & s e^{-\eta_2} & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & -s/\sigma & 0 \\ 0 & s/\sigma & 0 \end{bsmallmatrix}, \\
  \frac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial \eta_1} & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & -1/\sigma \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial \eta_2} & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & -e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 1/\sigma \end{bsmallmatrix}.
\end{align}

\paragraph{Third derivative $\Theta_{\eta\eta\eta}$.}

We differentiate $\Theta_{\eta\eta}^{(x')}, \Theta_{\eta\eta}^{(\nu')}, \Theta_{\eta\eta}^{(\delta')}$ with respect to $\eta_1 = \log s$ and $\eta_2 = \log \sigma$:
%
\begin{align}
  \frac{\partial \Theta_{\eta\eta}^{(x')}}{\partial \eta_1}      & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & 0 \\ 0 & x' \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\eta}^{(x')}}{\partial \eta_2}      & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & 0 \\ 0 & x e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -x e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -x' \end{bsmallmatrix}, \\
  \frac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial \eta_1}    & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix} = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial \eta_2}    & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix} = \begin{bsmallmatrix} -\nu' & \nu' \\ \nu' & -\nu' \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial \eta_1} & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta' \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial \eta_2} & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -\delta e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -\delta' \end{bsmallmatrix}.
\end{align}
%
These match the tensors given in \cref{eq:third-derivative-theta-log-s-eta-xi,eq:third-derivative-theta-log-sigma-eta-xi,eq:third-derivative-theta-log-s-eta-eta,eq:third-derivative-theta-log-sigma-eta-eta}.

\subsection{Explicit change-of-variables tensors}\label{app:explicit-change-of-vars-derivatives}

The Jacobians of $\theta$ with respect to $\xi$ and $\eta$ are
%
\begin{align}
  \Theta_{\xi} = \frac{\partial \theta}{\partial \xi} =
  \begin{bmatrix}
    \frac{1}{\sigma} & 0                & 0                \\
    0                & \frac{s}{\sigma} & 0                \\
    0                & 0                & \frac{1}{\sigma}
  \end{bmatrix},
  \qquad
  \Theta_{\eta} = \frac{\partial \theta}{\partial \eta} =
  \begin{bmatrix}
    0    & -x'      \\
    \nu' & -\nu'    \\
    0    & -\delta'
  \end{bmatrix}.
\end{align}
%
The components of the second-derivative 3-tensors
$\Theta_{\eta\xi} = \frac{\partial^2 \theta}{\partial \eta \partial \xi}$ and
$\Theta_{\eta\eta} = \frac{\partial^2 \theta}{\partial \eta^2}$
are
%
\begin{align}
  \Theta_{\eta\xi}^{(x')}                                                                                           & = \frac{\partial^2 x'}{\partial \eta \partial \xi} = \begin{bsmallmatrix} 0 & 0 & 0 \\ -\frac{1}{\sigma} & 0 & 0 \end{bsmallmatrix}, &   &
  \Theta_{\eta\xi}^{(\nu')} = \frac{\partial^2 \nu'}{\partial \eta \partial \xi} = \begin{bsmallmatrix} 0 & \frac{s}{\sigma} & 0 \\ 0 & -\frac{s}{\sigma} & 0 \end{bsmallmatrix}, &                                                                                       &
  \Theta_{\eta\xi}^{(\delta')} = \frac{\partial^2 \delta'}{\partial \eta \partial \xi} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & -\frac{1}{\sigma} \end{bsmallmatrix},                                                                                         \\
  \Theta_{\eta\eta}^{(x')}                                                                                          & = \frac{\partial^2 x'}{\partial \eta^2} = \begin{bsmallmatrix} 0 & 0 \\ 0 & x' \end{bsmallmatrix},            &   &
  \Theta_{\eta\eta}^{(\nu')} = \frac{\partial^2 \nu'}{\partial \eta^2} = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix},           &                                                                                       &
  \Theta_{\eta\eta}^{(\delta')} = \frac{\partial^2 \delta'}{\partial \eta^2} = \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta' \end{bsmallmatrix}.
\end{align}
%
The components of the third-derivative 4-tensors
$\Theta_{\eta\eta\xi} \coloneqq \frac{\partial^3 \theta}{\partial \eta^2 \partial \xi}$ and $\Theta_{\eta\eta\eta} \coloneqq \frac{\partial^3 \theta}{\partial \eta^3}$ are
%
\begin{align}
  \frac{\partial \Theta_{\eta\xi}^{(x')}}{\partial \eta_1}  & = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix}, &  & \frac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial \eta_1} = \begin{bsmallmatrix} 0 & \frac{s}{\sigma} & 0 \\ 0 & -\frac{s}{\sigma} & 0 \end{bsmallmatrix},  &  & \frac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial \eta_1} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix}, \label{eq:third-derivative-theta-log-s-eta-xi}       \\
  \frac{\partial \Theta_{\eta\xi}^{(x')}}{\partial \eta_2}  & = \begin{bsmallmatrix} 0 & 0 & 0 \\ \frac{1}{\sigma} & 0 & 0 \end{bsmallmatrix}, &  & \frac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial \eta_2} = \begin{bsmallmatrix} 0 & -\frac{s}{\sigma} & 0 \\ 0 & \frac{s}{\sigma} & 0 \end{bsmallmatrix},  &  & \frac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial \eta_2} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & \frac{1}{\sigma} \end{bsmallmatrix}, \label{eq:third-derivative-theta-log-sigma-eta-xi}   \\
  \frac{\partial \Theta_{\eta\eta}^{(x')}}{\partial \eta_1} & = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix}, &  & \frac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial \eta_1} = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix}, &  & \frac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial \eta_1} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix}, \label{eq:third-derivative-theta-log-s-eta-eta}     \\
  \frac{\partial \Theta_{\eta\eta}^{(x')}}{\partial \eta_2} & = \begin{bsmallmatrix} 0 & 0 \\ 0 & -x' \end{bsmallmatrix}, &  & \frac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial \eta_2} = \begin{bsmallmatrix} -\nu' & \nu' \\ \nu' & -\nu' \end{bsmallmatrix}, &  & \frac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial \eta_2} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -\delta' \end{bsmallmatrix} \label{eq:third-derivative-theta-log-sigma-eta-eta}.
\end{align}

\section{Derivatives of the per-observation log-likelihood $\ell$ and VJPs}\label{app:ell-derivatives}

Using the derivative tensors for $\theta(\xi,\eta)$ from \Cref{app:theta-derivatives}, we apply the multivariate chain rule to derive the first-, second-, and third-order derivatives of $\ell(\xi,\eta) = \Omega(\theta(\xi,\eta))$.
We then construct the vector-Jacobian products (VJPs) required by the pullbacks in \Cref{eq:pullback-eta-hat,eq:pullback-H}.

\subsection{Derivatives of $\ell$}

\subsubsection{First derivatives}

By the chain rule, the gradients with respect to $\xi$ and $\eta$ are
%
\begin{align}
  (\ell_\xi)_i = \frac{\partial \ell}{\partial \xi_i}   & = \sum_k \frac{\partial \Omega}{\partial \theta_k} \frac{\partial \theta_k}{\partial \xi_i} = \sum_k (\Theta_\xi)_{ki} (\nabla_\theta \Omega)_k,   \\
  (\ell_\eta)_j = \frac{\partial \ell}{\partial \eta_j} & = \sum_k \frac{\partial \Omega}{\partial \theta_k} \frac{\partial \theta_k}{\partial \eta_j} = \sum_k (\Theta_\eta)_{kj} (\nabla_\theta \Omega)_k.
\end{align}
%
In matrix notation:
%
\begin{align}
  \ell_\xi = \Theta_{\xi}^{T} \nabla_\theta \Omega, \quad \ell_\eta = \Theta_{\eta}^{T} \nabla_\theta \Omega.
\end{align}

\subsubsection{Second derivatives}

Differentiating the gradients yields the Hessian matrices.
For $\ell_{\eta\eta} = \frac{\partial^2 \ell}{\partial \eta^2}$, we have
%
\begin{align}
  (\ell_{\eta\eta})_{ij} & = \frac{\partial}{\partial \eta_i} (\ell_\eta)_j = \frac{\partial}{\partial \eta_i} \left( \sum_k \frac{\partial \Omega}{\partial \theta_k} \frac{\partial \theta_k}{\partial \eta_j} \right)                                                                                                           \\
                         & = \sum_k \left( \frac{\partial}{\partial \eta_i}\left(\frac{\partial \Omega}{\partial \theta_k}\right) \frac{\partial \theta_k}{\partial \eta_j} + \frac{\partial \Omega}{\partial \theta_k} \frac{\partial^2 \theta_k}{\partial \eta_i \partial \eta_j} \right)                                        \\
                         & = \sum_k \left( \left( \sum_m \frac{\partial^2 \Omega}{\partial \theta_m \partial \theta_k} \frac{\partial \theta_m}{\partial \eta_i} \right) \frac{\partial \theta_k}{\partial \eta_j} + \frac{\partial \Omega}{\partial \theta_k} \frac{\partial^2 \theta_k}{\partial \eta_i \partial \eta_j} \right) \\
                         & = \sum_{k,m} (\Theta_\eta)_{mi} (\nabla_\theta^2 \Omega)_{mk} (\Theta_\eta)_{kj} + \sum_k (\nabla_\theta \Omega)_k (\Theta_{\eta\eta}^{(\theta_k)})_{ij}.
\end{align}
%
In matrix and Einstein summation notation (summing over repeated indices $a, b, c \in \{1,2,3\}$):
%
\begin{align}
  \ell_{\eta \eta}       & = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\eta + \sum_k (\nabla_\theta \Omega)_{k}   \Theta_{\eta\eta}^{(\theta_k)},                                                                                                                               \\
  (\ell_{\eta\eta})_{ij} & = \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial \Omega}{\partial \theta_c} \frac{\partial^2 \theta_c}{\partial \eta_i \partial \eta_j}.
\end{align}
%
The mixed Hessian $\ell_{\eta\xi}$ follows similarly:
%
\begin{align}
  \ell_{\eta \xi} = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\xi + \sum_k (\nabla_\theta \Omega)_{k}   \Theta_{\eta\xi}^{(\theta_k)}.
\end{align}

\subsubsection{Third derivatives}

Differentiating $(\ell_{\eta\eta})_{ij}$ with respect to $\xi_k$ and $\eta_l$ via the product and chain rules yields the third-derivative tensors.
Using the Einstein notation form:
%
\begin{align}
  (\ell_{\eta\eta\xi})_{ijk} = \frac{\partial (\ell_{\eta\eta})_{ij}}{\partial \xi_k} & = \frac{\partial}{\partial \xi_k} \left( \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial \Omega}{\partial \theta_c} \frac{\partial^2 \theta_c}{\partial \eta_i \partial \eta_j} \right)                                                                                                                                                                                                               \\
                                                                                      & = \left( \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} \frac{\partial \theta_c}{\partial \xi_k} \right) \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \left( \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial^2 \theta_b}{\partial \eta_j \partial \xi_k} \right) \\
                                                                                      & \quad + \left( \frac{\partial^2 \Omega}{\partial \theta_c \partial \theta_d} \frac{\partial \theta_d}{\partial \xi_k} \right) \frac{\partial^2 \theta_c}{\partial \eta_i \partial \eta_j} + \frac{\partial \Omega}{\partial \theta_c} \frac{\partial^3 \theta_c}{\partial \eta_i \partial \eta_j \partial \xi_k}.
\end{align}
%
Rearranging and relabeling dummy indices recovers the expression in \cref{eq:third-derivative-ell-eta-eta-xi}.
Differentiation with respect to $\eta_l$ yields the corresponding expression in \cref{eq:third-derivative-ell-eta-eta-eta}.

\subsubsection{Vector-Jacobian products with $\ell_{\eta\eta}$}

The VJPs needed for the pullbacks involve contracting the third-derivative tensors of $\ell$ with the symmetric cotangent matrix $\Delta^H$.
The VJP with respect to $\xi_k$ is $(\Delta^H \bullet \ell_{\eta\eta\xi})_k = \Delta^H_{ij} (\ell_{\eta\eta\xi})_{ijk}$:
%
\begin{align}
  \Delta^H_{ij} (\ell_{\eta\eta\xi})_{ijk} & = \Delta^H_{ij} \left( \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} \frac{\partial \theta_c}{\partial \xi_k} \right) + \Delta^H_{ij} \left( \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \left( \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial^2 \theta_b}{\partial \eta_j \partial \xi_k} \right) \right) \\
                                           & + \Delta^H_{ij} \left( \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial \theta_b}{\partial \xi_k} \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \right) + \Delta^H_{ij} \left( \frac{\partial \Omega}{\partial \theta_a} \frac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k} \right).
\end{align}
%
Exploiting the symmetry of $\Delta^H$ and $\frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b}$, the second term becomes $2 \Delta^H_{ij} \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j}$.
Grouping by derivatives of $\Omega$ yields
%
\begin{align}
  \frac{\partial (\Delta^H \bullet \ell_{\eta\eta})}{\partial \xi_k} & = \left( \Delta^H_{ij} \frac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k} \right) \frac{\partial \Omega}{\partial \theta_a}                                                                                                                                                                      \\
                                                                     & + \left( 2 \Delta^H_{ij} \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j} + \Delta^H_{ij} \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \frac{\partial \theta_b}{\partial \xi_k} \right) \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \\
                                                                     & + \left( \Delta^H_{ij} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} \right) \frac{\partial \theta_c}{\partial \xi_k} \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}.
\end{align}
%
This confirms the VJP with respect to $\xi_k$ in \cref{eq:third-derivative-ell-eta-eta-xi-delta-h}.
An identical procedure for $\Delta^H_{ij} (\ell_{\eta\eta\eta})_{ijl}$ yields the VJP with respect to $\eta_l$ in \cref{eq:third-derivative-ell-eta-eta-eta-delta-h}.

\subsection{Explicit derivative expressions}\label{app:explicit-derivative-expressions}

We now derive explicit expressions for the derivatives of $\ell$ by substituting the concrete derivatives of $\theta$ into the general chain rule formulas.

\subsubsection{First derivatives}

Using the chain rule, the gradients of $\ell$ with respect to $\xi$ and $\eta$ are
%
\begin{align}
  \ell_\xi = \Theta_\xi^T \nabla_\theta \Omega =
  \begin{bmatrix} \ell_x \\ \ell_\mu \\ \ell_\delta \end{bmatrix}
  =
  \begin{bmatrix}
    \frac{1}{\sigma} & 0                & 0                \\
    0                & \frac{s}{\sigma} & 0                \\
    0                & 0                & \frac{1}{\sigma}
  \end{bmatrix}
  \begin{bmatrix} \Omega_{x'} \\ \Omega_{\nu'} \\ \Omega_{\delta'} \end{bmatrix}
  =
  \begin{bmatrix}
    \frac{1}{\sigma} \Omega_{x'}   \\
    \frac{s}{\sigma} \Omega_{\nu'} \\
    \frac{1}{\sigma} \Omega_{\delta'}
  \end{bmatrix},
\end{align}
%
and
%
\begin{align}
  \ell_\eta = \Theta_\eta^T \nabla_\theta \Omega =
  \begin{bmatrix} \ell_{\log s} \\ \ell_{\log \sigma} \end{bmatrix}
  =
  \begin{bmatrix}
    0   & \nu'  & 0        \\
    -x' & -\nu' & -\delta'
  \end{bmatrix}
  \begin{bmatrix} \Omega_{x'} \\ \Omega_{\nu'} \\ \Omega_{\delta'} \end{bmatrix}
  =
  \begin{bmatrix}
    \nu' \Omega_{\nu'} \\
    -x' \Omega_{x'} - \nu' \Omega_{\nu'} - \delta' \Omega_{\delta'}
  \end{bmatrix}.
\end{align}
%
Here we use the shorthand $\Omega_{x'} \coloneqq \frac{\partial \Omega}{\partial x'}$, and similarly for other components.

\subsubsection{Second derivatives}

The Hessian $\ell_{\eta\eta}$ is obtained by differentiating $\ell_\eta$:
%
\begin{align}
  \ell_{\eta\eta} = \frac{\partial}{\partial \eta} (\ell_\eta) = \frac{\partial}{\partial \eta} (\Theta_\eta^T \nabla_\theta \Omega) = \underbrace{(\frac{\partial}{\partial \eta} \Theta_\eta^T) \nabla_\theta \Omega}_{H^{\text{lin}}} + \underbrace{\Theta_\eta^T (\frac{\partial}{\partial \eta} \nabla_\theta \Omega)}_{H^{\text{quad}}}.
\end{align}
%
The linear part $H^{\text{lin}} = (\frac{\partial}{\partial \eta} \Theta_\eta^T) \nabla_\theta \Omega$ expands to
%
\begin{align}
  H^{\text{lin}} & = \sum_k \Omega_k \Theta_{\eta\eta}^{(\theta_k)} = \Omega_{x'} \Theta_{\eta\eta}^{(x')} + \Omega_{\nu'} \Theta_{\eta\eta}^{(\nu')} + \Omega_{\delta'} \Theta_{\eta\eta}^{(\delta')} \\
                 & = \Omega_{x'} \begin{bsmallmatrix} 0 & 0 \\ 0 & x' \end{bsmallmatrix} + \Omega_{\nu'} \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix} + \Omega_{\delta'} \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta' \end{bsmallmatrix}                                    \\
                 & = \begin{bsmallmatrix} \nu' \Omega_{\nu'} & -\nu' \Omega_{\nu'} \\ -\nu' \Omega_{\nu'} & x' \Omega_{x'} + \nu' \Omega_{\nu'} + \delta' \Omega_{\delta'} \end{bsmallmatrix}.
\end{align}
%
The components of this symmetric matrix are
%
\begin{align}
  H^{\text{lin}}_{11} & = \nu' \Omega_{\nu'},                                             \\
  H^{\text{lin}}_{12} & = -\nu' \Omega_{\nu'},                                            \\
  H^{\text{lin}}_{22} & = x' \Omega_{x'} + \nu' \Omega_{\nu'} + \delta' \Omega_{\delta'}.
\end{align}
%
The quadratic part $H^{\text{quad}} = \Theta_\eta^T (\nabla_\theta^2 \Omega) \Theta_\eta$ expands to
%
\begin{align}
  H^{\text{quad}} & = \begin{bmatrix}
    0   & \nu'  & 0        \\
    -x' & -\nu' & -\delta'
  \end{bmatrix}
  \begin{bmatrix}
    \Omega_{x'x'}      & \Omega_{x'\nu'}      & \Omega_{x'\delta'}      \\
    \Omega_{\nu'x'}    & \Omega_{\nu'\nu'}    & \Omega_{\nu'\delta'}    \\
    \Omega_{\delta'x'} & \Omega_{\delta'\nu'} & \Omega_{\delta'\delta'}
  \end{bmatrix}
  \begin{bmatrix}
    0    & -x'      \\
    \nu' & -\nu'    \\
    0    & -\delta'
  \end{bmatrix}                      \\
                  & = \begin{bsmallmatrix}
    \nu'^2 \Omega_{\nu'\nu'} & -x'\nu'\Omega_{x'\nu'} - \nu'^2\Omega_{\nu'\nu'} - \nu'\delta'\Omega_{\nu'\delta'} \\
    -x'\nu'\Omega_{x'\nu'} - \nu'^2\Omega_{\nu'\nu'} - \nu'\delta'\Omega_{\nu'\delta'} & x'^2\Omega_{x'x'} + \nu'^2\Omega_{\nu'\nu'} + \delta'^2\Omega_{\delta'\delta'} + 2x'\nu'\Omega_{x'\nu'} + 2x'\delta'\Omega_{x'\delta'} + 2\nu'\delta'\Omega_{\nu'\delta'}
  \end{bsmallmatrix},
\end{align}
%
where we use $\Omega_{x'x'} \coloneqq \frac{\partial^2 \Omega}{\partial x'^2}$, etc.
The components are
%
\begin{align}
  H^{\text{quad}}_{11} & = \nu'^2 \Omega_{\nu'\nu'},                                                                                                                                                  \\
  H^{\text{quad}}_{12} & = -x'\nu'\Omega_{x'\nu'} - \nu'^2\Omega_{\nu'\nu'} - \nu'\delta'\Omega_{\nu'\delta'},                                                                                        \\
  H^{\text{quad}}_{22} & = x'^2\Omega_{x'x'} + \nu'^2\Omega_{\nu'\nu'} + \delta'^2\Omega_{\delta'\delta'} + 2x'\nu'\Omega_{x'\nu'} + 2x'\delta'\Omega_{x'\delta'} + 2\nu'\delta'\Omega_{\nu'\delta'},
\end{align}
%
where we have used the symmetry of $\nabla_\theta^2 \Omega$.

\subsubsection{Vector-Jacobian products with $\ell_{\eta\eta}$}

We now derive the vector-Jacobian products required for the third-order pullbacks.
Starting from the expression
%
\begin{align}
  \ell_{\eta\eta} = \Theta_\eta^T (\nabla_\theta^2 \Omega) \Theta_\eta + \sum_{k} (\nabla_\theta \Omega)_k \Theta_{\eta\eta}^{(\theta_k)},
\end{align}
%
we contract with the symmetric cotangent $\Delta^H$ to obtain the scalar
%
\begin{align}
  S \coloneqq \Delta^H \bullet \ell_{\eta\eta}
  = \Delta^H_{ij} (\ell_{\eta\eta})_{ij}
  = \underbrace{(\Theta_\eta \Delta^H \Theta_\eta^T)_{ab}}_{\Delta^\theta_{ab}} \Omega_{\theta_a \theta_b}
  + \underbrace{\operatorname{tr}(\Delta^H \Theta_{\eta\eta}^{(\theta_k)})}_{\alpha_k} \Omega_{\theta_k}
\end{align}
%
where we have defined $\Delta^\theta = \Theta_\eta \Delta^H \Theta_\eta^T$ and $\alpha_k = \mathrm{tr}(\Delta^H \Theta_{\eta\eta}^{(\theta_k)})$.
With $\theta = (x', \nu', \delta')$ and the derivative tensors
%
\begin{align}
  \Theta_\eta = \begin{bmatrix} 0 & -x' \\ \nu' & -\nu' \\ 0 & -\delta' \end{bmatrix},
  \quad
  \Theta_{\eta\eta}^{(x')} = \begin{bmatrix} 0 & 0 \\ 0 & x' \end{bmatrix},
  \quad
  \Theta_{\eta\eta}^{(\nu')} = \begin{bmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bmatrix},
  \quad
  \Theta_{\eta\eta}^{(\delta')} = \begin{bmatrix} 0 & 0 \\ 0 & \delta' \end{bmatrix},
\end{align}
%
the nonzero entries of $\Delta^\theta$ and the coefficients $\alpha_k$ are given by
%
\begin{align}
  \Delta^{\theta}_{11} & = x'^2 \Delta^H_{22},                                      &  & \Delta^{\theta}_{12} = -x' \nu' (\Delta^H_{21} - \Delta^H_{22}),       &  & \Delta^{\theta}_{13} = x' \delta' \Delta^H_{22}, \\
  \Delta^{\theta}_{22} & = \nu'^2 (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}), &  & \Delta^{\theta}_{23} = \nu' \delta' (\Delta^H_{22} - \Delta^H_{12}),   &  & \Delta^{\theta}_{33} = \delta'^2 \Delta^H_{22},  \\
  \alpha_{x'}          & = x' \Delta^H_{22},                                        &  & \alpha_{\nu'} = \nu' (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}), &  & \alpha_{\delta'} = \delta' \Delta^H_{22}.
\end{align}
%
Differentiating $S$ with respect to $\theta_c \in \{x', \nu', \delta'\}$ yields
%
\begin{align}
  \frac{\partial S}{\partial \theta_c}
  = \underbrace{\Delta^\theta_{ab} \Omega_{\theta_a \theta_b \theta_c}}_{C_{\theta_c}}
  + \underbrace{\left(\frac{\partial \Delta^\theta_{ab}}{\partial \theta_c}\right) \Omega_{\theta_a \theta_b}}_{H_{\theta_c}}
  + \underbrace{\left(\frac{\partial \alpha_k}{\partial \theta_c}\right) \Omega_{\theta_k}}_{G_{\theta_c}}
  + \underbrace{\alpha_k \Omega_{\theta_k \theta_c}}_{Q_{\theta_c}}.
\end{align}
%
We identify four contributions: the cubic contraction $C_{\theta_c}$, the Hessian term $H_{\theta_c}$, the gradient term $G_{\theta_c}$, and the linear-Hessian coupling $Q_{\theta_c}$.

\paragraph{Gradient contribution.}

From the explicit forms $\alpha_{x'} = x' \Delta^H_{22}$, $\alpha_{\nu'} = \nu' (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22})$, and $\alpha_{\delta'} = \delta' \Delta^H_{22}$, we obtain
%
\begin{align}
  G_{x'}      = \Delta^H_{22} \Omega_{x'}, \qquad
  G_{\nu'}    = (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) \Omega_{\nu'}, \qquad
  G_{\delta'} = \Delta^H_{22} \Omega_{\delta'}.
\end{align}

\paragraph{Hessian contribution.}

Defining $\Theta_{\eta\theta_c} \coloneqq \frac{\partial \Theta_\eta}{\partial \theta_c}$, the derivative of $\Delta^\theta$ is
%
\begin{align}
  \frac{\partial \Delta^\theta}{\partial \theta_c} & = \Theta_{\eta\theta_c} \Delta^H \Theta_\eta^T + \Theta_\eta \Delta^H \Theta_{\eta\theta_c}^T, \\
  \text{where} \quad
  \Theta_{\eta x'}                                 & = \begin{bsmallmatrix} 0 & -1 \\ 0 & 0 \\ 0 & 0 \end{bsmallmatrix},
  \quad
  \Theta_{\eta\nu'} = \begin{bsmallmatrix} 0 & 0 \\ 1 & -1 \\ 0 & 0 \end{bsmallmatrix},
  \quad
  \Theta_{\eta\delta'} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \\ 0 & -1 \end{bsmallmatrix}.
\end{align}
%
Contracting with $\Omega_{\theta\theta}$ yields
%
\begin{align}
  H_{x'}      & = 2 \Delta^H_{22} \big( x' \Omega_{x'x'} + \delta' \Omega_{x'\delta'} \big) + 2(\Delta^H_{22} - \Delta^H_{12}) \nu' \Omega_{x'\nu'},                                         \\
  H_{\nu'}    & = 2(\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) \nu' \Omega_{\nu'\nu'} + 2(\Delta^H_{22} - \Delta^H_{12}) \big( x' \Omega_{x'\nu'} + \delta' \Omega_{\nu'\delta'} \big), \\
  H_{\delta'} & = 2 \Delta^H_{22} \big( x' \Omega_{x'\delta'} + \delta' \Omega_{\delta'\delta'} \big) + 2(\Delta^H_{22} - \Delta^H_{12}) \nu' \Omega_{\nu'\delta'}.
\end{align}

\paragraph{Cubic contribution.}

The cubic contraction is defined by
%
\begin{align}
  C_{\theta_c} = \Delta^\theta_{ab} \Omega_{\theta_a \theta_b \theta_c}
  = (\Theta_\eta \Delta^H \Theta_\eta^T)_{ab} \Omega_{\theta_a \theta_b \theta_c}.
\end{align}

\paragraph{Linear-Hessian coupling.}

Using the coefficients $\alpha_k$ derived above, we have
%
\begin{align}
  Q_{x'}      = x' \Delta^H_{22} \Omega_{x'x'} + \nu' (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) \Omega_{\nu' x'} + \delta' \Delta^H_{22} \Omega_{\delta' x'},      \\
  Q_{\nu'}    = x' \Delta^H_{22} \Omega_{x'\nu'} + \nu' (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) \Omega_{\nu'\nu'} + \delta' \Delta^H_{22} \Omega_{\delta' \nu'}, \\
  Q_{\delta'} = x' \Delta^H_{22} \Omega_{x'\delta'} + \nu' (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) \Omega_{\nu'\delta'} + \delta' \Delta^H_{22} \Omega_{\delta' \delta'}.
\end{align}

\paragraph{Combined expressions.}

Summing all four contributions, each $\theta$-space component is
%
\begin{align}
  \mathcal{J}_{\theta_c} \coloneqq \frac{\partial S}{\partial \theta_c}
  = G_{\theta_c} + H_{\theta_c} + C_{\theta_c} + Q_{\theta_c}.
\end{align}
%
After algebraic simplification, we have
%
\begin{align}
  \mathcal{J}_{x'}      & = \Delta^H_{22} \Omega_{x'} + 3 \Delta^H_{22} \big( x' \Omega_{x'x'} + \delta' \Omega_{x'\delta'} \big) + (\Delta^H_{11} - 4\Delta^H_{12} + 3\Delta^H_{22}) \nu' \Omega_{x'\nu'} + C_{x'},                                                                                                                          \\
  \mathcal{J}_{\nu'}    & = (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) \big( \Omega_{\nu'} + 3\nu' \Omega_{\nu'\nu'} \big) + (3\Delta^H_{22} - 2\Delta^H_{12}) \big( x' \Omega_{x'\nu'} + \delta' \Omega_{\nu'\delta'} \big) + C_{\nu'},                                                                                                                        \\
  \mathcal{J}_{\delta'} & = \Delta^H_{22} \big( \Omega_{\delta'} + 3 x' \Omega_{x'\delta'} + 3 \delta' \Omega_{\delta'\delta'} \big) + (\Delta^H_{11} - 4\Delta^H_{12} + 3\Delta^H_{22}) \nu' \Omega_{\nu'\delta'} + C_{\delta'}.
\end{align}

\paragraph{Transformation to $(\xi,\eta)$ coordinates.}

Recalling $\theta = (x', \nu', \delta') = (\frac{x}{\sigma}, \frac{s\mu}{\sigma}, \frac{\delta}{\sigma})$ and the Jacobians
%
\begin{align}
  \Theta_\xi = \frac{\partial \theta}{\partial \xi} = \operatorname{diag}\Big(\frac{1}{\sigma}, \frac{s}{\sigma}, \frac{1}{\sigma}\Big),
  \qquad
  \Theta_\eta = \frac{\partial \theta}{\partial \eta} = \begin{bmatrix} 0 & -x' \\ \nu' & -\nu' \\ 0 & -\delta' \end{bmatrix}
\end{align}
%
and applying the chain rule, the desired VJPs in $(\xi,\eta)$ coordinates are
%
\begin{align}
  (\Delta^H \bullet \ell_{\eta\eta\xi})_x              & = \frac{1}{\sigma} \mathcal{J}_{x'},                                                          \\
  (\Delta^H \bullet \ell_{\eta\eta\xi})_\mu            & = \frac{s}{\sigma} \mathcal{J}_{\nu'},                                                        \\
  (\Delta^H \bullet \ell_{\eta\eta\xi})_\delta         & = \frac{1}{\sigma} \mathcal{J}_{\delta'},                                                     \\
  (\Delta^H \bullet \ell_{\eta\eta\eta})_{\log s}      & = \nu' \mathcal{J}_{\nu'},                                                                    \\
  (\Delta^H \bullet \ell_{\eta\eta\eta})_{\log \sigma} & = -\big( x' \mathcal{J}_{x'} + \nu' \mathcal{J}_{\nu'} + \delta' \mathcal{J}_{\delta'} \big).
\end{align}
%
These expressions complete the derivation of the third-order VJPs required for the pullback computations.

\end{document}
