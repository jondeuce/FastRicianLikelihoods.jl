\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage[sort,compress]{cleveref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{siunitx}

\graphicspath{{figures/output/}}
\sisetup{round-mode = figures, round-precision = 3}

\title{Stable, efficient evaluation of gradients and Hessians for the Rician log-likelihood}
\author{Jonathan Doucette}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}

We consider the Rician probability density for a positive observation $x>0$ with noncentrality parameter $\nu>0$ and scale $\sigma>0$.
The PDF is
%
\begin{align}
  p(x \mid \nu, \sigma) = \frac{x}{\sigma^2} \exp\left(-\frac{x^2+\nu^2}{2\sigma^2}\right) I_0\left(\frac{x\nu}{\sigma^2}\right) \label{eq:rician-pdf}
\end{align}
%
where $I_0$ is the modified Bessel function of the first kind of order zero.

We seek numerically stable, machine-precision formulas for the negative log-likelihood and its first, second, and third partial derivatives with respect to $x$ and $\nu$.
If we define
%
\begin{align}
  f(x, \nu) \coloneqq -\log p(x \mid \nu, \sigma = 1),
\end{align}
%
then the log of the likelihood of \cref{eq:rician-pdf} can be given in terms of $f$ via
%
\begin{align}
  \log p(x \mid \nu, \sigma) = -\log\sigma - f\left(\frac{x}{\sigma}, \frac{\nu}{\sigma}\right).
\end{align}
%
Similarly, the derivatives of $\log p$ with respect to $x$, $\nu$, and $\sigma$ are given in terms of the derivatives of $f$ evaluated at $(x', \nu') = (x / \sigma, \nu / \sigma)$:
%
\begin{align}
  \frac{\partial \log p}{\partial x}      = -\frac{1}{\sigma} \frac{\partial f}{\partial x'},                                                                                        \qquad
  \frac{\partial \log p}{\partial \nu}    = -\frac{1}{\sigma} \frac{\partial f}{\partial \nu'},                                                                                      \qquad
  \frac{\partial \log p}{\partial \sigma} = \frac{1}{\sigma} \left( x' \frac{\partial f}{\partial x'} + \nu' \frac{\partial f}{\partial \nu'} - 1 \right)
\end{align}
%
and likewise for higher-order derivatives.

Henceforth we take $\sigma=1$ and, by slight abuse of notation, write $x, \nu$ for the rescaled variables $x', \nu'$.
We will show that the key to numerically stable evaluation of $f$ and its derivatives is to carefully consider the limiting behavior in the regimes where $z = x\nu \ll 1$ and $z \gg 1 \Leftrightarrow u=1/z \ll 1$.
Naive evaluation of $f$ and its derivatives produces expressions with catastrophic cancellation in both regimes.
To evaluate these expressions in a numerically stable way, we derive algebraic simplifications in both the $z \ll 1$ and $z \gg 1$ regimes.

\section{Methods}

\subsection{The Rician log-likelihood and basic simplifications}\label{sec:rician-log-likelihood-and-basic-simplifications}

With $\sigma=1$, the Rician negative log-likelihood is given by
%
\begin{align}
  f(x, \nu) & \eqqcolon -\log p(x \mid \nu, \sigma=1)                                                                                                                        \\
            & = \frac{x^2 + \nu^2}{2} - \log x - \log I_0(x \nu) \label{eq:rician-neg-log-likelihood}                                                                        \\
            & = \frac{(x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{x}{\nu}\right) - \log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi). \label{eq:rician-neg-log-likelihood-scaled}
\end{align}
%
Here, $\hat{I}_0$ is the modified Bessel function of the first kind normalized such that $\lim_{z \mapsto \infty} \hat{I}_0(z) = 1$:
%
\begin{align}
  I_0(z) = \frac{e^z}{\sqrt{2\pi z}} \hat{I}_0(z) \quad \Leftrightarrow \quad \hat{I}_0(z) = \sqrt{2\pi z} e^{-z} I_0(z)
\end{align}
%
where $z = x\nu \ge 0$.
Next, we consider the numerical stability of these algebraically equivalent forms of $f$.

\subsubsection{Stability of the Log-Likelihood}

\paragraph{Small $z \ll 1$.}

For small $z$, we have that $\log I_0(z) = z^2/4 + \mathcal{O}(z^4)$, and thus \cref{eq:rician-neg-log-likelihood} becomes
%
\begin{align}
  f(x, \nu) = \frac{x^2 + \nu^2}{2} - \log x - \frac{z^2}{4} + \mathcal{O}(z^4).
\end{align}
%
The terms in this expression are generally not of the same order of magnitude, so there are no cancellation issues.
% This is slightly more numerically stable than using $\log \hat{I}_0(z) = \log I_0(z) - z + \frac{1}{2}\log(2\pi z) = \frac{1}{2}\log(2\pi z) - z + \mathcal{O}(z^2)$, which may suffer small cancellation issues when computing $-\frac{1}{2}\log(\frac{x}{\nu}) - \log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi) = -\log x + z + \mathcal{O}(z^2)$.
% For example, if $x \approx 1$ and $\nu \approx z$, then $\frac{1}{2}\log(\frac{x}{\nu}) \approx -\frac{1}{2}\log z$ and $-\log \hat{I}_0(x \nu) + \frac{1}{2}\log(2\pi) \approx -\frac{1}{2}\log z$ but the result is $z \ll -\log z$.

\paragraph{Large $z \gg 1$.}

For large $z$, we have that $\log \hat{I}_0(z) = \frac{1}{8 z} + \mathcal{O}(z^{-2})$, and thus \cref{eq:rician-neg-log-likelihood-scaled} becomes
%
\begin{align}
  f(x, \nu) = \frac{(x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{x}{\nu}\right) + \frac{1}{2}\log(2\pi) - \frac{1}{8 z} + \mathcal{O}(z^{-2}).
\end{align}
%
The terms in this expression are generally not of the same order of magnitude -- unless $x \approx \nu$, which we address next -- so there are typically no cancellation issues.

\paragraph{High SNR $x \approx \nu \approx \sqrt{z} \gg 1$.}

A common case in practice is when the signal-to-noise ratio (SNR) is large:
observations are concentrated near the noncentrality parameter such that $x \approx \nu \pm 1$ (recall that the noise level is normalized to $\sigma=1$).
As was the case for large $z$, we should use the scaled form of \cref{eq:rician-neg-log-likelihood-scaled} but with a stable evaluation of $\log(x/\nu)$:
%
\begin{align}
  \log\left(\frac{x}{\nu}\right) = \begin{cases}
    \log\left(1 + \frac{x-\nu}{\nu}\right) & \text{if } x \ge \nu \\
    -\log\left(1 + \frac{\nu-x}{x}\right)  & \text{if } x < \nu
  \end{cases}
\end{align}
%
where $\log(1 + \epsilon) = \epsilon - \epsilon^2/2 + \mathcal{O}(\epsilon^3)$ is computed using the common special-function routine \texttt{log1p($\epsilon$)} which is designed to be accurate when $\epsilon$ is small.
With this modification, \cref{eq:rician-neg-log-likelihood-scaled} is accurate for all $z \gg 1$.

We note that \cref{eq:rician-neg-log-likelihood} is not stable in the high SNR regime, since $\log I_0(z) = \log \hat{I}_0(z) + z - \frac{1}{2}\log(2\pi z)$ has a large component $z$ which catastrophically cancels with $(x^2 + \nu^2) / 2 \approx z$.

\subsubsection{First derivatives}

Now, we differentiate $f$ with respect to $x$ and $\nu$ and simplify algebraically:
%
\begin{align}
  f_x   & = x-\nu -\frac{1}{2x} - \nu\frac{d}{dz}\log\hat{I}_0(z) \label{eq:first-derivatives-unsimplified-x}  \\
  f_\nu & = \nu-x +\frac{1}{2\nu} - x\frac{d}{dz}\log\hat{I}_0(z) \label{eq:first-derivatives-unsimplified-nu}
\end{align}
%
where
%
\begin{align}
  \frac{d}{dz}\log\hat{I}_0(z) & = r(z) - 1 + \frac{1}{2z} \label{eq:log-scaled-bessel-derivative} \\
  r(z)                         & = \frac{I_1(z)}{I_0(z)}. \label{eq:ratio-r}
\end{align}
%
Substituting \cref{eq:log-scaled-bessel-derivative} and $z=x\nu$ into \cref{eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu} gives
%
\begin{align}
  f_x   & = x - \nu r - \frac{1}{x} \label{eq:first-derivatives-simplified-x} \\
  f_\nu & = \nu - x r \label{eq:first-derivatives-simplified-nu}
\end{align}

\subsubsection{Second derivatives}

Next, we differentiate the simplified first derivatives in \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu} to obtain the second derivatives:
%
\begin{align}
  f_{xx}     & = \frac{\partial}{\partial x}\left(x - \nu r(z) - \frac{1}{x}\right) = 1 + \frac{1}{x^2} - \nu^2 r' \label{eq:second-derivatives-unsimplified-x-x} \\
  f_{x\nu}   & = \frac{\partial}{\partial \nu}\left(x - \nu r(z) - \frac{1}{x}\right) = -(r + z r') = -z(1 - r^2) \label{eq:second-derivatives-unsimplified-x-nu} \\
  f_{\nu\nu} & = \frac{\partial}{\partial \nu}\left(\nu - x r(z)\right) = 1 - x^2 r' \label{eq:second-derivatives-unsimplified-nu-nu}
\end{align}
%
where the last equality in \cref{eq:second-derivatives-unsimplified-x-nu} follows from the recurrence relation \cref{eq:r-prime-recurrence}.

\subsubsection{Third derivatives}

Differentiate once more to obtain third derivatives;
each depends only on $r'$ and $r''$:
%
\begin{align}
  f_{xxx}       & = \frac{\partial}{\partial x}\left(1+\frac{1}{x^2}-\nu^2 r'(z)\right) = -\frac{2}{x^3} - \nu^3 r'' \label{eq:third-derivatives-unsimplified-x-x-x}                                 \\
  f_{xx\nu}     & = \frac{\partial}{\partial \nu}\left(1+\frac{1}{x^2}-\nu^2 r'(z)\right) = -2\nu r' - x\nu^2 r'' = -\nu(2r' + z r'')              \label{eq:third-derivatives-unsimplified-x-x-nu}  \\
  f_{x\nu\nu}   & = \frac{\partial}{\partial x}\left(1 - x^2 r'(z)\right) = -2x r' - x^2\nu r'' = -x(2r' + z r'')                                  \label{eq:third-derivatives-unsimplified-x-nu-nu} \\
  f_{\nu\nu\nu} & = \frac{\partial}{\partial \nu}\left(1 - x^2 r'(z)\right) = -x^3 r''. \label{eq:third-derivatives-unsimplified-nu-nu-nu}
\end{align}
%
Therefore it suffices to compute $r$, $r'$, and $r''$ with high relative accuracy.

\subsubsection{Recurrence relations for derivatives of $r$}

To obtain $r$, $r'$, and $r''$, we differentiate $r=I_1/I_0$.
Using $I_0'=I_1$ and $I_1'=I_0 - I_1/z$, we have the recurrence relations
%
\begin{align}
  r'  & = \frac{I_1'}{I_0} - r\frac{I_0'}{I_0} = \frac{I_0 - \frac{1}{z} I_1}{I_0} - r^2 = 1 - \frac{r}{z} - r^2 \label{eq:r-prime-recurrence}                                           \\
  r'' & = -\left(\frac{r}{z}\right)' - 2 r r' = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r' = 2 r (r^2 - 1) + \frac{3 r^2 - 1}{z} + \frac{2 r}{z^2} \label{eq:r-second-derivative-recurrence}
\end{align}

These are exact identities but are numerically unstable as $z \mapsto 0$ and as $z \mapsto \infty$.
To address this, for each of $r$, $r'$, and $r''$, we split the domain $z>0$ into three regimes and approximate a single well-scaled quantity in each regime:
\begin{itemize}
  \item Small $z$:
        use a Taylor expansion, group terms by order in $z$, and fit a minimax polynomial to the residual.
  \item Large $z$:
        use the asymptotic expansion in $u = 1/z$, group terms by order in $u$, and fit a minimax polynomial to the residual.
  \item Intermediate $z$:
        cancellation is negligible;
        employ a rational minimax approximant.
\end{itemize}

\subsubsection{Small-$z$ reparametrizations for $r$, $r'$, $r''$}\label{sec:bessel-ratio-small-z}

For small $z$, $r \approx z/2$.
We therefore define
%
\begin{align}\label{eq:a0-small-z}
  a_0(z) = \frac{r(z)}{z} = \frac{1}{2} - \frac{1}{16}z^2 + \mathcal{O}(z^4)
\end{align}
%
so that computing $r$ reduces to approximating $a_0$ to high precision:
%
\begin{align}\label{eq:r-small-reparametrized}
  \boxed{r(z) = z a_0(z).}
\end{align}
%
For $r'$, we substitute \cref{eq:r-small-reparametrized} into \cref{eq:r-prime-recurrence} to obtain
%
\begin{align}\label{eq:r-prime-small-reparametrized}
  \boxed{r'(z) = 1 - \frac{r}{z} - r^2 = 1 - a_0 - z^2 a_0^2}
\end{align}
%
which is numerically stable since $a_0 = \frac{1}{2} + \mathcal{O}(z^2)$, so both the constant coefficient $1 - a_0$ and the quadratic coefficient $-a_0^2$ do not suffer from cancellation in floating-point arithmetic.
However, the second derivative $r''$ requires more care.
Substituting \cref{eq:r-prime-small-reparametrized} into \cref{eq:r-second-derivative-recurrence} yields
%
\begin{align}
  r''(z) & = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r'                                             \\
         & = -\frac{1 - a_0 - z^2 a_0^2}{z} + \frac{z a_0}{z^2} -  2 z a_0(1 - a_0 - z^2 a_0^2) \\
         & = \frac{2 a_0 - 1}{z} + z a_0 (3 a_0 - 2) + 2 z^3 a_0^3.
\end{align}
%
The leading $\mathcal{O}(1/z)$ term has coefficient $2a_0 - 1$, but for small $z$, $a_0 \approx \frac{1}{2} - \frac{1}{16}z^2$, so computing $2a_0 - 1$ requires subtracting two $\mathcal{O}(1)$ terms to obtain a $\mathcal{O}(z^2)$ term.
We therefore reparametrize to
%
\begin{align}
  a_0                 & = \frac{1}{2} + z^2 a_1 \label{eq:a0-reparametrized}                                                          \\
  \Leftrightarrow a_1 & = \frac{1}{z^2} (a_0 - \frac{1}{2}) = \frac{1}{z^2} (\frac{r}{z} - \frac{1}{2}). \label{eq:a1-reparametrized}
\end{align}
%
Note that since $r = \frac{1}{2}z - \frac{1}{16}z^3 + \mathcal{O}(z^5)$, it follows that $a_1 = -\frac{1}{16} + \mathcal{O}(z^2)$.
Using $(2 a_0 - 1) / z = 2 z a_1$, we obtain a numerically stable form for the second derivative:
%
\begin{align}\label{eq:r-prime-prime-small-reparametrized}
  \boxed{r''(z) = z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3}
\end{align}
%
We also note that the quantity $2r' + z r''$ from \cref{eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu} does not suffer cancellation issues, since $2r' = 1 + \mathcal{O}(z^2)$ and $z r'' = \mathcal{O}(z^2)$.
Thus, we need only fit one minimax polynomial to \cref{eq:a1-reparametrized} to estimate $a_1(z)$, and then $r$, $r'$, and $r''$ can be formed from $a_1$ and $a_0 = \frac{1}{2} + z^2 a_1$ without catastrophic cancellation using \cref{eq:r-prime-small-reparametrized,eq:r-prime-prime-small-reparametrized}.

% We can then rewrite the first and second derivatives of $r$ as
% %
% \begin{align}
%   r'(z) & = 1 - a_0 - z^2 a_0^2 \\
%   & = 1 - (\frac{1}{2} + z^2 a_1) - z^2 (\frac{1}{2} + z^2 a_1)^2 \\
%   &= \frac{1}{2} - z^2 (a_1 + \frac{1}{4}) - z^4 a_1 - z^6 a_1^2
% \end{align}
% %
% \begin{align}
%   r''(z) & = \frac{2 a_0 - 1}{z} + z a_0 (3 a_0 - 2) + 2 z^3 a_0^3 \\
%   &= \frac{2(\frac{1}{2} + z^2 a_1)-1}{z} - 2z(\frac{1}{2} + z^2 a_1) + 2z(\frac{1}{2} + z^2 a_1)^{2} + 2z^3 (\frac{1}{2} + z^2 a_1)^3 \\
%          & = z (2a_1 - \frac{1}{2}) + \frac{1}{4} z^3 + z^5 a_1 (\frac{3}{2} + 2 a_1) + z^7 (3 a_1^2) + z^9 (2 a_1^3)
% \end{align}

% Finally, we see from \cref{eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu} that we should double-check the quantity $2r' + zr''$ for cancellation issues:
% %
% \begin{align}
%   2r' + zr'' & = 2(1 - a_0 - z^2 a_0^2) + z(z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3) \\
%              & = 2 - 2a_0 - 2z^2 a_0^2 + z^2 (2a_1 + a_0 (3 a_0 - 2)) + 2 z^4 a_0^3 \\
%              & = 2 a_0^3 z^4 + a_0^2 z^2 - 2 a_0 z^2 - 2 a_0 + 2 a_1 z^2 + 2 \\
%              & = 2 - 2a_0 + z^2 (2a_1 - a_0 (2 - a_0)) + 2 a_0^3 z^4
% \end{align}

\subsubsection{Large-$z$ reparametrizations for $r$, $r'$, and $r''$}\label{sec:bessel-ratio-large-z}

As $z \mapsto \infty$, $r \mapsto 1$.
To avoid cancellation issues analogous to the small-$z$ case, we work with $u=1/z$ and rescale the residual $1-r$ to obtain a well-scaled quantity:
%
\begin{align}
  \boxed{
    \begin{aligned}
      b_0(u)           & = \frac{1-r(z)}{u} = \frac{1}{2} + \frac{1}{8} u + \frac{1}{8} u^2 + \mathcal{O}(u^3) \\
      \Rightarrow r(z) & = 1 - u b_0(u) \label{eq:r-large-reparametrized}
    \end{aligned}
  }
\end{align}
%
Approximating $b_0$ with a minimax polynomial allows us to compute $r$ without catastrophic cancellation.
For the first derivative,
%
\begin{align}
  r'(z) & = 1 - \frac{r}{z} - r^2                                                     \\
        & = 1 - u(1 - u b_0) - (1 - u b_0)^2                                          \\
        & = u (2 b_0 - 1) + u^2 b_0 (1 - b_0) \label{eq:r-prime-large-reparametrized}
\end{align}
%
Analogous to the small-$z$ case, this will lead to cancellation issues since $2 b_0 - 1 = \mathcal{O}(u)$.
Thus, we reparametrize to
%
\begin{align}
  b_1(u)             & = \frac{2 b_0(u)-1}{u} = \frac{1}{4} + \frac{1}{4} u + \mathcal{O}(u^2) \\
  \Rightarrow b_0(u) & = \frac{1 + u b_1(u)}{2}
\end{align}
%
which simplifies $r'$ to
%
\begin{align}
  \boxed{
    \begin{aligned}
      r'(z) = u(2 b_0-1) + u^2 b_0(1-b_0) = u^2 (b_1 + b_0(1-b_0)) \label{eq:r-prime-large-reparametrized-simplified}
    \end{aligned}
  }
\end{align}
%
and avoids cancellation issues since $b_1 \mapsto \frac{1}{4}$ and $b_0 \mapsto \frac{1}{2}$ as $u \mapsto 0$, thus $r' = \frac{1}{2} u^2 + \mathcal{O}(u^3)$.
For the second derivative,
%
\begin{align}
  r'' & = -\frac{r'}{z} + \frac{r}{z^2} - 2 r r'                                                                                                  \\
      & = -u(u^2 (b_1 + b_0(1-b_0))) + u^2 (1 - u b_0) - 2 (1 - u b_0) (u^2 (b_1 + b_0(1-b_0)))                                                   \\
      & = u^2 (2 b_0^2 - 2 b_1 - (2 b_0 - 1)) + u^3 (b_1 (2 b_0 - 1) - 2 b_0 + 3b_0^2 - 2 b_0^3)                                                  \\
      & = u^2 (2 b_0^2 - 2 b_1) + u^3 (b_0 (- 2 + 3b_0 - 2 b_0^2) - b_1) + u^4 b_1^2                                                              \\
      & = u^2 (2 b_0^2 - 2 b_1) + u^3 (b_0 (2(2 b_0 - 1) - b_0 (1 + 2 b_0)) - b_1) + u^4 b_1^2                                                    \\
      & = u^2 (2 b_0^2 - 2 b_1) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0) \label{eq:r-prime-prime-large-reparametrized-unsimplified}
\end{align}
%
Since $b_0 = \frac{1}{2} + \mathcal{O}(u)$ and $b_1 = \frac{1}{4} + \mathcal{O}(u)$, we have $b_0^2 - b_1 = \mathcal{O}(u)$, thus the leading term is order $\mathcal{O}(u^3)$, as expected.
To avoid cancellation issues, we introduce
%
\begin{align}
  b_2(u)             & = \frac{4 b_1(u) - 1}{u} = 1 + \mathcal{O}(u) \label{eq:b2-reparametrized} \\
  \Rightarrow b_1(u) & = \frac{1 + u b_2(u)}{4} \label{eq:b1-reparametrized}
\end{align}
%
Then,
%
\begin{align}
  2 b_0^2 - 2 b_1 & = 2(\frac{1 + u b_1}{2})^2 - 2 b_1 = \frac{1}{2} (1 - 4 b_1 + 2 u b_1 + u^2 b_1^2) \\
                  & = \frac{1}{2} u (-b_2 + 2 b_1) + \frac{1}{2} u^2 b_1^2
\end{align}
%
which we substitute into \cref{eq:r-prime-prime-large-reparametrized-unsimplified} and simplify:
%
\begin{align}
  r'' & = u^2 (2 b_0^2 - 2 b_1) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0)                          \\
      & = \frac{1}{2} u^2 (u (-b_2 + 2 b_1) + u^2 b_1^2) - u^3 (b_0^2 (1 + 2 b_0) + b_1) + u^4 b_1(b_1 + 2 b_0) \\
      & = -u^3 (\frac{1}{2}b_2 + b_0^2 (1 + 2 b_0)) + u^4 b_1 (\frac{3}{2} b_1 + 2 b_0)
\end{align}
%
yielding
%
\begin{align}\label{eq:r-prime-prime-large-reparametrized}
  \boxed{r''(z) = -\frac{1}{2} u^3 (b_2 + 2 b_0^2 (1 + 2 b_0) - u b_1 (3 b_1 + 4 b_0))}
\end{align}
%
Now $r$, $r'$, and $r''$ can be formed from $b_2$, $b_1 = \frac{1 + u b_2}{4}$, and $b_0 = \frac{1 + u b_1}{2}$ using \cref{eq:r-prime-large-reparametrized,eq:r-prime-prime-large-reparametrized} without catastrophic cancellation.
However, as we will see in \cref{sec:rician-third-derivatives}, a final reparameterization
%
\begin{align}
  b_3(u)             & = \frac{b_2(u)-1}{u} = \frac{25}{16} + \mathcal{O}(u) \\
  \Rightarrow b_2(u) & = 1 + u b_3(u)
\end{align}
%
will be required to stably compute the quantity $2r'+zr''$ needed for third derivatives of $f$ when $z \gg 1$.
Additionally, we will need to compute $1-r-zr'$ stably in \cref{sec:second-derivatives-residual}, which can be done using $b_2$ without requiring further reparameterizations.
Finally, we can stably compute all of $r$, $r'$, $r''$, $2r'+zr''$, and $1-r-zr'$ from one minimax polynomial fit to $b_3(u)$.

% \subsection{Intermediate-$z$ analysis} %TODO?

\subsection{Further Rician derivative simplifications}\label{sec:further-rician-derivative-simplifications}

\subsubsection{First derivatives}\label{sec:rician-first-derivatives}

\paragraph{Small-$z$}

The algebraic simplification of \cref{eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu} leading to \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu} is sufficient to avoid catastrophic cancellation.
In particular, when $\nu \ll 1$, the problematic $\frac{1}{2\nu}$ and $\frac{-x}{2z} = \frac{-1}{2\nu}$ terms in \cref{eq:first-derivatives-unsimplified-nu} are cancelled analytically, avoiding subtraction of two $\mathcal{O}(1/\nu)$ terms.
%
\begin{align}\label{eq:first-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_x   & = x - \nu r - \frac{1}{x} \\
      f_\nu & = \nu - x r
    \end{aligned}
  }
\end{align}

\paragraph{Large-$z$}

For $z \gg 1$, we have $r = 1 - b_0(1/z) / z$ from \cref{eq:r-large-reparametrized}, resulting in
%
\begin{align}
  f_x   & = x - \nu (1 - \frac{b_0}{z}) - \frac{1}{x}
  = x - \nu - \frac{1 - b_0}{x}                       \\
  f_\nu & = \nu - x (1 - \frac{b_0}{z})
  = \nu - x + \frac{b_0}{\nu}
\end{align}
%
These forms of $f_x$ and $f_\nu$ are more numerically stable when $x \approx \nu \approx \sqrt{z}$.
The improved numerical stability results from the approximation error being scaled by $1/x$ and $1/\nu$ instead of by $\nu$ and $x$.
If $\hat{r}=r(1+\delta_r)$ and $\hat{b}_0=b_0(1+\delta_b)$, the naive forms incur absolute errors $\mathcal{O}(\nu|\delta_r|)$ in $f_x$ and $\mathcal{O}(x|\delta_r|)$ in $f_\nu$, whereas the simplified forms incur errors $\mathcal{O}(|\delta_b|/x)$ and $\mathcal{O}(|\delta_b|/\nu)$.
Since $z = x\nu \gg 1$ and the relative errors $|\delta_r|$ and $|\delta_b|$ are comparably small, this yields an absolute-error reduction approximately by a factor of $z$.
%
%
\begin{align}\label{eq:first-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_x   & = x - \nu - \frac{1 - b_0}{x} \\
      f_\nu & = \nu - x + \frac{b_0}{\nu}
    \end{aligned}
  }
\end{align}

\subsubsection{Second derivatives}\label{sec:rician-second-derivatives}

\paragraph{Small-$z$}

Similar to the first derivatives, the algebraically simplified expressions in \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu} are sufficient to avoid catastrophic cancellation.
%
%
\begin{align}\label{eq:second-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_{xx}     & = 1 + \frac{1}{x^2} - \nu^2 r' \\
      f_{x\nu}   & = -z (1 - r^2) = -(r+z r')     \\
      f_{\nu\nu} & = 1 - x^2 r'
    \end{aligned}
  }
\end{align}
%
Here $r= z a_0$ and $r' = 1-a_0 - z^2 a_0^2$, with $1-a_0=\frac{1}{2}+\mathcal{O}(z^2)$ and $z^2 a_0^2=\mathcal{O}(z^2)$, so no catastrophic cancellation occurs.
The cross term may use either $-z(1-r^2)$ or $-(r+z r')$:
since $r \approx z/2$ and $zr' \approx z/2$, the former subtracts terms of different magnitudes, and the latter adds terms of the same magnitude but whose leading terms do not suffer cancellation issues.

\paragraph{Large-$z$}

We start by rewriting the second derivatives expressions in \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu} for stability at large-$z$:
%
\begin{align}
  f_{xx}     & = 1 + \frac{1}{x^2} - \nu^2 r' = 1 + \frac{1}{x^2}(1 - z^2 r') \\
  f_{x\nu}   & = -z(1 - r^2) = -z (1 - r) (1 + r)                             \\
  f_{\nu\nu} & = 1 - x^2 r'
\end{align}
%
Recall that for large-$z$, we have from \cref{eq:r-large-reparametrized,eq:r-prime-large-reparametrized-simplified} that
\begin{align}
  r                      & = 1 - \frac{b_0}{z} = 1 - \frac{1}{2z} - \frac{1}{8z^2} + \mathcal{O}(1/z^3)                \\
  r'                     & = \frac{1}{z^2} (b_1 + b_0(1 - b_0)) = \frac{1}{2z^2} + \frac{1}{4z^3} + \mathcal{O}(1/z^4) \\
  \Rightarrow z(1 - r^2) & = r + z r' = 1 + \frac{1}{z} (b_1 - b_0^2) = 1 + \frac{1}{8z^2} + \mathcal{O}(1/z^3)
\end{align}
where $b_0 = \frac{1}{2} (1 + u b_1)$.
%
We make the following observations:
%
\begin{itemize}
  \item $f_{xx}$:
        since $z^2 r' \approx \frac{1}{2}$, $1 - z^2 r'$ has no cancellation issues and $f_{xx} \approx 1 + \frac{1}{2x^2}$.
  \item $f_{\nu\nu}$:
        we have that $x^2 r' \approx \frac{x^2}{2z^2} = \frac{1}{2\nu^2}$;
        this also has no cancellation issues, and $f_{\nu\nu} \approx 1 - \frac{1}{2\nu^2}$.
  \item $f_{x\nu}$:
        $1 + r$ is stable to compute, but we must rewrite $z(1 - r) = b_0$ to avoid cancellation, so $f_{x\nu} = -b_0(1 + r) \approx -1 - \frac{1}{8z^2}$.
\end{itemize}
%
and therefore the stable forms are:
%
\begin{align}\label{eq:second-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_{xx}     & = 1 + \frac{1}{x^2}(1 - z^2 r') \\
      f_{x\nu}   & = -b_0 (1 + r)                  \\
      f_{\nu\nu} & = 1 - x^2 r'
    \end{aligned}
  }
\end{align}

% \subsubsection{Error analysis}
%
% Following the large-$z$ analysis for the first derivatives, we substitute $r = 1 - \frac{b_0}{z}$ from \cref{eq:r-large-reparametrized} and $r' = \frac{1}{z^2} (b_1 + b_0(1 - b_0))$ from \cref{eq:r-prime-large-reparametrized-simplified} into the second derivatives expressions \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu}:
% %
% \begin{align}\label{eq:second-derivatives-large-z}
%   \boxed{
%     \begin{aligned}
%       f_{xx}     & = 1 + \frac{1 - (b_1 + b_0(1-b_0))}{x^2} \\
%       f_{x\nu}   & = -b_0 (2 - \frac{b_0}{z})               \\
%       f_{\nu\nu} & = 1 - \frac{b_1 + b_0(1-b_0)}{\nu^2}
%     \end{aligned}
%   }
% \end{align}
% %
% To analyze the error propagation, let $\hat{r}=r(1+\delta_r)$, $\hat{r}' = r'(1+\delta_{r'})$, $\hat{b}_0=b_0(1+\delta_{b_0})$, and $\hat{b}_1=b_1(1+\delta_{b_1})$ with comparable small relative errors.
% For the naive forms, we have
% %
% \begin{align}
%   |\hat{f}_{xx} - f_{xx}|         & = \nu^2 |\hat{r}' - r'| = \nu^2 |r'| |\delta_{r'}| \sim \nu^2 \frac{1}{z^2} |\delta_{r'}| = \frac{|\delta_{r'}|}{x^2}      \\
%   |\hat{f}_{\nu\nu} - f_{\nu\nu}| & = x^2 |\hat{r}' - r'| = x^2 |r'| |\delta_{r'}| \sim x^2 \frac{1}{z^2} |\delta_{r'}| = \frac{|\delta_{r'}|}{\nu^2}          \\
%   |\hat{f}_{x\nu} - f_{x\nu}|     & = |\hat{r} + z\hat{r}' - (r + zr')| \le |r| |\delta_r| + z |r'| |\delta_{r'}| \sim |\delta_r| + \frac{1}{z} |\delta_{r'}|.
% \end{align}
% %
% Next, the stable large-$z$ forms:
% Let $c = b_1 + b_0(1 - b_0)$ so that $f_{xx} = 1 + (1-c)/x^2$ and $f_{\nu\nu} = 1 - c/\nu^2$.
% Then, we have that:
% %
% \begin{align}
%   \hat{c} - c & = (\hat{b}_1 - b_1) + (\hat{b}_0 - b_0) - (\hat{b}_0^2 - b_0^2)             \\
%               & = b_1 \delta_{b_1} + b_0 \delta_{b_0} - b_0 \delta_{b_0} (\hat{b}_0 + b_0).
% \end{align}
% %
% This yields the first-order error bounds:
% %
% \begin{align}
%   |\hat{f}_{xx} - f_{xx}|         & = | \frac{c - \hat{c}}{x^2} | \lesssim \frac{|b_1| |\delta_{b_1}| + |b_0| (1 + |\hat{b}_0 + b_0|) |\delta_{b_0}|}{x^2}     \\
%   |\hat{f}_{\nu\nu} - f_{\nu\nu}| & = | \frac{c - \hat{c}}{\nu^2} | \lesssim \frac{|b_1| |\delta_{b_1}| + |b_0| (1 + |\hat{b}_0 + b_0|) |\delta_{b_0}|}{\nu^2} \\
%   |\hat{f}_{x\nu} - f_{x\nu}|     & = | 2 (b_0 - \hat{b}_0) + \frac{\hat{b}_0^2 - b_0^2}{z} | \le 2 (|b_0| + \frac{|b_0||\hat{b}_0 + b_0|}{z}) |\delta_{b_0}|.
% \end{align}

\subsubsection{Third derivatives}\label{sec:rician-third-derivatives}

\paragraph{Small-$z$}

We begin by noting that the third derivative expressions in \cref{eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-nu-nu-nu} depend only on $r'$ and $r''$.
Recall the small-$z$ parametrizations for $r'$ and $r''$ from \cref{eq:r-prime-small-reparametrized,eq:r-prime-prime-small-reparametrized}:
%
\begin{align}
  r'                      & = 1 - a_0 - z^2 a_0^2 = \frac{1}{2} - \frac{3}{16}z^2 + \mathcal{O}(z^4)                          \\
  r''                     & = z (2a_1 + a_0 (3 a_0 - 2)) + 2 z^3 a_0^3 = -\frac{3}{8}z + \mathcal{O}(z^3)                     \\
  \Rightarrow 2r' + z r'' & = 2 (1 - a_0) + (a_0 (a_0 - 2) + 2 a_1) z^2 + 2 a_0^3 z^4 = 1 - \frac{3}{4}z^2 + \mathcal{O}(z^3)
\end{align}
%
where $a_0 = \frac{r}{z}$ and $a_1 = \frac{1}{z^2} (a_0 - \frac{1}{2})$.
We see that for small-$z$, the original simplified expressions in \cref{eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-nu-nu-nu}
%
\begin{align}\label{eq:third-derivatives-small-z}
  \boxed{
    \begin{aligned}
      f_{xxx}       & = -\frac{2}{x^3} - \nu^3 r'' \\
      f_{xx\nu}     & = -\nu(2r' + z r'')          \\
      f_{x\nu\nu}   & = -x(2r' + z r'')            \\
      f_{\nu\nu\nu} & = -x^3 r''
    \end{aligned}
  }
\end{align}
%
are stable to compute, since neither $r''$ nor $2r' + z r''$ suffer cancellation issues.

\paragraph{Large-$z$}

For large $z$, the expression for $r''$ in \cref{eq:r-prime-prime-large-reparametrized} is stable.
The term $2r'+zr''$ is more delicate.
We derive a stable form from the identity $2r' + zr'' = r' + r/z - 2zrr'$ which follows from \cref{eq:r-prime-large-reparametrized-simplified,eq:r-prime-prime-large-reparametrized}.
Substituting $r=1-ub_0$ and $r'=u^2(b_1+b_0(1-b_0))$ gives
%
\begin{align}
  2r' + zr'' & = r' + r/z - 2zrr'                                              \\
             & = u^2 (b_1+b_0(1-b_0)) + u(1-ub_0) - 2u(1-ub_0)(b_1+b_0(1-b_0)) \\
             & = u ( u(b_1+b_0-b_0^2) + (1-ub_0)(1 - 2(b_1+b_0-b_0^2)) )
\end{align}
%
The term $1-2(b_1+b_0-b_0^2) = -\frac{1}{2}u + \mathcal{O}(u^2)$ is prone to cancellation issues which can be resolved by substituting $b_0=(1+ub_1)/2$ followed by $b_1=(1+ub_2)/4$:
%
\begin{align}
  1-2(b_1+b_0-b_0^2) = \frac{1-4b_1+u^2 b_1^2}{2} = -\frac{1}{2}u(b_2 - u b_1^2)
\end{align}
%
Substituting this back yields
%
\begin{align}
  2r' + zr'' & = u^2 \left( (b_1+b_0-b_0^2) + (1-ub_0)\frac{u b_1^2 - b_2}{2} \right)                                          \\
             & = \frac{1}{2} u^2 \left( (1-b_2) + u (b_2 (\frac{1}{2}+b_0 ) + b_1^2 ) - u^2 b_1^2 (\frac{1}{2} + b_0 ) \right)
\end{align}
%
but now, as anticipated in \cref{sec:bessel-ratio-large-z}, the term $1-b_2$ causes a cancellation issue since $b_2 = 1 + \mathcal{O}(u)$.
Therefore, we make the final change of variables $b_2 = 1 + u b_3$, giving the numerically stable expression:
%
\begin{align}
  2r' + zr'' & = \frac{1}{2} u^2 \left( -u b_3 + u (b_2 (\frac{1}{2}+b_0 ) + b_1^2 \right) - u^2 b_1^2 ( \frac{1}{2} + b_0 )                                     \\
             & = \frac{1}{2} u^3 \left( b_2 (\frac{1}{2}+b_0 ) + b_1^2 - b_3 - u b_1^2 (\frac{1}{2} + b_0) \right). \label{eq:two-rprime-z-r-primeprime-large-z}
\end{align}
%
This expression is $\mathcal{O}(u^3)$ with no remaining cancellation issues.
All terms are computed from $b_0, b_1, b_2$, and $b_3$, which depend on a single minimax approximation for $b_3(u)$.

With these stable forms, the third derivatives for large $z$ are computed as:
\begin{align}\label{eq:third-derivatives-large-z}
  \boxed{
    \begin{aligned}
      f_{xxx}       & = -\frac{2}{x^3} - \nu^3 r''                                                                      \\
      f_{xx\nu}     & = -\frac{1}{2 x z^2} \left( b_1^2 - b_3 + (b_2 - \frac{1}{z} b_1^2) (\frac{1}{2} + b_0) \right)   \\
      f_{x\nu\nu}   & = -\frac{1}{2 \nu z^2} \left( b_1^2 - b_3 + (b_2 - \frac{1}{z} b_1^2) (\frac{1}{2} + b_0) \right) \\
      f_{\nu\nu\nu} & = -x^3 r''
    \end{aligned}
  }
\end{align}

\subsection{The Quantized Rician distribution}

In magnetic resonance imaging (MRI), the magnitude of the signal in the presence of noise is often modeled with a Gaussian distribution for simplicity.
However, a more physically accurate model is the Rician distribution, which correctly accounts for the non-negativity of magnitude data.
Furthermore, MRI data is invariably quantized during acquisition, meaning the continuous signal is stored as discrete integer values.
A truly faithful statistical model must therefore account for this quantization step.
This leads us to the Quantized Rician distribution, which describes the probability of observing a signal within a specific quantization bin.

Given the Rician probability density function $p(y \mid \nu, \sigma)$ from \cref{eq:rician-pdf}, the probability of an observation $y$ falling into a bin $[x, x + \delta)$ is given by the integral of the Rician density over the interval:
%
\begin{align}\label{eq:quantized-rician-pmf}
  p(x \mid \nu, \sigma, \delta) = \int_x^{x + \delta} p(y \mid \nu, \sigma) dy.
\end{align}
%
This defines the probability mass function for the Quantized Rician distribution, which we denote $\mathrm{QRice}(\nu, \sigma, \delta)$.

\subsubsection{Numerical evaluation strategy}

We are interested in performing statistical inference procedures such as maximum likelihood estimation (MLE), maximum a posteriori (MAP) estimation, and Markov-chain Monte Carlo (MCMC) sampling under a Quantized Rician likelihood model.
These methods require many evaluations of the likelihood and its gradient, necessitating fast and accurate evaluation of the defining integral in \cref{eq:quantized-rician-pmf}.
While adaptive quadrature routines can compute the integral to arbitrary precision, they are generally too slow for this context, especially for GPU-based implementations.
Our goal is to use a fixed-order, non-adaptive quadrature scheme which is significantly faster.

The suitability of such a scheme depends on the integration regime.
Integrating an exponentially decaying function over a large interval in its tails, for example, can be numerically catastrophic for a fixed-order rule.
Fortunately, the nature of statistical inference itself ensures that we predominantly operate in a numerically favourable regime when the noise scale $\sigma$ is a nuisance parameter which is to be estimated jointly with other model parameters.
Crucially, the inference algorithms operate in high-likelihood regions:
%
\begin{itemize}
  \item MLE and MAP explicitly drive the parameters $(\nu,\sigma)$ toward the mode of the likelihood or posterior.
  \item MCMC methods spend the most time sampling from regions of high posterior density, visiting the tails only rarely.
\end{itemize}
%
This behavior is made concrete by considering the high signal-to-noise ratio (SNR) regime, where the Rician distribution is well-approximated by a Gaussian.
For a Gaussian model, a property of the MLE is that the noise variance estimate equals the mean squared error:
%
\begin{align}
  \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \nu_i)^2.
\end{align}
%
This implies that for a well-fit model the residuals satisfy $|x_i - \nu_i| = \mathcal{O}(\sigma)$, i.e., the normalized residual is on the order of one, $|x' - \nu'| = \mathcal{O}(1)$.
Thus, inference naturally results in evaluation of the density near the mode.
Pathological cases where $|x'-\nu'| \gg 1$ correspond to regions of low likelihood that are comparatively less important;
coarse estimates suffice there.

This operating regime is further constrained by practical considerations from signal acquisition.
The signal-to-quantization-noise ratio (SQNR) is engineered to exceed the physical SNR, ensuring quantization error is a smaller source of variance than the physical noise of the signal.
Empirically, the ratio of maximum signal magnitude to quantization bin width is large ($\sim\!500\text{--}1000$), and with 16-bit signal magnitude quantization (Int16) the dynamic range allows a theoretical upper bound of about $2^{15} \approx 30000$.
Consequently, the signal-to-bin-width ratio typically exceeds the SNR, i.e. $x/\delta \gtrsim x/\sigma$, which implies that the normalized integration width satisfies $\delta' = \delta/\sigma \lesssim 1$.
Thus, we have reasonable justification for the following two assumptions about the model parameters:
%
\begin{enumerate}
  \item The observed magnitude $x$ is close to the noncentrality parameter $\nu$, i.e., $|x - \nu| \lesssim \sigma \Rightarrow |x' - \nu'| \lesssim 1$.
  \item The quantization width $\delta$ is not larger than the noise level $\sigma$, i.e. $\delta' = \delta / \sigma \lesssim 1$.
\end{enumerate}
%
Combined, these factors create a best-case scenario for numerical integration:
we are integrating a smooth, well-behaved density over a short interval located near its mode.
This justifies the use of a fixed-order quadrature rule.
As a safeguard, on the rare occasions where a poor parameter choice leads to evaluating the likelihood in the tails (i.e., $|x'-\nu'| \gg 1$), one could temporarily increase the quadrature order or fall back to an adaptive routine;
under normal inference, these cases are exceptional.

\subsubsection{Gauss--Legendre quadrature}

We choose Gauss--Legendre quadrature to approximate the likelihood integral.
On the canonical interval $[-1,1]$, the $N$-point Gauss--Legendre rule approximates
%
\begin{align}
  \int_{-1}^{1} g(x) \, dx \approx \sum_{i=1}^{N} w_i g(x_i),
\end{align}
%
where $\{x_i\}_{i=1}^N$ are the Legendre nodes, defined as the roots of the $N^\text{th}$ Legendre polynomial $P_N(x)$, and $\{w_i\}_{i=1}^N$ the associated positive weights.
This rule is extremely powerful despite its simplicity:
it is exact for all polynomials of degree up to $2N-1$, and for analytic functions the approximation error decreases exponentially fast as $N$ increases.
The rule's fixed nodes and weights are easy to precompute, and the positivity of the weights ensures that no additional cancellation issues are introduced.
Furthermore, the weighted sum structure is trivial to differentiate, making it straightforward to compute gradients and Hessians of the integral approximation.

To apply the rule on $[0,1]$, use the affine change of variables $t = (x+1)/2$, transforming the nodes and weights to $t_i = (x_i+1)/2$ and $w_i/2$, respectively:
%
\begin{align}
  \int_{0}^{1} g(t) \, dt \approx \sum_{i=1}^{N} \frac{w_i}{2} g(t_i).
\end{align}
%
To integrate over $[x, x + \delta)$, change variables to $y = x + \delta t$ with $t\in[0,1]$ and $dy = \delta dt$:
%
\begin{align}
  \int_{x}^{x + \delta} g(y) \, dy = \int_{0}^{1} g(x + \delta t) \, \delta dt
  \approx \delta \sum_{i=1}^{N} \frac{w_i}{2} g(x + \delta t_i).
\end{align}
%
In our regime, $\delta$ is small and $g$ is smooth, so moderate orders $N \approx 8\text{--}16$ achieve high accuracy.

\subsubsection{Log-likelihood formulation and simplification}

Before deriving expressions for the gradient and Hessian, we first simplify the problem by exploiting the scale-invariance of the Quantized Rician distribution.
If we change variables in the integrand to $y' = y / a$ for some $a > 0$ and similarly rescale $x$, $\nu$, $\sigma$, and $\delta$, the Rician PDF transforms as
%
\begin{align}
  p(a y' \mid a \nu', a \sigma')
  = \frac{a y'}{(a \sigma')^2} \exp\left(-\frac{(a y')^2+(a \nu')^2}{2(a \sigma')^2}\right) I_0\left(\frac{a y' (a \nu')}{(a \sigma')^2}\right)
  = \frac{1}{a} p(y' \mid \nu', \sigma').
\end{align}
%
Thus, the quantized likelihood integral is scale-invariant:
%
\begin{align}
  p(x \mid \nu, \sigma, \delta)
  = \int_{x/a}^{(x + \delta)/a} p(a y' \mid a\nu', a\sigma') (a dy') = \int_{x'}^{x'+\delta'} p(y' \mid \nu', \sigma') dy'
  = p(x' \mid \nu', \sigma', \delta').
\end{align}
%
Similar to the Rician distribution analysis, we choose $a = \sigma$ such that $\sigma'=1$;
for the remainder of this analysis, we will work with these normalized parameters and write $p(x \mid \nu, \delta)$ as shorthand for $p(x \mid \nu, \sigma=1, \delta)$, dropping the prime annotations.

In terms of $f$, the negative log-likelihood of the standard Rician distribution from \cref{eq:rician-neg-log-likelihood}, the Quantized Rician likelihood is given by
%
\begin{align}
  p(x \mid \nu, \delta) = \int_x^{x + \delta} \exp(-f(y, \nu)) dy \coloneqq I(\theta)
\end{align}
%
where we have introduced a shorthand notation for the normalized parameters $\theta = (x, \nu, \delta)$ and the integral $I(\theta)$.
Similarly, we introduce $\Omega(\theta) = -\log I(\theta)$ for the negative logarithm of the quantized likelihood:
%
\begin{align}
  -\log p(x \mid \nu, \delta) = -\log I(\theta) \coloneqq \Omega(\theta).
\end{align}
%
We now make the affine change of variables $y = x + \delta t$ such that the integration bounds are independent of $\theta$, resulting in an integral over the unit interval $t \in [0, 1]$:
%
\begin{align}
  I(\theta)                  & = \int_0^1 \exp(-f(x + \delta t, \nu)) \, \delta \, dt                       \\
  \Rightarrow \Omega(\theta) & = -\log\left(\int_0^1 \exp(-f(x + \delta t, \nu)) \, dt\right) - \log\delta.
\end{align}
%
This expression has the form of a log-partition function;
denote the energy function as $\tilde{f}(t, \theta) = f(x + \delta t, \nu)$ and the partition function as
%
\begin{align}
  Z(\theta) = \int_0^1 \exp(-\tilde{f}(t, \theta)) \, dt.
\end{align}
%
Then, $\Omega(\theta) = -\log Z(\theta) - \log\delta$, and we can define a probability density over $t \in [0,1]$ as
%
\begin{align}
  P(t \mid \theta) = \frac{\exp(-\tilde{f}(t, \theta))}{Z(\theta)}.
\end{align}
%
This formulation will prove useful for computing gradients and higher derivatives of $\Omega$.

\subsubsection{Numerical differentiation strategy}

We differentiate the fixed-node Gauss--Legendre approximation to $Z(\theta)$, and hence to $\Omega(\theta)$, rather than applying the Leibniz integral rule to the true integral.
This yields a coherent discretization: the gradients and higher derivatives are exact for the discretized integral.
In optimization, mixing a discretized integral objective with derivatives of the exact integral can produce minima with nonzero gradient and unreliable Hessian curvature tests.
For gradient-based MCMC, proposals use gradients while acceptance depends on likelihood ratios; inconsistency between likelihoods and gradients can undermine both correctness and efficiency.
Furthermore, exact endpoint formulas require differences such as $e^{-f(x+\delta,\nu)} - e^{-f(x,\nu)}$ and $f_x(x+\delta,\nu) - f_x(x,\nu)$, which catastrophically cancel when $\delta \ll 1$ and require careful handling at $x=0$ where $f$ has a logarithmic singularity; fixed-node Gauss--Legendre uses interior nodes and avoids this issue.
Differentiating the quadrature therefore provides stable averages of well-conditioned integrand derivatives, resulting in consistent $\Omega$ values and derivatives.

\subsubsection{The Residual Likelihood Method}

Since the quadrature scheme approximates the partition function $Z(\theta)$ as a weighted sum of positive terms, $\log Z(\theta)$ can be stably computed using the log-sum-exp trick:
%
\begin{align}
  -\log Z(\theta)
  = -\log\left(\sum_i \frac{w_i}{2} \exp(-\tilde{f}(t_i, \theta))\right)
  = \tilde{f}_{\min}(\theta) - \log\left(\sum_i \frac{w_i}{2} \exp(\tilde{f}_{\min}(\theta) - \tilde{f}(t_i, \theta))\right)
\end{align}
%
where $\tilde{f}_{\min}(\theta) = \min_i \tilde{f}(t_i, \theta)$, avoiding naive overflow.
However, a more subtle numerical issue arises at high SNR ($x \approx \nu$).
In this regime, the Rician negative log-likelihood $f$ is very nearly Gaussian:
%
\begin{align}
  f(x, \nu) \approx f_G(x,\nu) = \frac{(x-\nu)^2}{2} + \frac{1}{2}\log(2\pi).
\end{align}
%
In other words, the residual energy $\tilde{r}(t,\theta) = \tilde{f}(t,\theta) - f_G(x,\nu) = f(x + \delta t, \nu) - f_G(x,\nu)$ is relatively small.
Since both integration and Gauss--Legendre quadrature are linear functionals, we can factor $\exp(-f_G(x,\nu))$ out of the $Z(\theta)$ integral, resulting in a better-conditioned quadrature problem via the residual energy $\tilde{r}$:
%
\begin{align}
  -\log Z(\theta)
  = - \log\left(\int_0^1 \exp(-(\tilde{r}(t,\theta) + f_G(x,\nu))) \, dt \right)
  = f_G(x,\nu) - \log\left(\int_0^1 \exp(-\tilde{r}(t,\theta)) \, dt \right). \label{eq:logz-residual-derivation}
\end{align}
%
This transformation is most beneficial at large $z$ but -- as we will see -- introduces no numerical issues at small $z$, so we apply it universally for simplicity.

\subsubsection{Properties and Derivatives of the Residual Energy}\label{sec:residual-energy-properties-and-derivatives}

\paragraph{Properties of the residual energy}

Since $f_G$ is quadratic in $x$ and $\nu$ and independent of $\delta$, it follows from $\tilde{r} = \tilde{f} - f_G$ that third and higher-order partial derivatives of $\tilde{r}$ with respect to $x$ and $\nu$ are equal to those of $\tilde{f}$.
Similarly, any partial derivative of $\tilde{r}$ with respect to $\delta$ equals that of $\tilde{f}$.
We therefore need only derive stable expressions for $\tilde{r}$ and its first and second partial derivatives with respect to $x$ and $\nu$.
From \cref{eq:rician-neg-log-likelihood-scaled}, we have:
%
\begin{align}
  \tilde{r}(t,\theta) = f(y, \nu) - f_G(x,\nu) & = \frac{(y-\nu)^2 - (x-\nu)^2}{2} - \frac{1}{2}\log\left(\frac{y}{\nu}\right) - \log \hat{I}_0(y \nu)                                               \\
                                               & = \delta t \left(x - \nu + \frac{\delta t}{2}\right) - \frac{1}{2}\log\left(\frac{y}{\nu}\right) - \log \hat{I}_0(y \nu) \label{eq:residual-energy}
\end{align}
%
where $y = x + \delta t$.
The first term in \cref{eq:residual-energy} is small when $|x-\nu| \lesssim 1$ and $\delta \lesssim 1$.
The size of the second and third terms $\tilde{c}(y,\nu) = \frac{1}{2}\log(y/\nu) + \log\hat{I}_0(y\nu)$ depends on the regime of $z=y\nu$.

\paragraph{Small $z=y\nu$}

Rewriting $\tilde{c}(y,\nu) = \log y + \frac{1}{2}\log(2\pi) + \log I_0(z) - z$ and Taylor expanding $\log I_0(z)$, we have
%
\begin{align}\label{eq:c-small-z}
  \tilde{c}(y,\nu) = \log y + \frac{1}{2}\log(2\pi) - z + \frac{z^2}{4} + \mathcal{O}(z^4).
\end{align}

\paragraph{Large $z=y\nu$}

Using the asymptotic expansion $\log \hat{I}_0(z) = \frac{1}{8z} + \mathcal{O}(z^{-2})$, we have
%
\begin{align}\label{eq:c-large-z}
  \tilde{c}(y,\nu) = \frac{1}{2}\log\left(\frac{y}{\nu}\right) + \frac{1}{8z} + \mathcal{O}(z^{-2}).
\end{align}

\paragraph{High-SNR $\nu \approx y \gg 1$}

Let $y = x + \delta t = \nu + \epsilon$ with $|\epsilon| = |x - \nu + \delta t| \lesssim 1$ and $\nu$ large.
Then, expanding \cref{eq:c-large-z} to first order in $\epsilon/\nu \ll 1$ gives:
%
\begin{align}
  \tilde{c}(\nu+\epsilon, \nu) & = \frac{1}{2}\log\left(1 + \frac{\epsilon}{\nu}\right) + \frac{1}{8\nu(\nu+\epsilon)} + \mathcal{O}(z^{-2})                                                                                                   \\
                               & = \frac{\epsilon}{2\nu} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right) + \frac{1}{8\nu^2}\left(1 - \frac{\epsilon}{\nu} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right)\right) + \mathcal{O}(\nu^{-4}) \\
                               & = \frac{\epsilon}{2\nu}\left(1 - \frac{1}{4\nu^2}\right) + \frac{1}{8\nu^2} + \mathcal{O}\left(\frac{\epsilon^2}{\nu^2}\right).
\end{align}
%
Thus, $\tilde{c}$ is small in the high-SNR regime and therefore $\tilde{r} = \delta t (\epsilon - \frac{\delta t}{2}) - \tilde{c}$ is also small when $\delta \lesssim 1$.

\paragraph{First derivatives of the residual}\label{sec:first-derivatives-residual}

The first partial derivatives of $\tilde{r}$ can be simplified using \cref{eq:first-derivatives-simplified-x,eq:first-derivatives-simplified-nu}:
%
\begin{align}
  \tilde{r}_x   & = f_x(y,\nu) - (x-\nu) = (y - \nu r - \frac{1}{y}) - (x-\nu) = \delta t + \nu (1 - r) - \frac{1}{y} \\
  \tilde{r}_\nu & = f_\nu(y,\nu) - (\nu-x) = (\nu - y r) - (\nu-x) = -\delta t r + x (1 - r)
\end{align}
%
These forms are stable for small $z = y \nu$, but for large $z$, we substitute $1-r = \frac{b_0}{z}$ from \cref{eq:r-large-reparametrized}:
%
\begin{align}
  \tilde{r}_x   & = \delta t + \nu (1 - r) - \frac{1}{y} = \delta t - \frac{1 - b_0}{y}           \\
  \tilde{r}_\nu & = -\delta t r + x (1 - r) = -\delta t + y (1 - r) = -\delta t + \frac{b_0}{\nu}
\end{align}

\paragraph{Second derivatives of the residual}\label{sec:second-derivatives-residual}

The second partial derivatives of $\tilde{r}$ can be simplified using \cref{eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu}:
%
\begin{align}
  \tilde{r}_{xx}     & = f_{xx} - 1 = \frac{1}{y^2} - \nu^2 r'          \\
  \tilde{r}_{x\nu}   & = f_{x\nu} + 1 = 1 - (r + z r') = 1 - z(1 - r^2) \\
  \tilde{r}_{\nu\nu} & = f_{\nu\nu} - 1 = - y^2 r'
\end{align}
%
Similar to the small- and large $z$ expressions in \cref{eq:second-derivatives-small-z,eq:second-derivatives-large-z}, the above expressions for $\tilde{r}_{xx}$ and $\tilde{r}_{\nu\nu}$ are numerically stable.
However, for large $z$, the mixed-derivative $\tilde{r}_{x\nu}$ requires some care, since $f_{x\nu} = -1 + \mathcal{O}(\frac{1}{z^2})$ and so computing $\tilde{r}_{x\nu} = \mathcal{O}(\frac{1}{z^2})$ naively leads to catastrophic cancellation.
A stable form can be derived by substituting the large-$z$ reparametrizations $1 - r = u b_0$, $2 b_0 - 1 = u b_1$, and $4 b_1 - 1 = u b_2$ where $u = \frac{1}{z}$:
%
\begin{align}
  1 - r - z r' & = 1 - z(1 - r^2) = 1 - z(1 - r)(1 + r)                                                                                            \\
               & = 1 - b_0 (1 + r) = 1 - b_0(2 - u b_0) = (1 - 2 b_0) + u b_0^2 = -u b_1 + u b_0^2                                                 \\
               & = u (b_0^2 - b_1) = u ((\frac{1 + u b_1}{2})^2 - b_1) = \frac{u}{4} (1 - 4b_1 + 2u b_1 + u^2 b_1^2)                               \\
               & = \frac{u}{4}(-u b_2 + 2u b_1 + u^2 b_1^2) = \frac{u^2}{4}(2 b_1 - b_2 + u b_1^2). \label{eq:one-minus-r-minus-z-r-prime-large-z}
\end{align}
%
This final expression is stable to compute and $\mathcal{O}(\frac{1}{z^2})$ as expected.

\subsubsection{Expectations and derivatives under $P(t \mid \theta)$}

\paragraph{Expectations over $P(t \mid \theta)$}\label{sec:expectation-covariance}

Since $f_G(x,\nu)$ is constant with respect to $t$, we can factor $\exp(-f_G(x,\nu))$ out of the numerator and denominator of $P(t \mid \theta)$, resulting in the more numerically stable form
%
\begin{align}
  P(t \mid \theta)
  = \frac{\exp(-\tilde{f}(t, \theta))}{\int_0^1 \exp(-\tilde{f}(t', \theta)) \, dt'}
  = \frac{\exp(-\tilde{r}(t, \theta))}{\int_0^1 \exp(-\tilde{r}(t', \theta)) \, dt'}.
\end{align}
%
In the high SNR regime with $\delta$ small such that $\tilde{r}$ is small, we can compute $\log P$ in a more stable form:
%
\begin{align}
  \log P(t \mid \theta)
   & = -\log \left( \int_0^1 \exp(\tilde{r}(t, \theta) - \tilde{r}(t', \theta)) \, dt' \right)        \\
   & = -\log \left(1 + \int_0^1 \exp(\tilde{r}(t, \theta) - \tilde{r}(t', \theta)) - 1 \, dt' \right)
\end{align}
%
where $\log(1 + \epsilon)$ and $\exp(\epsilon - 1)$ are computed stably for small $\epsilon$ using the special-function routines \texttt{log1p($\epsilon$)} and \texttt{expm1($\epsilon$)}.

\paragraph{Differentiating expectations over $P(t \mid \theta)$}\label{sec:jacobian-expectation-covariance}

Let $g : [0, 1] \times \mathbb{R}^3 \mapsto \mathbb{R}^m$, and denote as $\mathbb{E}[g] \in \mathbb{R}^m$ the expectation of $g(t, \theta)$ with respect to the density $P(t \mid \theta)$.
The Jacobian $\nabla_\theta \mathbb{E}[g] \in \mathbb{R}^{m \times 3}$ is given by
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{f}) \label{eq:gradient-expectation-covariance}          \\
                              & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{r}) \label{eq:gradient-expectation-covariance-residual}
\end{align}
%
where, denoting $\mu_X = \mathbb{E}[X]$ and $\mu_Y = \mathbb{E}[Y]$, the covariance is defined as usual as
%
\begin{align}
  \mathrm{Cov}(X, Y) = \mathbb{E}[(X - \mu_X)(Y - \mu_Y)^T] = \mathbb{E}[X Y^T] - \mu_X \mu_Y^T.
\end{align}
%
\Cref{eq:gradient-expectation-covariance,eq:gradient-expectation-covariance-residual} follow from differentiating the definition of the expectation $\mathbb{E}[g] = \int_0^1 g(t, \theta) P(t \mid \theta) \, dt$;
a complete proof is provided in \Cref{app:proof-gradient-expectation}.
Crucially, \cref{eq:gradient-expectation-covariance,eq:gradient-expectation-covariance-residual} also hold exactly when the expectation is computed via a fixed-node quadrature, $\mathbb{E}[g] \approx \sum_i \frac{w_i}{2} g(t_i, \theta) P(t_i \mid \theta)$, since both integration and quadrature are linear functionals of the integrand $t \mapsto g(t,\theta) P(t \mid \theta)$ and thus commute with differentiation.

\paragraph{Differentiating covariances with respect to $P(t \mid \theta)$}\label{sec:derivative-covariance}

For any two scalar functions $g(t,\theta)$ and $h(t,\theta)$, the derivative of their covariance with respect to a parameter $\gamma \in \{x, \nu, \delta\}$ is given by
%
\begin{align}\label{eq:derivative-covariance-identity}
  \partial_{\gamma} \mathrm{Cov}(g, h) = \mathrm{Cov}(\partial_{\gamma} g, h) + \mathrm{Cov}(g, \partial_{\gamma} h) - \mathrm{Cov3}(g, h, \partial_{\gamma} \tilde{r}),
\end{align}
%
where $\mathrm{Cov3}(X,Y,Z) = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])(Z-\mathbb{E}[Z])]$ is the third joint central moment;
a complete proof is provided in \Cref{app:proof-covariance-derivative}.

\subsubsection{First Derivatives}\label{sec:qrician-first-derivatives}

We compute the gradient of $\Omega = -\log Z - \log\delta$ in terms of the gradient of $-\log Z$:
%
\begin{align}
  \nabla_{\theta} (-\log Z) = -\frac{\nabla_{\theta} Z}{Z} = -\frac{1}{Z}\int_0^1 -\nabla_{\theta} \tilde{f} \, \exp(-\tilde{f}) \, dt = \mathbb{E}[\nabla_{\theta} \tilde{f}].
\end{align}
%
Rewriting the expectation in terms of the residual energy $\tilde{r}$, we have
%
\begin{align}
  \mathbb{E}[\nabla_{\theta} \tilde{f}] = \mathbb{E}[\nabla_{\theta} (\tilde{r} + f_G)] = \mathbb{E}[\nabla_{\theta} \tilde{r}] + \nabla_{\theta} f_G
\end{align}
%
since $f_G$ is constant with respect to $t$.
Finally, the gradient $\nabla_{\theta} \Omega$ simplifies to
%
\begin{align}
  \nabla_{\theta} \Omega & = - \nabla_\theta \log Z - \nabla_\theta \log\delta = \mathbb{E}[\nabla_{\theta} \tilde{f}] - (0, 0, \delta^{-1})^T \\
                         & = \mathbb{E}\left[ \begin{pmatrix} \tilde{r}_x \\ \tilde{r}_\nu \\ t \tilde{f}_x \end{pmatrix} \right]
  + \begin{pmatrix} x - \nu \\ \nu - x \\ -\delta^{-1} \end{pmatrix}
\end{align}
%
where $\frac{\partial}{\partial x} f_G = x - \nu$, $\frac{\partial}{\partial \nu} f_G = \nu - x$, and $\tilde{r}_x$, $\tilde{r}_\nu$, and $f_x$ are evaluated at $y=x + \delta t$ as in \cref{sec:first-derivatives-residual}.

\subsubsection{Second Derivatives}\label{sec:qrician-second-derivatives}

We can compute the Hessian of $\Omega = -\log Z - \log\delta$ by differentiating $\nabla_{\theta} (-\log Z)$ from \cref{sec:qrician-first-derivatives} using the gradient of the expectation identity \cref{eq:gradient-expectation-covariance}:
%
\begin{align}
  \nabla_{\theta}^2 (-\log Z) = \nabla_\theta ( \mathbb{E}[\nabla_{\theta} \tilde{f}] ) = \mathbb{E}[\nabla_{\theta}^2 \tilde{f}] - \mathrm{Cov}(\nabla_{\theta} \tilde{f}, \nabla_{\theta} \tilde{f})
\end{align}
%
Substituting $\tilde{f} = \tilde{r} + f_G$ and recalling that $f_G$ is constant with respect to $t$, we have
%
\begin{align}
  \nabla_{\theta}^2 (-\log Z)                                                     & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r} + \nabla_{\theta}^2 f_G] - \mathrm{Cov}(\nabla_{\theta} \tilde{r} + \nabla_{\theta} f_G, \nabla_{\theta} \tilde{r} + \nabla_{\theta} f_G)  \\
                                                                                  & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r}] + \nabla_{\theta}^2 f_G - \mathrm{Cov}(\nabla_{\theta} \tilde{r}, \nabla_{\theta} \tilde{r}) \label{eq:residual-hessian-neglogz}          \\
  \Rightarrow \nabla_{\theta}^2 \Omega = \nabla_{\theta}^2 (-\log Z - \log\delta) & = \mathbb{E}[\nabla_{\theta}^2 \tilde{r}] - \mathrm{Cov}(\nabla_{\theta} \tilde{r}, \nabla_{\theta} \tilde{r}) + \begin{pmatrix} 1 & -1 & 0 \\ -1 & 1 & 0 \\ 0 & 0 & \delta^{-2} \end{pmatrix} \label{eq:residual-hessian-qrician}
\end{align}
%
where $\frac{\partial^2}{\partial x^2} f_G = \frac{\partial^2}{\partial \nu^2} f_G = 1$, $\frac{\partial^2}{\partial x \partial \nu} f_G = -1$.
The Hessian of the residual energy $\nabla_{\theta}^2 \tilde{r}$ is
%
\begin{align}
  \nabla_{\theta}^2 \tilde{r} = \begin{pmatrix} \tilde{r}_{xx} & \tilde{r}_{x\nu} & t \tilde{f}_{xx} \\ \tilde{r}_{x\nu} & \tilde{r}_{\nu\nu} & t \tilde{f}_{x\nu} \\ t \tilde{f}_{xx} & t \tilde{f}_{x\nu} & t^2 f_{xx} \end{pmatrix}
\end{align}
%
where $\tilde{r}_{xx}$, $\tilde{r}_{x\nu}$, and $\tilde{r}_{\nu\nu}$ are evaluated at $y=x + \delta t$ as in \cref{sec:second-derivatives-residual}.

\subsubsection{Third Derivatives and Vector-Jacobian Products}

Differentiating through statistical inference procedures, such as Laplace's approximation, requires gradients of scalar functions of the log-likelihood Hessian, $\mathcal{L}(\nabla_\theta^2 \Omega(\theta))$.
This necessitates third-order derivatives of $\Omega(\theta)$.
We derive the components of the 3-tensor $\nabla_\theta^3 \Omega(\theta)$ and the vector-Jacobian product (VJP) for the map $\theta \mapsto \nabla_\theta^2 \Omega(\theta)$ required for backpropagation.

\paragraph{Third derivatives of $\Omega$}

The components of the VJP are linear combinations of the third-order partial derivatives of $\Omega = -\log Z - \log\delta$.
We use the shorthand $\partial_i \coloneqq \frac{\partial}{\partial \theta_i}$ where $\theta = (x, \nu, \delta)^T$, and similarly $\partial_{ij} \coloneqq \frac{\partial^2}{\partial \theta_i \partial \theta_j}$ and $\partial_{ijk} \coloneqq \frac{\partial^3}{\partial \theta_i \partial \theta_j \partial \theta_k}$, and derive the third derivatives by differentiating the Hessian $\nabla_{\theta}^2 (-\log Z)$ from \cref{eq:residual-hessian-neglogz} with respect to a parameter $\theta_k$:
%
\begin{align}
  \partial_{k}(\nabla_{\theta}^2 (-\log Z)) = \partial_{k} \mathbb{E}[\nabla_\theta^2 \tilde{r}] - \partial_{k}\mathrm{Cov}(\nabla_\theta \tilde{r}, \nabla_\theta \tilde{r}).
\end{align}
%
The components of the third-derivative tensor follow by applying \cref{eq:gradient-expectation-covariance-residual,eq:derivative-covariance-identity} to differentiate the expectation and covariance terms:
%
\begin{align}
  \partial_{k} \mathbb{E}[\partial_{ij} \tilde{r}]                         & = \mathbb{E}[\partial_{ijk} \tilde{r}] - \mathrm{Cov}(\partial_{ij} \tilde{r}, \partial_{k} \tilde{r})                                                                                                                                                                                                                                                             \\
  \partial_{k}\mathrm{Cov}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}) & = \mathrm{Cov}(\partial_{ik} \tilde{r}, \partial_{j} \tilde{r}) + \mathrm{Cov}(\partial_{i} \tilde{r}, \partial_{jk} \tilde{r}) - \mathrm{Cov3}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}, \partial_{k} \tilde{r})                                                                                                                                            \\
  \Rightarrow \partial_{ijk} (-\log Z)                                     & = \mathbb{E}[\partial_{ijk} \tilde{r}] - \mathrm{Cov}(\partial_{ij} \tilde{r}, \partial_{k} \tilde{r}) - \mathrm{Cov}(\partial_{ik} \tilde{r}, \partial_{j} \tilde{r}) - \mathrm{Cov}(\partial_{jk} \tilde{r}, \partial_{i} \tilde{r}) + \mathrm{Cov3}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}, \partial_{k} \tilde{r}) \label{eq:neglogz-third-derivative} \\
  \Rightarrow \partial_{ijk} \Omega                                        & = \partial_{ijk} (-\log Z) - 2\delta^{-3} \mathbf{1}_{\{\theta_i=\theta_j=\theta_k=\delta\}}. \label{eq:omega-third-derivative-qrician}
\end{align}
%
We now discuss two strategies for the efficient evaluation of $\partial_{ijk}(-\log Z)$.

\paragraph{Raw-Moment Formulation}

We rewrite the covariances and third central moments in terms of raw moments.
Let $\mu_{i} = \mathbb{E}[\partial_{i} \tilde{r}]$, $\mu_{ij} = \mathbb{E}[\partial_{i} \tilde{r} \partial_{j} \tilde{r}]$, and $\mu_{ijk} = \mathbb{E}[\partial_{i} \tilde{r} \partial_{j} \tilde{r} \partial_{k} \tilde{r}]$.
Then,
%
\begin{align}
  \mathrm{Cov}(\partial_{ij} \tilde{r}, \partial_{k} \tilde{r})                         & = \mathbb{E}[\partial_{ij} \tilde{r} \partial_{k} \tilde{r}] - \mathbb{E}[\partial_{ij} \tilde{r}] \mu_{k}                                                                                       \\
  \mathrm{Cov3}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}, \partial_{k} \tilde{r}) & = \mu_{ijk} - \mu_{i} \mu_{jk} - \mu_{j} \mu_{ik} - \mu_{k} \mu_{ij} + 2 \mu_{i} \mu_{j} \mu_{k}                                                                                                 \\
  \Rightarrow \partial_{ijk} (-\log Z)                                                  & = \mathbb{E}[\partial_{ijk} \tilde{r}] + \mu_{ijk} - \mu_{i} \mu_{jk} - \mu_{j} \mu_{ik} - \mu_{k} \mu_{ij} + 2 \mu_{i} \mu_{j} \mu_{k} \label{eq:neglogz-third-derivative-raw-moment}           \\
                                                                                        & - \mathbb{E}[\partial_{ij} \tilde{r} \partial_{k} \tilde{r}] - \mathbb{E}[\partial_{ik} \tilde{r} \partial_{j} \tilde{r}] - \mathbb{E}[\partial_{jk} \tilde{r} \partial_{i} \tilde{r}] \nonumber \\
                                                                                        & + \mathbb{E}[\partial_{ij} \tilde{r}] \mu_{k} + \mathbb{E}[\partial_{ik} \tilde{r}] \mu_{j} + \mathbb{E}[\partial_{jk} \tilde{r}] \mu_{i}. \nonumber
\end{align}
%
For a three-parameter system, a minimal basis of raw moments requires integrating 25 unique quantities.
The derivatives $\partial_{ijk} (-\log Z)$ are then assembled from sums and products of these raw moments.

\paragraph{Centred-Moment Formulation}

Let the centred first derivatives be given by $c_{i} = \partial_{i} \tilde{r} - \mu_{i}$ where $\mu_{i} = \mathbb{E}[\partial_{i} \tilde{r}]$.
Then, $\partial_{ijk}(-\log Z)$ can be rewritten as an expectation:
%
\begin{align}
  \mathrm{Cov3}(\partial_{i} \tilde{r}, \partial_{j} \tilde{r}, \partial_{k} \tilde{r}) & = \mathbb{E}[c_{i} c_{j} c_{k}]                                                                                                                                                                           \\
  \mathrm{Cov}(\partial_{ij} \tilde{r}, \partial_{k} \tilde{r})                         & = \mathbb{E}[\partial_{ij} \tilde{r} (\partial_{k} \tilde{r} - \mathbb{E}[\partial_{k} \tilde{r}])] = \mathbb{E}[\partial_{ij} \tilde{r} c_{k}]                                                           \\
  \Rightarrow \partial_{ijk} (-\log Z)                                                  & = \mathbb{E} [ \partial_{ijk} \tilde{r} - \partial_{ij} \tilde{r} c_{k} - \partial_{ik} \tilde{r} c_{j} - \partial_{jk} \tilde{r} c_{i} + c_{i} c_{j} c_{k} ]. \label{eq:neglogz-two-pass-centred-moment}
\end{align}
%
After computing the 3 means, this requires integrating 6 unique second-order and 10 unique third-order derivatives.
While this method requires evaluating first-derivatives twice, the linearity of the expectation in \cref{eq:neglogz-two-pass-centred-moment} can make for more efficient vector-Jacobian products than \cref{eq:neglogz-third-derivative-raw-moment} since contraction with a cotangent $\Delta$ can move inside the expectation (i.e. $\Delta \bullet \mathbb{E}[X] = \mathbb{E}[\Delta \bullet X]$), resulting in fewer scalar integrands.

\paragraph{Vector-Jacobian product of $\Omega$}

The vector-Jacobian product (VJP) is the core operation in reverse-mode automatic differentiation.
We define the VJP in terms of the tensor contraction
%
\begin{align}\label{eq:general-bullet-contraction}
  (\Delta \bullet J)_{j_1, \ldots, j_q} \coloneqq \sum_{i_1, \ldots, i_p} \Delta_{i_1, \ldots, i_p} \, J_{i_1, \ldots, i_p, j_1, \ldots, j_q}
\end{align}
%
where $\Delta \in \mathbb{R}^{m_1 \times \cdots \times m_p}$ is a rank $p$ \textit{cotangent} tensor and $J \in \mathbb{R}^{m_1 \times \cdots \times m_p \times n_1 \times \cdots \times n_q}$ is a rank $p+q$ Jacobian tensor.
As a concrete example, for an $\mathbb{R}^n \to \mathbb{R}^m$ function with Jacobian $J \in \mathbb{R}^{m \times n}$ and cotangent $\Delta \in \mathbb{R}^m$,
%
\begin{align}
  (\Delta \bullet J)_j = \sum_{i=1}^m \Delta_i J_{i j} = (\Delta^{T} J)_j
\end{align}
%
which recovers the traditional VJP.
Now, let $\mathcal{L}$ be a scalar function depending on the Hessian $\nabla_\theta^2 \Omega$.
The pullback $\mathcal{B}$ for the map $\theta \mapsto \nabla_\theta^2 \Omega(\theta)$ transforms the symmetric cotangent matrix $\Delta$ with components $\Delta_{ij} = \frac{\partial \mathcal{L}}{\partial (\partial_{ij} \Omega)}$ to the input cotangent vector $\frac{\partial \mathcal{L}}{\partial \theta}$:
%
\begin{align}
  \mathcal{B}(\Delta)_k \coloneqq \frac{\partial \mathcal{L}}{\partial \theta_k} = \sum_{i,j} \frac{\partial \mathcal{L}}{\partial (\partial_{ij} \Omega)} \frac{\partial (\partial_{ij} \Omega)}{\partial \theta_k} = \sum_{i,j} \Delta_{ij} \partial_{ijk} \Omega = (\Delta \bullet \nabla_\theta^3 \Omega)_k. \label{eq:vjp-omega-hessian}
\end{align}
%
% We note that, by linearity, $(\Delta \bullet \nabla_\theta^3 \Omega)_k = \partial_k (\Delta \bullet \nabla_\theta^2 \Omega)$, which may be more computationally efficient in some cases.
Using the third-derivative results from \cref{eq:omega-third-derivative-qrician,eq:neglogz-two-pass-centred-moment}, the pullback \cref{eq:vjp-omega-hessian} evaluates to
%
\begin{align}
  \mathcal{B}(\Delta)_k = \mathbb{E} \bigg[ \sum_{i,j} \Delta_{ij} (\partial_{ijk} \tilde{r} - \partial_{ij} \tilde{r} c_k - \partial_{ik} \tilde{r} c_j - \partial_{jk} \tilde{r} c_i + c_i c_j c_k) \bigg] - 2 \delta^{-3} \Delta_{33} \mathbf{1}_{\{\theta_k=\delta\}}
\end{align}
%
where the sum over $i,j$ has been moved inside the expectation.

% Figure: Bessel ratio accuracy
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{bessel_ratio_accuracy_figure.pdf}
  \caption{Error envelopes (maxima over sliding windows) for naive vs.\ proposed evaluation of $r(z) = I_1(z)/I_0(z)$, its derivatives $r'(z)$ and $r''(z)$, $2 r'(z) + z r''(z)$, and $1 - r(z) - z r'(z)$ over 1001 points $z \in [10^{-6}, 10^{6}]$. Rows (top to bottom) show $e_{\mathrm{abs}} = |\hat{y} - y|$ and $e_{\mathrm{rel}} = |\hat{y} - y| / |y|$ for single precision, then for double precision. Horizontal reference lines indicate machine epsilon for single (dotted) and double precision (dashed).}
  \label{fig:bessel-accuracy}
\end{figure}

% Table: Bessel ratio accuracy
\begin{table}[t]
  \centering
  \input{figures/output/bessel_ratio_accuracy_table.tex}
  \caption{Maximum error $e_{\min} = \min(|\hat{y} - y|, |\hat{y} - y| / |y|)$ over $z \in [10^{-6}, 10^{6}]$ for $r(z)$, $r'(z)$, $r''(z)$, $2 r'(z) + z r''(z)$, and $1 - r(z) - z r'(z)$, comparing naive vs.\ proposed implementations in single and double precision.}
  \label{fig:bessel-table}
\end{table}

% Table: NLL accuracy table
\begin{table}[t]
  \centering
  \resizebox{\textwidth}{!}{\input{figures/output/nll_accuracy_table.tex}}
  \caption{Maximum error $e_{\min} = \min(|\hat{y} - y|, |\hat{y} - y| / |y|)$ over $(x, \nu)$ pairs with $(\log_{10} x, \log_{10} \nu) \in [-3, 3]^2$ for $f$ and its first through third derivatives. Columns ``Bessels.jl'' and ``SF.jl'' use AD-based baselines using Bessels.jl and SpecialFunctions.jl, respectively; ``Naive'' uses numerically unstable expressions equivalent to manual AD; ``Proposed'' uses the derived stable expressions.}
  \label{fig:nll-table}
\end{table}

% Figure: QRice accuracy vs order
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{qrice_accuracy.pdf}
  \caption{Error vs.\ Gauss--Legendre order $N$ for $\Omega$, $\nabla\Omega$, $\nabla^2\Omega$, and $\nabla^3\Omega$. For each precision and each $(\delta, N)$, we take the maximum $e_{\min} = \min(|\hat{y} - y|, |\hat{y} - y| / |y|)$ over 1024 samples $\nu$ with $\log_{10}\nu \in [-3, 3]$ and 10 samples $x \sim \mathrm{QRice}(\nu, 1, \delta)$ per $\nu$; for tensor quantities we report the maximum entrywise error.}
  \label{fig:qrice-accuracy}
\end{figure}

% Table: Rician performance
\begin{table}[h]
  \centering
  \caption{Runtime for $f$, $\nabla f$, $\nabla^2 f$, and $\nabla^3 f$, comparing the proposed implementation against AD-based baselines using Bessels.jl and SpecialFunctions.jl. CPU timings are measured at random $(x,\nu)$ with $(\log_{10} x, \log_{10} \nu) \in [-3, 3]^2$; GPU timings measure parallel evaluation over $1024 \times 1024$ grids of $(x,\nu)$ pairs.}
  \label{tab:rician-performance}
  \resizebox{\textwidth}{!}{\input{figures/output/rician_performance_table.tex}}
\end{table}

% Table: Quantized Rician performance
\begin{table}[h]
  \centering
  \caption{Runtime vs.\ Gauss--Legendre order $N$ for $\Omega$, $\nabla\Omega$, $\nabla^2\Omega$, and $\nabla^3\Omega$ using the proposed method. CPU timings are measured with $\delta = 1$ at random ``high-SNR'' samples with $\log_{10} \nu \in [-3, 3]$, $x \sim \mathrm{Rice}(\nu,1)$; GPU timings measure parallel evaluation over $1024 \times 1024 \times 1$ grids of $(x, \nu, \delta)$ triples.}
  \label{tab:qrician-performance}
  \input{figures/output/qrician_performance_table.tex}
\end{table}

\section{Results}

Numerical accuracy is evaluated against 500-bit extended-precision reference expressions computed with ArbNumerics.jl \citep{johansson_arb_2014}.
CPU measurements use an AMD Ryzen 9 3950X with 16 cores and 32 threads;
GPU measurements use an NVIDIA GeForce RTX 3080 with 10 GB of memory.
We report errors using the absolute error $e_{\mathrm{abs}} = |\hat{y} - y|$, the relative error $e_{\mathrm{rel}} = |\hat{y} - y| / |y|$, and the minimum error $e_{\min} = \min(e_{\mathrm{abs}}, e_{\mathrm{rel}})$.
Unless stated otherwise, figures and tables report maxima of $e_{\min}$ over the stated domains.

\paragraph{Bessel ratio and its derivatives}

\Cref{fig:bessel-accuracy} shows error envelopes computed as maxima over local sliding windows for $r(z) = I_1(z) / I_0(z)$, $r'(z)$, $r''(z)$, $2 r'(z) + z r''(z)$, and $1 - r(z) - z r'(z)$ over 1001 points $z \in [10^{-6}, 10^{6}]$.
The naive method evaluates the recurrences \cref{eq:r-prime-recurrence,eq:r-second-derivative-recurrence} directly.
The proposed method follows the small-$z$ formulations in \cref{sec:bessel-ratio-small-z}, computing $r$, $r'$, and $r''$ from the reparametrized quantity $a_1$ via \cref{eq:r-small-reparametrized,eq:r-prime-small-reparametrized,eq:r-prime-prime-small-reparametrized}, and the large-$z$ formulations in \cref{sec:bessel-ratio-large-z}, computing $r$, $r'$, $r''$, $2r'+zr''$, and $1-r-zr'$ from the reparametrized quantity $b_3$ via \cref{eq:r-large-reparametrized,eq:r-prime-large-reparametrized-simplified,eq:r-prime-prime-large-reparametrized,eq:two-rprime-z-r-primeprime-large-z,eq:one-minus-r-minus-z-r-prime-large-z}, and uses rational polynomial approximants in the mid-$z$ regime.
The proposed piecewise evaluation remains at or near machine precision in both single and double precision, whereas the naive recurrence degrades severely, especially for $r''$, $2r'+zr''$, and $1-r-zr'$.
\Cref{fig:bessel-table} confirms these trends using the $e_{\min}$ metric.

\paragraph{Rician negative log-likelihood and derivatives}

\Cref{fig:nll-table} reports the maximum of $e_{\min}$ over a Cartesian grid with $(\log_{10} x, \log_{10} \nu) \in [-3, 3]^2$ for $f$ and its first through third derivatives.
Baselines labeled ``Bessels.jl'' and ``SF.jl'' compute derivatives of the Rician log-likelihood using forward-mode automatic differentiation (AD) via ForwardDiff.jl, where AD is applied to expressions using $I_0$ implementations from Bessels.jl and SpecialFunctions.jl, respectively.
``Naive'' implements the unsimplified relations from the \Cref{sec:rician-log-likelihood-and-basic-simplifications}, namely \cref{eq:rician-neg-log-likelihood,eq:first-derivatives-unsimplified-x,eq:first-derivatives-unsimplified-nu,eq:second-derivatives-unsimplified-x-x,eq:second-derivatives-unsimplified-x-nu,eq:second-derivatives-unsimplified-nu-nu,eq:third-derivatives-unsimplified-x-x-x,eq:third-derivatives-unsimplified-x-x-nu,eq:third-derivatives-unsimplified-x-nu-nu,eq:third-derivatives-unsimplified-nu-nu-nu}.
``Proposed'' implements the stable forms from the \Cref{sec:further-rician-derivative-simplifications}, namely \cref{eq:first-derivatives-small-z,eq:first-derivatives-large-z,eq:second-derivatives-small-z,eq:second-derivatives-large-z,eq:third-derivatives-small-z,eq:third-derivatives-large-z}.
The proposed method achieves near-machine-precision accuracy for all quantities in both single and double precision.
The naive evaluation is numerically unstable; the AD-based baselines degrade as the derivative order increases, particularly in single precision.

\paragraph{Quantized Rician accuracy vs.\ quadrature order}

\Cref{fig:qrice-accuracy} presents the maximum error versus Gauss--Legendre order $N$ for $\Omega$, $\nabla\Omega$, $\nabla^2\Omega$, and $\nabla^3\Omega$.
For each precision and for each pair $(\delta, N)$, we take the maximum over 1024 samples $\nu$ with $\log_{10} \nu \in [-3, 3]$ and 10 samples $x \sim \mathrm{QRice}(\nu, 1, \delta)$ per $\nu$.
The error metric is $e_{\min}$, with the maximum entrywise error reported for vector and tensor quantities.
Errors decrease rapidly with $N$ but increase with $\delta$; for $\delta \le 1$, $N=6$ is sufficient to reach the minimum error for single precision and $N=10$ for double precision.

\paragraph{Performance}

\Cref{tab:rician-performance} summarizes per-evaluation runtime for $f$, $\nabla f$, $\nabla^2 f$, and $\nabla^3 f$.
The proposed implementation is consistently faster than AD-based baselines on both the CPU and GPU, with speedups increasing with derivative order.
\Cref{tab:qrician-performance} reports runtime versus $N$ for $\Omega$, $\nabla\Omega$, $\nabla^2\Omega$, and $\nabla^3\Omega$.
Runtime grows with $N$ and derivative order.

\section{Application: Differentiating Through Laplace's Approximation}

In Type-II maximum likelihood frameworks, hyperparameters are tuned by optimizing the model evidence \citep{mackay_bayesian_1992}.
Using the Laplace approximation (LA) of the evidence, this optimization requires backpropagating through the posterior's maximum a posteriori (MAP) estimate and its curvature;
while this naively introduces third-order derivative tensors, the process can be made practical via efficient adjoint methods \citep{margossian_hamiltonian_2020}.
LA applications include Bayesian inference in latent Gaussian models, Variational Laplace Autoencoders, and end-to-end learning of data-augmentation invariances in deep networks \citep{margossian_hamiltonian_2020, park_variational_2019, immer_invariance_2022}.
For large-scale deep learning, tractability requires either replacing the full Hessian with approximations like Kronecker-factored structures or the generalized Gauss-Newton matrix \citep{ritter_scalable_2018, immer_improving_2021}, or applying the LA only to a subset of parameters.
A special case of this latter approach, \textit{last-layer} LA, applies the LA to a network's final linear layer \citep{kristiadi_being_2020, daxberger_laplace_2021}.
This motivates our setting:
a small, two-dimensional LA over nuisance parameters is embedded within a larger variational posterior approximation.

\subsection{MRI Problem Formulation}

In a common MRI setting, magnitude data $x = (x_1, \ldots, x_N)^T$ arise by taking the magnitude of complex measurements corrupted by Gaussian noise with variance $\sigma^2$, yielding Rician-distributed signals.
These signals are then quantized;
for example, stored as 16-bit integers with a scale factor $\delta$ to convert the compressed format back to physical units.
Suppose the measured signal $x_i$ is modeled as $s \cdot \mu_i(\lambda)$ where $\mu(\lambda)$ is a normalized signal model, $\lambda$ are the physical parameters of interest, and $s$ is a scaling factor.
The simplest such model is mono-exponential $T_2$ relaxation wherein $\lambda$ is the transverse relaxation time $T_2$, $\mu_i(\lambda) = \exp(-t_i/T_2)$ is the attenuation at echo time $t_i$, and $s$ is the initial signal amplitude.
Assuming the measurements are independent, this family of models results in the factorized likelihood:
%
\begin{align}\label{eq:likelihood}
  p(x \mid \lambda, \eta, \delta) = \prod_{i=1}^N p_{\mathrm{QRice}}(x_i \mid \nu = s \mu_i(\lambda), \sigma, \delta).
\end{align}

We consider a Bayesian setting where the goal is to infer the physical parameters $\lambda$ while marginalizing out the nuisance parameters $\eta = (\log s, \log \sigma)^T$.
Following \cref{eq:likelihood}, we write the negative log-likelihood as
%
\begin{align}\label{eq:negative-log-likelihood}
  L(\lambda, \eta; x, \delta)              & \coloneqq -\log p(x \mid \lambda, \eta, \delta) = \sum_{i=1}^N \ell(\xi^{(i)}(\lambda), \eta)                                                                      \\
  \text{where} \quad \ell(\xi^{(i)}, \eta) & \coloneqq -\log p_{\mathrm{QRice}}(x_i \mid \nu = s \mu_i, \sigma, \delta) = \Omega\left(\frac{x_i}{\sigma}, \frac{s \mu_i}{\sigma}, \frac{\delta}{\sigma}\right), \\
  \xi^{(i)}(\lambda)                       & \coloneqq (x_i, \mu_i(\lambda), \delta),
\end{align}
%
and $\Omega$ is the negative log-likelihood function for the quantized Rician distribution.
The joint density is then
%
\begin{align}
  p(x, \lambda, \eta \mid \delta) & = p(x \mid \lambda, \eta, \delta) p(\lambda, \eta) = e^{-U(\lambda, \eta; x, \delta)} \\
  \quad \text{where} \quad
  U(\lambda, \eta; x, \delta)     & \coloneqq L(\lambda, \eta; x, \delta) - \log p(\lambda, \eta)
\end{align}
%
where we assume a factorized prior $p(\lambda, \eta) = p(\lambda) p(\eta)$.

\subsection{Hybrid Variational Inference with Laplace's Approximation}

Suppose a hybrid variational inference scheme is employed wherein the posterior $p(\lambda, \eta \mid x, \delta)$ is modeled by a factorized distribution $q(\lambda, \eta) = q(\lambda) q(\eta \mid \lambda)$.
Let $q(\lambda)$ be a parametric approximation to the marginal posterior $p(\lambda \mid x, \delta)$ from a chosen variational family, and $q(\eta \mid \lambda)$ be a Gaussian surrogate for the conditional posterior $p(\eta \mid \lambda, x, \delta)$ computed using Laplace's method:
%
\begin{align}
  p(\eta \mid \lambda, x, \delta) \approx q(\eta \mid \lambda) & = \mathcal{N}(\eta \mid \hat{\eta}(\xi(\lambda)), \hat{H}(\xi(\lambda))^{-1}) \\
  \text{with} \quad
  \hat{\eta}(\xi)                                              & = \arg\min_{\eta} U(\xi, \eta) \label{eq:laplace-mode}                        \\
  \hat{H}(\xi)                                                 & = H(\xi, \hat{\eta}(\xi)) \label{eq:laplace-hessian}
\end{align}
%
where $\xi(\lambda) = (\mu(\lambda), x, \delta)$ groups all variables that the conditional posterior $p(\eta \mid \cdot)$ depends on, $\hat{\eta}(\xi)$ is the mode of $U(\xi, \cdot)$, and $H(\xi, \eta) \coloneqq \frac{\partial^2 U}{\partial \eta^2}$ is the Hessian of $U(\xi, \cdot)$.

The parameters $\psi$ of the variational family for $q(\lambda)$ are tuned by optimizing an objective $\mathcal{L}$ which depends on the Laplace approximation's mode $\hat{\eta}(\xi(\lambda))$ and Hessian $\hat{H}(\xi(\lambda))$.
For instance, if $\mathcal{L} = \mathbb{E}_{q(\lambda, \eta)} \left[\log \frac{p(x, \lambda, \eta \mid \delta)}{q(\lambda, \eta)}\right]$ is the evidence lower bound, the gradient $\nabla_\psi$ flows through $\hat{\eta}$ and $\hat{H}$ via two mechanisms:
1) the entropy term $-\log q(\lambda, \eta)$ has a direct contribution from $-\log q(\eta \mid \lambda) = \tfrac{1}{2}\log\det(2\pi\hat{H}^{-1}) + \tfrac{1}{2} \lVert \hat{H}^{1/2} (\eta - \hat{\eta}) \rVert^2$;
and 2) if $q(\lambda)$ is reparameterizable (i.e. $\lambda \sim q(\lambda) \Leftrightarrow \lambda = g(\psi, \epsilon)$ where $\epsilon \sim p(\epsilon)$ with $p(\epsilon)$ fixed), then unbiased gradients of expectations $\mathbb{E}_{q(\lambda, \eta)}[f(\lambda, \eta)]$ follow from the reparameterization trick:
$\nabla_\psi \mathbb{E}_{q(\lambda, \eta)}[f(\lambda, \eta)] = \mathbb{E}_{\epsilon \sim p(\epsilon), z \sim \mathcal{N}(0, I)}[\nabla_\psi f(g(\psi, \epsilon), \hat{\eta} + \hat{H}^{-1/2} z)]$.
Therefore, backpropagating through $\mathcal{L}$ requires the implementation of \textit{pullback} functions to pass gradients through \cref{eq:laplace-mode,eq:laplace-hessian}.

\subsection{Pullbacks for the Laplace Approximation}

Since $\hat{H}(\xi) = H(\xi, \hat{\eta}(\xi))$, we need only provide pullbacks for the implicit map $\xi \mapsto \hat{\eta}$ and the explicit map $(\xi, \eta) \mapsto H$;
the pullback for $\hat{H}$ is constructed by the automatic differentiation engine by composing the pullbacks for these two maps.
The pullback for $\xi \mapsto \hat{\eta}$ is derived from the optimality condition $\nabla_\eta U(\xi, \hat{\eta}(\xi)) = 0$.
Applying the implicit function theorem at $\eta = \hat{\eta}(\xi)$ yields:
%
\begin{align}
  \frac{\partial^2 U}{\partial \eta \partial \xi} + \left(\frac{\partial^2 U}{\partial \eta^2}\right) \left(\frac{\partial \hat{\eta}}{\partial \xi}\right) = 0
  \quad \Rightarrow \quad
  \frac{\partial \hat{\eta}}{\partial \xi} = -\left(\frac{\partial^2 U}{\partial \eta^2}\right)^{-1} \frac{\partial^2 U}{\partial \eta \partial \xi}.
\end{align}
%
Since $\hat{H}(\xi) \eqqcolon \left. \frac{\partial^2 U}{\partial \eta^2} \right|_{\eta=\hat{\eta}(\xi)}$, the pullback $\mathcal{B}_{\hat{\eta}}$ mapping the cotangent $\Delta^{\hat{\eta}} = \frac{\partial \mathcal{L}}{\partial \hat{\eta}}$ to $\frac{\partial \mathcal{L}}{\partial \xi}$ is given by
%
\begin{align}\label{eq:pullback-eta-hat}
  \mathcal{B}_{\hat{\eta}}(\Delta^{\hat{\eta}})
  = \Delta^{\hat{\eta}} \bullet \frac{\partial \hat{\eta}}{\partial \xi}
  = -\left( \hat{H}(\xi)^{-1} \Delta^{\hat{\eta}} \right)^{T} \frac{\partial^2 U}{\partial \eta \partial \xi}.
\end{align}
%
The map for the Hessian is explicit, $H(\xi, \eta) = \frac{\partial^2 U}{\partial \eta^2}$.
Its pullback $\mathcal{B}_H$ mapping the cotangent $\Delta^H = \frac{\partial \mathcal{L}}{\partial H}$ to $\frac{\partial \mathcal{L}}{\partial \xi}$ and $\frac{\partial \mathcal{L}}{\partial \eta}$ thus requires third-order derivative tensors of $U$:
%
\begin{align}\label{eq:pullback-H}
  \mathcal{B}_H(\Delta^H) = \left( \Delta^H \bullet \frac{\partial^3 U}{\partial \eta^2 \partial \xi}, \, \Delta^H \bullet \frac{\partial^3 U}{\partial \eta^3} \right).
\end{align}

Since $U = L - \log p(\lambda, \eta)$ and $L = \sum_i \ell(\xi^{(i)}, \eta)$, mixed partial derivatives of $U$ and $L$ are equal, $\frac{\partial^{k+1}}{\partial \eta^k \partial \xi} U = \frac{\partial^{k+1}}{\partial \eta^k \partial \xi} L$, and $\frac{\partial^k}{\partial \eta^k} U = \frac{\partial^k}{\partial \eta^k} L - \frac{\partial^k}{\partial \eta^k} \log p(\lambda, \eta)$.
The derivatives of $L$ can themselves be written in terms of the derivatives of $\ell_i = \ell(\xi^{(i)}, \eta) = \Omega(\frac{x_i}{\sigma}, \frac{s \mu_i}{\sigma}, \frac{\delta}{\sigma})$:
%
\begin{align}
  \frac{\partial^{k+1} L}{\partial \eta^k \partial x_i} = \frac{\partial^{k+1} \ell_i}{\partial \eta^k \partial x_i},
  \quad
  \frac{\partial^{k+1} L}{\partial \eta^k \partial \mu_i} = \frac{\partial^{k+1} \ell_i}{\partial \eta^k \partial \mu_i},
  \quad
  \frac{\partial^{k+1} L}{\partial \eta^k \partial \delta} = \sum_{i=1}^N \frac{\partial^{k+1} \ell_i}{\partial \eta^k \partial \delta},
  \quad
  \frac{\partial^{k} L}{\partial \eta^{k}} = \sum_{i=1}^N \frac{\partial^{k} \ell_i}{\partial \eta^{k}},
\end{align}
%
and so the VJPs in \cref{eq:pullback-eta-hat,eq:pullback-H} simplify to
%
\begin{align}
  \Delta^{\hat{\eta}'} \bullet \frac{\partial^2 U}{\partial \eta \partial \xi} & = \left( \Delta^{\hat{\eta}'} \bullet \frac{\partial^2 \ell_1}{\partial \eta \partial x_1}, \ldots, \Delta^{\hat{\eta}'} \bullet \frac{\partial^2 \ell_1}{\partial \eta \partial \mu_1}, \ldots, \sum_{i=1}^N \Delta^{\hat{\eta}'} \bullet \frac{\partial^2 \ell_i}{\partial \eta \partial \delta} \right), \label{eq:pullback-eta-hat-simplified} \\
  \Delta^H \bullet \frac{\partial^3 U}{\partial \eta^2 \partial \xi}           & = \left( \Delta^H \bullet \frac{\partial^3 \ell_1}{\partial \eta^2 \partial x_1}, \ldots, \Delta^H \bullet \frac{\partial^3 \ell_1}{\partial \eta^2 \partial \mu_1}, \ldots, \sum_{i=1}^N \Delta^H \bullet \frac{\partial^3 \ell_i}{\partial \eta^2 \partial \delta} \right), \label{eq:pullback-H-Phi-eta-squared-simplified}                     \\
  \Delta^H \bullet \frac{\partial^3 U}{\partial \eta^3}                        & = -\Delta^H \bullet \frac{\partial^3 \log p(\lambda, \eta)}{\partial \eta^3} + \sum_{i=1}^N \Delta^H \bullet \frac{\partial^3 \ell_i}{\partial \eta^3} \label{eq:pullback-H-eta-cubed-simplified}
\end{align}
%
where $\Delta^{\hat{\eta}'} = \hat{H}^{-1} \Delta^{\hat{\eta}}$.
Note that for a factorized prior $p(\lambda, \eta) = p(\lambda) p(\eta)$ with $p(\eta)$ Gaussian, $\frac{\partial^3}{\partial \eta^3} \log p(\lambda, \eta) = 0$.
Finally, differentiating through $\hat{\eta}$ and $\hat{H}$ reduces to computing the following derivative tensors:
%
\begin{align}\label{eq:ell-derivative-tensors}
  \ell_{\eta \xi^{(i)}} \coloneqq \frac{\partial^2 \ell_i}{\partial \eta \partial \xi^{(i)}},
  \quad
  \ell_{\eta \eta \xi^{(i)}} \coloneqq \frac{\partial^3 \ell_i}{\partial \eta^2 \partial \xi^{(i)}},
  \quad
  \ell_{\eta \eta \eta} \coloneqq \frac{\partial^3 \ell_i}{\partial \eta^3}.
\end{align}

\subsection{Derivatives of the Per-Observation Log-Likelihood}

The derivatives of $\ell_i$ required for the pullbacks in \cref{eq:ell-derivative-tensors} are found by applying the multivariate chain rule to the composition $\ell_i = \ell(\xi^{(i)}, \eta) = \Omega(\theta(\xi^{(i)}, \eta))$, where
%
\begin{align}
  \theta(\xi^{(i)}, \eta) = (x', \nu', \delta')^T = \left(\frac{x_i}{\sigma}, \frac{s \mu_i}{\sigma}, \frac{\delta}{\sigma}\right)^T,
  \quad
  \xi^{(i)} = (x_i, \mu_i, \delta)^T,
  \quad
  \eta = (\log s, \log \sigma)^T.
\end{align}
%
In previous sections, we derived expressions for the numerically stable evaluation of the derivative tensors $\nabla_\theta \Omega$, $\nabla_\theta^2\Omega$, and $\nabla_\theta^3\Omega$.
We can express derivatives of $\ell_i$ in \cref{eq:ell-derivative-tensors} in terms of these derivatives of $\Omega$ via the multivariate chain rule.
Dropping the index $i$ for brevity, let $\Theta_{\xi} \coloneqq \frac{\partial \theta}{\partial \xi}$ and $\Theta_{\eta} \coloneqq \frac{\partial \theta}{\partial \eta}$ be the Jacobians of $\theta$ with respect to $\xi$ and $\eta$.
The gradients of $\ell$ are
%
\begin{align}
  \ell_{\xi} = \Theta_{\xi}^{T} \nabla_\theta \Omega,
  \qquad
  \ell_{\eta}   = \Theta_{\eta}^{T} \nabla_\theta \Omega.
\end{align}
%
The second derivative matrices are found by applying the chain rule again:
%
\begin{align}
  \ell_{\eta \xi}  & = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\xi + \sum_{k} (\nabla_\theta \Omega)_{k} \Theta_{\eta\xi}^{(\theta_k)},   \\
  \ell_{\eta \eta} & = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\eta + \sum_{k} (\nabla_\theta \Omega)_{k} \Theta_{\eta\eta}^{(\theta_k)}.
\end{align}
%
where $\Theta_{\eta\xi}$ and $\Theta_{\eta\eta}$ are 3-tensors containing the second derivatives of $\theta$, with components $\Theta_{\eta\xi}^{(\theta_k)} \coloneqq \frac{\partial^2 \theta_k}{\partial \eta \partial \xi}$ and $\Theta_{\eta\eta}^{(\theta_k)} \coloneqq \frac{\partial^2 \theta_k}{\partial \eta^2}$.
The third-derivative tensors required by \cref{eq:pullback-eta-hat-simplified,eq:pullback-H-Phi-eta-squared-simplified,eq:pullback-H-eta-cubed-simplified}
are obtained by differentiating the Hessian $\ell_{\eta\eta}$.
Using the Einstein summation convention, the $ij^\text{th}$ component of the Hessian is
%
\begin{align}
  (\ell_{\eta\eta})_{ij}
  = \frac{\partial^2 \ell}{\partial \eta_i \partial \eta_j}
  = \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j}
  + \frac{\partial \Omega}{\partial \theta_c} \frac{\partial^2 \theta_c}{\partial \eta_i \partial \eta_j}.
\end{align}
%
Differentiating $(\ell_{\eta\eta})_{ij}$ with respect to $\xi_k$ and $\eta_l$ yields
%
\begin{align}
  \tfrac{\partial (\ell_{\eta\eta})_{ij}}{\partial \xi_k}  & = \tfrac{\partial \Omega}{\partial \theta_a} \tfrac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k} + \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \left( \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \tfrac{\partial \theta_b}{\partial \eta_j} + \tfrac{\partial^2 \theta_a}{\partial \eta_j \partial \xi_k} \tfrac{\partial \theta_b}{\partial \eta_i} + \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \tfrac{\partial \theta_b}{\partial \xi_k} \right) + \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} \tfrac{\partial \theta_a}{\partial \eta_i} \tfrac{\partial \theta_b}{\partial \eta_j} \tfrac{\partial \theta_c}{\partial \xi_k}, \label{eq:third-derivative-ell-eta-eta-xi}       \\
  \tfrac{\partial (\ell_{\eta\eta})_{ij}}{\partial \eta_l} & = \tfrac{\partial \Omega}{\partial \theta_c} \tfrac{\partial^3 \theta_c}{\partial \eta_i \partial \eta_j \partial \eta_l} + \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \left( \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_l} \tfrac{\partial \theta_b}{\partial \eta_j} + \tfrac{\partial^2 \theta_a}{\partial \eta_j \partial \eta_l} \tfrac{\partial \theta_b}{\partial \eta_i} + \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \tfrac{\partial \theta_b}{\partial \eta_l} \right) + \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} \tfrac{\partial \theta_a}{\partial \eta_i} \tfrac{\partial \theta_b}{\partial \eta_j} \tfrac{\partial \theta_c}{\partial \eta_l}. \label{eq:third-derivative-ell-eta-eta-eta}
\end{align}
%
The elements of the required VJPs are then obtained by contracting with the symmetric matrix $\Delta^H$:
%
\begin{align}
  (\Delta^H \bullet \ell_{\eta\eta\xi})_k  & = \left( \Delta^H_{ij} \tfrac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k} \right) \tfrac{\partial \Omega}{\partial \theta_a} + \Delta^H_{ij} \left( 2 \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \tfrac{\partial \theta_b}{\partial \eta_j} + \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \tfrac{\partial \theta_b}{\partial \xi_k} \right) \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} + \left( \Delta^H_{ij} \tfrac{\partial \theta_a}{\partial \eta_i} \tfrac{\partial \theta_b}{\partial \eta_j} \right) \tfrac{\partial \theta_c}{\partial \xi_k} \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}, \label{eq:third-derivative-ell-eta-eta-xi-delta-h}      \\
  (\Delta^H \bullet \ell_{\eta\eta\eta})_l & = \left( \Delta^H_{ij} \tfrac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \eta_l} \right) \tfrac{\partial \Omega}{\partial \theta_a} + \Delta^H_{ij} \left( 2 \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_l} \tfrac{\partial \theta_b}{\partial \eta_j} + \tfrac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \tfrac{\partial \theta_b}{\partial \eta_l} \right) \tfrac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} + \left( \Delta^H_{ij} \tfrac{\partial \theta_a}{\partial \eta_i} \tfrac{\partial \theta_b}{\partial \eta_j} \right) \tfrac{\partial \theta_c}{\partial \eta_l} \tfrac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}. \label{eq:third-derivative-ell-eta-eta-eta-delta-h}
\end{align}
%
Explicit formulas for the tensors $\Theta_{\xi}$, $\Theta_{\eta}$, $\Theta_{\eta\xi}$, $\Theta_{\eta\eta}$, $\Theta_{\eta\eta\xi}$, and $\Theta_{\eta\eta\eta}$ are collected in \Cref{app:explicit-change-of-vars-derivatives}.

% Bibliography
\bibliographystyle{plainnat}
\bibliography{bibliography}

% Appendix
\appendix

\section{Proofs of expectation and covariance identities}\label{app:proofs-expectation-covariance}

These proofs are deferred from the main text for brevity;
they rely only on linearity of differentiation and integration, and apply equally well if integration is replaced with fixed-node quadrature.

\subsection{Gradient-of-expectation identity (\Cref{eq:gradient-expectation-covariance})}\label{app:proof-gradient-expectation}

We begin with the definition of the expectation and apply the product rule:
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \nabla_\theta \left( Z(\theta)^{-1} \int_0^1 g(t,\theta) \exp(-\tilde{r}(t,\theta)) \, dt \right)                                            \\
                              & = \left(\int_0^1 g e^{-\tilde{r}} \, dt\right) (\nabla_\theta Z^{-1})^T + Z^{-1} \nabla_\theta \left( \int_0^1 g e^{-\tilde{r}} \, dt \right).
\end{align}
%
The first term simplifies using the identity $\nabla_\theta Z^{-1} = -Z^{-1} \nabla_\theta \log Z$:
%
\begin{align}
  \left(Z \mathbb{E}[g]\right) \left(-Z^{-1} \nabla_\theta \log Z\right)^T = -\mathbb{E}[g] (\nabla_\theta \log Z)^T.
\end{align}
%
For the second term, we move the derivative inside the integral and apply the product rule again:
%
\begin{align}
  Z^{-1} \int_0^1 \nabla_\theta(g e^{-\tilde{r}}) \, dt & = Z^{-1} \int_0^1 \left( (\nabla_\theta g) e^{-\tilde{r}} - g (\nabla_\theta \tilde{r})^T e^{-\tilde{r}} \right) \, dt \\
                                                        & = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^T].
\end{align}
%
Combining these results gives
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^T] - \mathbb{E}[g](\nabla_\theta \log Z)^T.
\end{align}
%
To simplify the final term, we compute the gradient of the log-partition function:
%
\begin{align}
  \nabla_\theta \log Z = \frac{\nabla_\theta Z}{Z} = Z^{-1} \int_0^1 \nabla_\theta e^{-\tilde{r}} \, dt = -Z^{-1} \int_0^1 \nabla_\theta \tilde{r} e^{-\tilde{r}} \, dt = -\mathbb{E}[\nabla_\theta \tilde{r}].
\end{align}
%
Substituting this back yields the desired identity:
%
\begin{align}
  \nabla_\theta \mathbb{E}[g] & = \mathbb{E}[\nabla_\theta g] - \mathbb{E}[g (\nabla_\theta \tilde{r})^T] - \mathbb{E}[g] (-\mathbb{E}[\nabla_\theta \tilde{r}])^T \\
                              & = \mathbb{E}[\nabla_\theta g] - (\mathbb{E}[g (\nabla_\theta \tilde{r})^T] - \mathbb{E}[g] \mathbb{E}[\nabla_\theta \tilde{r}]^T)  \\
                              & = \mathbb{E}[\nabla_\theta g] - \mathrm{Cov}(g, \nabla_\theta \tilde{r}).
\end{align}
%
This proof relies only on the linearity of differentiation and integration (or finite summation for quadrature), so the identity holds for both the continuous integral and its discrete quadrature approximation.

\subsection{Covariance-derivative identity (\Cref{eq:derivative-covariance-identity})}\label{app:proof-covariance-derivative}

We begin with the definition of covariance and apply the product rule:
%
\begin{align}
  \partial_{\gamma} \mathrm{Cov}(g, h) & = \partial_{\gamma} \left( \mathbb{E}[g h] - \mathbb{E}[g]\mathbb{E}[h] \right) = \partial_{\gamma} \mathbb{E}[g h] - (\partial_{\gamma} \mathbb{E}[g]) \mathbb{E}[h] - \mathbb{E}[g] (\partial_{\gamma} \mathbb{E}[h]).
\end{align}
%
Next, we apply the gradient-of-expectation identity from \cref{eq:gradient-expectation-covariance-residual} to each of the three derivative terms:
%
\begin{align}
  \partial_{\gamma} \mathbb{E}[g h] & = \mathbb{E}[\partial_{\gamma}(g h)] - \mathrm{Cov}(g h, \partial_{\gamma} \tilde{r}) = \mathbb{E}[(\partial_{\gamma} g) h + g (\partial_{\gamma} h)] - \mathrm{Cov}(g h, \partial_{\gamma} \tilde{r}) \\
  \partial_{\gamma} \mathbb{E}[g]   & = \mathbb{E}[\partial_{\gamma} g] - \mathrm{Cov}(g, \partial_{\gamma} \tilde{r})                                                                                                                       \\
  \partial_{\gamma} \mathbb{E}[h]   & = \mathbb{E}[\partial_{\gamma} h] - \mathrm{Cov}(h, \partial_{\gamma} \tilde{r}).
\end{align}
%
Substituting these back into the main expression and grouping terms yields:
%
\begin{align}
  \partial_{\gamma} \mathrm{Cov}(g, h) & = \mathbb{E}[(\partial_{\gamma} g) h] - \mathbb{E}[\partial_{\gamma} g] \mathbb{E}[h] + \mathbb{E}[g (\partial_{\gamma} h)] - \mathbb{E}[g] \mathbb{E}[\partial_{\gamma} h]           \\
                                       & - \mathrm{Cov}(g h, \partial_{\gamma} \tilde{r}) + \mathrm{Cov}(g, \partial_{\gamma} \tilde{r}) \mathbb{E}[h] + \mathbb{E}[g] \mathrm{Cov}(h, \partial_{\gamma} \tilde{r}). \nonumber
\end{align}
%
The term in the parentheses is the third joint central moment, or third-order covariance, of $(g, h, \partial_{\gamma} \tilde{r})$.
Expanding the definition of the third central moment confirms its equivalence to this term:
%
\begin{align}
  \mathrm{Cov3}(g, h, \partial_{\gamma} \tilde{r}) & = \mathbb{E}[(g-\mathbb{E}[g])(h-\mathbb{E}[h])(\partial_{\gamma} \tilde{r}-\mathbb{E}[\partial_{\gamma} \tilde{r}])]                                                                                                                                                                           \\
                                                   & = \mathbb{E}[(gh - g\mathbb{E}[h] - h\mathbb{E}[g] + \mathbb{E}[g]\mathbb{E}[h])(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])]                                                                                                                                        \\
                                                   & = \mathbb{E}[gh(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] - \mathbb{E}[h]\mathbb{E}[g(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] - \mathbb{E}[g]\mathbb{E}[h(\partial_{\gamma} \tilde{r} - \mathbb{E}[\partial_{\gamma} \tilde{r}])] \\
                                                   & = \mathrm{Cov}(gh, \partial_{\gamma} \tilde{r}) - \mathbb{E}[h]\mathrm{Cov}(g, \partial_{\gamma} \tilde{r}) - \mathbb{E}[g]\mathrm{Cov}(h, \partial_{\gamma} \tilde{r}).
\end{align}
%
Substituting this back gives the desired expression for the derivative of the covariance.

\section{Coordinate-transformation derivatives for $\theta(\xi,\eta)$}\label{app:theta-derivatives}

This appendix derives the first-, second-, and third-order derivatives of the coordinate map $\theta(\xi,\eta)$ used in $\ell(\xi,\eta) = \Omega(\theta(\xi,\eta))$.
These tensors are used in the main text to construct the pullbacks for Laplace's approximation; see \Cref{eq:pullback-eta-hat,eq:pullback-H}.
We present variable definitions, then Jacobians and higher-order derivative tensors for $\theta$; a compact summary of the explicit tensors appears in \Cref{app:explicit-change-of-vars-derivatives}.
Derivatives of $\ell$ and the associated VJPs are developed in \Cref{app:ell-derivatives}.

\subsection{Variable Definitions and Coordinate Transformation}

The per-observation log-likelihood $\ell$ is a composition of functions $\ell(\xi, \eta) = \Omega(\theta(\xi, \eta))$.
The variables are defined as:
%
\begin{align}
  \theta & = (\theta_1, \theta_2, \theta_3)^T = (x', \nu', \delta')^T = \left(\frac{x}{\sigma}, \frac{s \mu}{\sigma}, \frac{\delta}{\sigma}\right)^T, \\
  \xi    & = (\xi_1, \xi_2, \xi_3)^T = (x, \mu, \delta)^T,                                                                                            \\
  \eta   & = (\eta_1, \eta_2)^T = (\log s, \log \sigma)^T.
\end{align}
%
From the definition of $\eta$, we have the explicit relationships $s = e^{\eta_1}$ and $\sigma = e^{\eta_2}$.
The components of $\theta$ can thus be expressed in terms of $\xi$ and $\eta$:
%
\begin{align}
  \theta_1 = x'(\xi, \eta)      & = x e^{-\eta_2},           \\
  \theta_2 = \nu'(\xi, \eta)    & = \mu e^{\eta_1 - \eta_2}, \\
  \theta_3 = \delta'(\xi, \eta) & = \delta e^{-\eta_2}.
\end{align}
%
Our goal is to compute the first, second, and third derivatives of $\ell$ with respect to $\xi$ and $\eta$.
By the multivariate chain rule, this requires computing the first, second, and third derivatives of the coordinate transformation $\theta(\xi, \eta)$.

\subsection{First derivatives of the transformation}

We first compute the Jacobians of $\theta$ with respect to $\xi$ and $\eta$.

\paragraph{Jacobian with respect to $\xi$}

The Jacobian $\Theta_\xi \coloneqq \frac{\partial \theta}{\partial \xi}$ is a $3 \times 3$ matrix whose entries are $(\Theta_\xi)_{ij} = \frac{\partial \theta_i}{\partial \xi_j}$.
%
\begin{align}
  \frac{\partial \theta_1}{\partial \xi_1} & = \frac{\partial x'}{\partial x} = \frac{1}{\sigma}, &  & \frac{\partial \theta_1}{\partial \xi_2} = \frac{\partial x'}{\partial \mu} = 0,                  &  & \frac{\partial \theta_1}{\partial \xi_3} = \frac{\partial x'}{\partial \delta} = 0,                     \\
  \frac{\partial \theta_2}{\partial \xi_1} & = \frac{\partial \nu'}{\partial x} = 0,              &  & \frac{\partial \theta_2}{\partial \xi_2} = \frac{\partial \nu'}{\partial \mu} = \frac{s}{\sigma}, &  & \frac{\partial \theta_2}{\partial \xi_3} = \frac{\partial \nu'}{\partial \delta} = 0,                   \\
  \frac{\partial \theta_3}{\partial \xi_1} & = \frac{\partial \delta'}{\partial x} = 0,           &  & \frac{\partial \theta_3}{\partial \xi_2} = \frac{\partial \delta'}{\partial \mu} = 0,             &  & \frac{\partial \theta_3}{\partial \xi_3} = \frac{\partial \delta'}{\partial \delta} = \frac{1}{\sigma}.
\end{align}
%
Assembling these components yields the matrix:
%
\begin{align}
  \Theta_{\xi} =
  \begin{bmatrix}
    \frac{1}{\sigma} & 0                & 0                \\
    0                & \frac{s}{\sigma} & 0                \\
    0                & 0                & \frac{1}{\sigma}
  \end{bmatrix}.
\end{align}

\paragraph{Jacobian with respect to $\eta$}

The Jacobian $\Theta_\eta \coloneqq \frac{\partial \theta}{\partial \eta}$ is a $3 \times 2$ matrix whose entries are $(\Theta_\eta)_{ij} = \frac{\partial \theta_i}{\partial \eta_j}$.
%
\begin{align}
  \frac{\partial \theta_1}{\partial \eta_1} & = \frac{\partial (x e^{-\eta_2})}{\partial \eta_1} = 0,                                        &  & \frac{\partial \theta_1}{\partial \eta_2} = \frac{\partial (x e^{-\eta_2})}{\partial \eta_2} = -x e^{-\eta_2} = -x',                       \\
  \frac{\partial \theta_2}{\partial \eta_1} & = \frac{\partial (\mu e^{\eta_1 - \eta_2})}{\partial \eta_1} = \mu e^{\eta_1 - \eta_2} = \nu', &  & \frac{\partial \theta_2}{\partial \eta_2} = \frac{\partial (\mu e^{\eta_1 - \eta_2})}{\partial \eta_2} = -\mu e^{\eta_1 - \eta_2} = -\nu', \\
  \frac{\partial \theta_3}{\partial \eta_1} & = \frac{\partial (\delta e^{-\eta_2})}{\partial \eta_1} = 0,                                   &  & \frac{\partial \theta_3}{\partial \eta_2} = \frac{\partial (\delta e^{-\eta_2})}{\partial \eta_2} = -\delta e^{-\eta_2} = -\delta'.
\end{align}
%
Assembling these components yields the matrix:
%
\begin{align}
  \Theta_{\eta} =
  \begin{bmatrix}
    0    & -x'      \\
    \nu' & -\nu'    \\
    0    & -\delta'
  \end{bmatrix}.
\end{align}

\subsection{Second derivatives of the transformation}

The second derivatives are 3-tensors.
We compute $\Theta_{\eta\xi} \coloneqq \frac{\partial^2 \theta}{\partial \eta \partial \xi}$ and $\Theta_{\eta\eta} \coloneqq \frac{\partial^2 \theta}{\partial \eta^2}$.
For clarity, we present these tensors as a collection of matrices, one for each component of $\theta$.

\paragraph{Mixed Second Derivative $\Theta_{\eta\xi}$}

The components of this tensor are $\frac{\partial}{\partial \eta_j} \left( \frac{\partial \theta_k}{\partial \xi_i} \right)$.
We can find these by differentiating $\Theta_\xi$ with respect to each component of $\eta$.

\begin{align}
  \frac{\partial^2 x'}{\partial \eta_1 \partial \xi}      & = \left( \frac{\partial}{\partial \eta_1}\frac{\partial x'}{\partial x}, \frac{\partial}{\partial \eta_1}\frac{\partial x'}{\partial \mu}, \frac{\partial}{\partial \eta_1}\frac{\partial x'}{\partial \delta} \right) = \left( \frac{\partial (1/\sigma)}{\partial \eta_1}, 0, 0 \right) = (0, 0, 0),                                   \\
  \frac{\partial^2 x'}{\partial \eta_2 \partial \xi}      & = \left( \frac{\partial}{\partial \eta_2}\frac{\partial x'}{\partial x}, \frac{\partial}{\partial \eta_2}\frac{\partial x'}{\partial \mu}, \frac{\partial}{\partial \eta_2}\frac{\partial x'}{\partial \delta} \right) = \left( \frac{\partial (e^{-\eta_2})}{\partial \eta_2}, 0, 0 \right) = (-e^{-\eta_2}, 0, 0) = (-1/\sigma, 0, 0), \\
  \frac{\partial^2 \nu'}{\partial \eta_1 \partial \xi}    & = \left( 0, \frac{\partial (s/\sigma)}{\partial \eta_1}, 0 \right) = \left( 0, \frac{\partial (e^{\eta_1}/\sigma)}{\partial \eta_1}, 0 \right) = (0, s/\sigma, 0),                                                                                                                                                                       \\
  \frac{\partial^2 \nu'}{\partial \eta_2 \partial \xi}    & = \left( 0, \frac{\partial (s/\sigma)}{\partial \eta_2}, 0 \right) = \left( 0, \frac{\partial (s e^{-\eta_2})}{\partial \eta_2}, 0 \right) = (0, -s/\sigma, 0),                                                                                                                                                                          \\
  \frac{\partial^2 \delta'}{\partial \eta_1 \partial \xi} & = \left( 0, 0, \frac{\partial (1/\sigma)}{\partial \eta_1} \right) = (0, 0, 0),                                                                                                                                                                                                                                                          \\
  \frac{\partial^2 \delta'}{\partial \eta_2 \partial \xi} & = \left( 0, 0, \frac{\partial (1/\sigma)}{\partial \eta_2} \right) = \left( 0, 0, \frac{\partial (e^{-\eta_2})}{\partial \eta_2} \right) = (0, 0, -1/\sigma).
\end{align}
%
For $\theta_1 = x'$, $\theta_2 = \nu'$, and $\theta_3 = \delta'$, the component matrices $\Theta_{\eta\xi}^{(x')} = \frac{\partial^2 x'}{\partial \eta \partial \xi}$, $\Theta_{\eta\xi}^{(\nu')} = \frac{\partial^2 \nu'}{\partial \eta \partial \xi}$, and $\Theta_{\eta\xi}^{(\delta')} = \frac{\partial^2 \delta'}{\partial \eta \partial \xi}$ are:
%
\begin{align}
  \Theta_{\eta\xi}^{(x')}      & = \begin{bsmallmatrix} 0 & 0 & 0 \\ -\frac{1}{\sigma} & 0 & 0 \end{bsmallmatrix}, &
  \Theta_{\eta\xi}^{(\nu')}    & = \begin{bsmallmatrix} 0 & \frac{s}{\sigma} & 0 \\ 0 & -\frac{s}{\sigma} & 0 \end{bsmallmatrix}, &
  \Theta_{\eta\xi}^{(\delta')} & = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & -\frac{1}{\sigma} \end{bsmallmatrix}.
\end{align}

\paragraph{Second Derivative $\Theta_{\eta\eta}$}

The components of this tensor are $\frac{\partial}{\partial \eta_j} \left( \frac{\partial \theta_k}{\partial \eta_i} \right)$.
We can find these by differentiating $\Theta_\eta$ with respect to each component of $\eta$.
%
\begin{align}
  \frac{\partial^2 x'}{\partial \eta_1^2}                 & = \frac{\partial}{\partial \eta_1}(0)   = 0, \qquad \frac{\partial^2 x'}{\partial \eta_2 \partial \eta_1} = \frac{\partial}{\partial \eta_2}(0) = 0,    \\
  \frac{\partial^2 x'}{\partial \eta_2^2}                 & = \frac{\partial}{\partial \eta_2}(-x') = \frac{\partial}{\partial \eta_2}(-x e^{-\eta_2}) = x e^{-\eta_2} = x',                                        \\
  \frac{\partial^2 \nu'}{\partial \eta_1^2}               & = \frac{\partial}{\partial \eta_1}(\nu') = \frac{\partial}{\partial \eta_1}(\mu e^{\eta_1 - \eta_2}) = \nu',                                            \\
  \frac{\partial^2 \nu'}{\partial \eta_2 \partial \eta_1} & = \frac{\partial}{\partial \eta_2}(\nu') = \frac{\partial}{\partial \eta_2}(\mu e^{\eta_1 - \eta_2}) = -\nu',                                           \\
  \frac{\partial^2 \nu'}{\partial \eta_2^2}               & = \frac{\partial}{\partial \eta_2}(-\nu') = \frac{\partial}{\partial \eta_2}(-\mu e^{\eta_1 - \eta_2}) = \nu',                                          \\
  \frac{\partial^2 \delta'}{\partial \eta_1^2}            & = \frac{\partial}{\partial \eta_1}(0) = 0, \qquad \frac{\partial^2 \delta'}{\partial \eta_2 \partial \eta_1} = \frac{\partial}{\partial \eta_2}(0) = 0, \\
  \frac{\partial^2 \delta'}{\partial \eta_2^2}            & = \frac{\partial}{\partial \eta_2}(-\delta') = \frac{\partial}{\partial \eta_2}(-\delta e^{-\eta_2}) = \delta e^{-\eta_2} = \delta'.
\end{align}
%
Thus, for $\theta_1 = x'$, $\theta_2 = \nu'$, and $\theta_3 = \delta'$, the component matrices are $\Theta_{\eta\eta}^{(x')} = \frac{\partial^2 x'}{\partial \eta^2}$, $\Theta_{\eta\eta}^{(\nu')} = \frac{\partial^2 \nu'}{\partial \eta^2}$, and $\Theta_{\eta\eta}^{(\delta')} = \frac{\partial^2 \delta'}{\partial \eta^2}$ are:
%
\begin{align}
  \Theta_{\eta\eta}^{(x')}      & = \begin{bsmallmatrix} 0 & 0 \\ 0 & x' \end{bsmallmatrix}, &
  \Theta_{\eta\eta}^{(\nu')}    & = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix}, &
  \Theta_{\eta\eta}^{(\delta')} & = \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta' \end{bsmallmatrix}.
\end{align}

\subsection{Third derivatives of the transformation}

The third derivatives are 4-tensors.
We compute $\Theta_{\eta\eta\xi} \coloneqq \frac{\partial^3 \theta}{\partial \eta^2 \partial \xi}$ and $\Theta_{\eta\eta\eta} \coloneqq \frac{\partial^3 \theta}{\partial \eta^3}$.
These are found by differentiating the second-derivative component matrices with respect to $\xi_k$ and $\eta_l$.

\paragraph{Mixed third derivative $\Theta_{\eta\eta\xi}$}

We differentiate the matrices $\Theta_{\eta\xi}^{(x')}, \Theta_{\eta\xi}^{(\nu')}, \Theta_{\eta\xi}^{(\delta')}$ with respect to $\eta_1 = \log s$ and $\eta_2 = \log \sigma$:
%
\begin{align}
  \frac{\partial \Theta_{\eta\xi}^{(x')}}{\partial \eta_1}      & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & 0 & 0 \\ -1/\sigma & 0 & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\xi}^{(x')}}{\partial \eta_2}      & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & 0 & 0 \\ -e^{-\eta_2} & 0 & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ e^{-\eta_2} & 0 & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 1/\sigma & 0 & 0 \end{bsmallmatrix}, \\
  \frac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial \eta_1}    & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & s/\sigma & 0 \\ 0 & -s/\sigma & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & s/\sigma & 0 \\ 0 & -s/\sigma & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial \eta_2}    & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & s e^{-\eta_2} & 0 \\ 0 & -s e^{-\eta_2} & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & -s e^{-\eta_2} & 0 \\ 0 & s e^{-\eta_2} & 0 \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & -s/\sigma & 0 \\ 0 & s/\sigma & 0 \end{bsmallmatrix}, \\
  \frac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial \eta_1} & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & -1/\sigma \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial \eta_2} & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & -e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 1/\sigma \end{bsmallmatrix}.
\end{align}

\paragraph{Third Derivative $\Theta_{\eta\eta\eta}$}

We differentiate the matrices $\Theta_{\eta\eta}^{(x')}, \Theta_{\eta\eta}^{(\nu')}, \Theta_{\eta\eta}^{(\delta')}$ with respect to $\eta_1 = \log s$ and $\eta_2 = \log \sigma$:
%
\begin{align}
  \frac{\partial \Theta_{\eta\eta}^{(x')}}{\partial \eta_1}      & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & 0 \\ 0 & x' \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\eta}^{(x')}}{\partial \eta_2}      & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & 0 \\ 0 & x e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -x e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -x' \end{bsmallmatrix}, \\
  \frac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial \eta_1}    & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix} = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial \eta_2}    & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix} = \begin{bsmallmatrix} -\nu' & \nu' \\ \nu' & -\nu' \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial \eta_1} & = \frac{\partial}{\partial \eta_1} \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta' \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix},                                   \\
  \frac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial \eta_2} & = \frac{\partial}{\partial \eta_2} \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -\delta e^{-\eta_2} \end{bsmallmatrix} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -\delta' \end{bsmallmatrix}.
\end{align}
%
These results match the tensors presented in \cref{eq:third-derivative-theta-log-s-eta-xi,eq:third-derivative-theta-log-sigma-eta-xi,eq:third-derivative-theta-log-s-eta-eta,eq:third-derivative-theta-log-sigma-eta-eta}.

\subsection{Explicit change-of-variables tensors}\label{app:explicit-change-of-vars-derivatives}

The Jacobians of $\theta$ with respect to $\xi$ and $\eta$ are
%
\begin{align}
  \Theta_{\xi} = \frac{\partial \theta}{\partial \xi} =
  \begin{bmatrix}
    \frac{1}{\sigma} & 0                & 0                \\
    0                & \frac{s}{\sigma} & 0                \\
    0                & 0                & \frac{1}{\sigma}
  \end{bmatrix},
  \qquad
  \Theta_{\eta} = \frac{\partial \theta}{\partial \eta} =
  \begin{bmatrix}
    0    & -x'      \\
    \nu' & -\nu'    \\
    0    & -\delta'
  \end{bmatrix}.
\end{align}
%
The components of the second-derivative 3-tensors
$\Theta_{\eta\xi} = \frac{\partial^2 \theta}{\partial \eta \partial \xi}$ and
$\Theta_{\eta\eta} = \frac{\partial^2 \theta}{\partial \eta^2}$
are
%
\begin{align}
  \Theta_{\eta\xi}^{(x')}                                                                                           & = \frac{\partial^2 x'}{\partial \eta \partial \xi} = \begin{bsmallmatrix} 0 & 0 & 0 \\ -\frac{1}{\sigma} & 0 & 0 \end{bsmallmatrix}, &   &
  \Theta_{\eta\xi}^{(\nu')} = \frac{\partial^2 \nu'}{\partial \eta \partial \xi} = \begin{bsmallmatrix} 0 & \frac{s}{\sigma} & 0 \\ 0 & -\frac{s}{\sigma} & 0 \end{bsmallmatrix}, &                                                                                       &
  \Theta_{\eta\xi}^{(\delta')} = \frac{\partial^2 \delta'}{\partial \eta \partial \xi} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & -\frac{1}{\sigma} \end{bsmallmatrix},                                                                                         \\
  \Theta_{\eta\eta}^{(x')}                                                                                          & = \frac{\partial^2 x'}{\partial \eta^2} = \begin{bsmallmatrix} 0 & 0 \\ 0 & x' \end{bsmallmatrix},            &   &
  \Theta_{\eta\eta}^{(\nu')} = \frac{\partial^2 \nu'}{\partial \eta^2} = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix},           &                                                                                       &
  \Theta_{\eta\eta}^{(\delta')} = \frac{\partial^2 \delta'}{\partial \eta^2} = \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta' \end{bsmallmatrix}.
\end{align}
%
Lastly, the components of the third-derivative 4-tensors
$\Theta_{\eta\eta\xi} \coloneqq \frac{\partial^3 \theta}{\partial \eta^2 \partial \xi}$ and $\Theta_{\eta\eta\eta} \coloneqq \frac{\partial^3 \theta}{\partial \eta^3}$ are
%
\begin{align}
  \Theta_{(\log s)\eta\xi}^{(x')}       & = \tfrac{\partial \Theta_{\eta\xi}^{(x')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix},       &  & \Theta_{(\log s)\eta\xi}^{(\nu')} = \tfrac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & \frac{s}{\sigma} & 0 \\ 0 & -\frac{s}{\sigma} & 0 \end{bsmallmatrix},             &  & \Theta_{(\log s)\eta\xi}^{(\delta')} = \tfrac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \end{bsmallmatrix}, \label{eq:third-derivative-theta-log-s-eta-xi}                  \\
  \Theta_{(\log \sigma)\eta\xi}^{(x')}  & = \tfrac{\partial \Theta_{\eta\xi}^{(x')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & 0 & 0 \\ \frac{1}{\sigma} & 0 & 0 \end{bsmallmatrix},  &  & \Theta_{(\log \sigma)\eta\xi}^{(\nu')} = \tfrac{\partial \Theta_{\eta\xi}^{(\nu')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & -\frac{s}{\sigma} & 0 \\ 0 & \frac{s}{\sigma} & 0 \end{bsmallmatrix},   &  & \Theta_{(\log \sigma)\eta\xi}^{(\delta')} = \tfrac{\partial \Theta_{\eta\xi}^{(\delta')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & 0 & 0 \\ 0 & 0 & \frac{1}{\sigma} \end{bsmallmatrix}, \label{eq:third-derivative-theta-log-sigma-eta-xi}    \\
  \Theta_{(\log s)\eta\eta}^{(x')}      & = \tfrac{\partial \Theta_{\eta\eta}^{(x')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix},      &  & \Theta_{(\log s)\eta\eta}^{(\nu')} = \tfrac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial (\log s)} = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix},           &  & \Theta_{(\log s)\eta\eta}^{(\delta')} = \tfrac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial (\log s)} = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix}, \label{eq:third-derivative-theta-log-s-eta-eta}               \\
  \Theta_{(\log \sigma)\eta\eta}^{(x')} & = \tfrac{\partial \Theta_{\eta\eta}^{(x')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -x' \end{bsmallmatrix}, &  & \Theta_{(\log \sigma)\eta\eta}^{(\nu')} = \tfrac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} -\nu' & \nu' \\ \nu' & -\nu' \end{bsmallmatrix}, &  & \Theta_{(\log \sigma)\eta\eta}^{(\delta')} = \tfrac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial (\log \sigma)} = \begin{bsmallmatrix} 0 & 0 \\ 0 & -\delta' \end{bsmallmatrix} \label{eq:third-derivative-theta-log-sigma-eta-eta}.
\end{align}

\section{Derivatives of the per-observation log-likelihood $\ell$ and VJPs}\label{app:ell-derivatives}

Building on the derivative tensors for $\theta(\xi,\eta)$ from \Cref{app:theta-derivatives}, we apply the multivariate chain rule to derive the first-, second-, and third-order derivatives of $\ell(\xi,\eta) = \Omega(\theta(\xi,\eta))$.
We then assemble the vector-Jacobian products (VJPs) required by the pullbacks in \Cref{eq:pullback-eta-hat,eq:pullback-H}.
Proofs of the expectation and covariance identities used in these derivations are provided in \Cref{app:proofs-expectation-covariance}.
For completeness, fully expanded forms are recorded in the subsection \Cref{app:explict-derivative-expressions}, followed by simplified VJP formulas.

\subsection{Derivatives of $\ell$}

\subsubsection{First derivatives}

The gradients with respect to $\xi$ and $\eta$ are:
%
\begin{align}
  (\ell_\xi)_i = \frac{\partial \ell}{\partial \xi_i}   & = \sum_k \frac{\partial \Omega}{\partial \theta_k} \frac{\partial \theta_k}{\partial \xi_i} = \sum_k (\Theta_\xi)_{ki} (\nabla_\theta \Omega)_k,   \\
  (\ell_\eta)_j = \frac{\partial \ell}{\partial \eta_j} & = \sum_k \frac{\partial \Omega}{\partial \theta_k} \frac{\partial \theta_k}{\partial \eta_j} = \sum_k (\Theta_\eta)_{kj} (\nabla_\theta \Omega)_k.
\end{align}
%
In matrix notation, these are:
%
\begin{align}
  \ell_\xi = \Theta_{\xi}^{T} \nabla_\theta \Omega, \quad \ell_\eta = \Theta_{\eta}^{T} \nabla_\theta \Omega.
\end{align}

\subsubsection{Second derivatives}

The Hessian matrices are found by differentiating the gradients.
Let us derive $\ell_{\eta\eta} = \frac{\partial^2 \ell}{\partial \eta^2}$.
%
\begin{align}
  (\ell_{\eta\eta})_{ij} & = \frac{\partial}{\partial \eta_i} (\ell_\eta)_j = \frac{\partial}{\partial \eta_i} \left( \sum_k \frac{\partial \Omega}{\partial \theta_k} \frac{\partial \theta_k}{\partial \eta_j} \right)                                                                                                           \\
                         & = \sum_k \left( \frac{\partial}{\partial \eta_i}\left(\frac{\partial \Omega}{\partial \theta_k}\right) \frac{\partial \theta_k}{\partial \eta_j} + \frac{\partial \Omega}{\partial \theta_k} \frac{\partial^2 \theta_k}{\partial \eta_i \partial \eta_j} \right)                                        \\
                         & = \sum_k \left( \left( \sum_m \frac{\partial^2 \Omega}{\partial \theta_m \partial \theta_k} \frac{\partial \theta_m}{\partial \eta_i} \right) \frac{\partial \theta_k}{\partial \eta_j} + \frac{\partial \Omega}{\partial \theta_k} \frac{\partial^2 \theta_k}{\partial \eta_i \partial \eta_j} \right) \\
                         & = \sum_{k,m} (\Theta_\eta)_{mi} (\nabla_\theta^2 \Omega)_{mk} (\Theta_\eta)_{kj} + \sum_k (\nabla_\theta \Omega)_k (\Theta_{\eta\eta}^{(\theta_k)})_{ij}.
\end{align}
%
In matrix and Einstein summation notation (summing over repeated indices $a, b, c$ from $\{1,2,3\}$):
%
\begin{align}
  \ell_{\eta \eta}       & = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\eta + \sum_k (\nabla_\theta \Omega)_{k}   \Theta_{\eta\eta}^{(\theta_k)},                                                                                                                               \\
  (\ell_{\eta\eta})_{ij} & = \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial \Omega}{\partial \theta_c} \frac{\partial^2 \theta_c}{\partial \eta_i \partial \eta_j}.
\end{align}
%
A similar derivation yields the mixed Hessian $\ell_{\eta\xi}$:
%
\begin{align}
  \ell_{\eta \xi} = \Theta_{\eta}^{T} (\nabla_\theta^2 \Omega) \Theta_\xi + \sum_k (\nabla_\theta \Omega)_{k}   \Theta_{\eta\xi}^{(\theta_k)}.
\end{align}

\subsubsection{Third derivatives}

We differentiate $(\ell_{\eta\eta})_{ij}$ to find the third-derivative tensors.
Using the product rule and chain rule on the Einstein notation form of $(\ell_{\eta\eta})_{ij}$:
%
\begin{align}
  (\ell_{\eta\eta\xi})_{ijk} = \frac{\partial (\ell_{\eta\eta})_{ij}}{\partial \xi_k} & = \frac{\partial}{\partial \xi_k} \left( \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial \Omega}{\partial \theta_c} \frac{\partial^2 \theta_c}{\partial \eta_i \partial \eta_j} \right)                                                                                                                                                                                                               \\
                                                                                      & = \left( \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} \frac{\partial \theta_c}{\partial \xi_k} \right) \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \left( \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial^2 \theta_b}{\partial \eta_j \partial \xi_k} \right) \\
                                                                                      & \quad + \left( \frac{\partial^2 \Omega}{\partial \theta_c \partial \theta_d} \frac{\partial \theta_d}{\partial \xi_k} \right) \frac{\partial^2 \theta_c}{\partial \eta_i \partial \eta_j} + \frac{\partial \Omega}{\partial \theta_c} \frac{\partial^3 \theta_c}{\partial \eta_i \partial \eta_j \partial \xi_k}.
\end{align}
%
Rearranging and relabeling dummy indices yields the expression in \cref{eq:third-derivative-ell-eta-eta-xi}.
A similar differentiation with respect to $\eta_l$ yields corresponding expression in \cref{eq:third-derivative-ell-eta-eta-eta}.

\subsubsection{Vector-Jacobian products with $\ell_{\eta\eta}$}

Computing the vector-Jacobian products (VJPs) needed for the pullbacks involves contracting the third-derivative tensors of $\ell$ with the upstream symmetric cotangent matrix $\Delta^H$.
The VJP with respect to $\xi_k$ is $(\Delta^H \bullet \ell_{\eta\eta\xi})_k = \Delta^H_{ij} (\ell_{\eta\eta\xi})_{ijk}$.
%
\begin{align}
  \Delta^H_{ij} (\ell_{\eta\eta\xi})_{ijk} & = \Delta^H_{ij} \left( \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} \frac{\partial \theta_c}{\partial \xi_k} \right) + \Delta^H_{ij} \left( \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \left( \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial^2 \theta_b}{\partial \eta_j \partial \xi_k} \right) \right) \\
                                           & + \Delta^H_{ij} \left( \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial \theta_b}{\partial \xi_k} \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \right) + \Delta^H_{ij} \left( \frac{\partial \Omega}{\partial \theta_a} \frac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k} \right).
\end{align}
%
Using the symmetry of $\Delta^H$ and $\frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b}$, the second term becomes $2 \Delta^H_{ij} \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j}$.
Grouping terms by derivatives of $\Omega$ yields:
%
\begin{align}
  \frac{\partial (\Delta^H \bullet \ell_{\eta\eta})}{\partial \xi_k} & = \left( \Delta^H_{ij} \frac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k} \right) \frac{\partial \Omega}{\partial \theta_a}                                                                                                                                                                      \\
                                                                     & + \left( 2 \Delta^H_{ij} \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j} + \Delta^H_{ij} \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \frac{\partial \theta_b}{\partial \xi_k} \right) \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \\
                                                                     & + \left( \Delta^H_{ij} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} \right) \frac{\partial \theta_c}{\partial \xi_k} \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}.
\end{align}
%
This confirms the expression for the VJP with respect to $\xi_k$ in \cref{eq:third-derivative-ell-eta-eta-xi-delta-h}.
An identical procedure for the contraction $\Delta^H_{ij} (\ell_{\eta\eta\eta})_{ijl}$ confirms the VJP with respect to $\eta_l$ in \cref{eq:third-derivative-ell-eta-eta-eta-delta-h}.

\subsection{Explicit Derivative Expressions}\label{app:explict-derivative-expressions}

We now derive the explicit expressions for the derivatives of $\ell$ by substituting the concrete derivatives of the coordinate transformation $\theta$ into the general formulae.
The final expressions are simplified by first defining a set of intermediate linear contractions involving the derivatives of $\Omega$.

\subsubsection{First derivatives (explicit)}

The gradients of $\ell$ with respect to $\xi$ and $\eta$ are computed using the chain rule:
%
\begin{align}
  \ell_\xi = \frac{\partial \theta^T}{\partial \xi} \nabla_\theta \Omega =
  \begin{bmatrix} \ell_x \\ \ell_\mu \\ \ell_\delta \end{bmatrix}
  =
  \begin{bmatrix}
    \frac{1}{\sigma} & 0                & 0                \\
    0                & \frac{s}{\sigma} & 0                \\
    0                & 0                & \frac{1}{\sigma}
  \end{bmatrix}
  \begin{bmatrix} \Omega_{x'} \\ \Omega_{\nu'} \\ \Omega_{\delta'} \end{bmatrix}
  =
  \begin{bmatrix}
    \frac{1}{\sigma} \Omega_{x'}   \\
    \frac{s}{\sigma} \Omega_{\nu'} \\
    \frac{1}{\sigma} \Omega_{\delta'}
  \end{bmatrix}.
\end{align}
%
and
%
\begin{align}
  \ell_\eta = \frac{\partial \theta^T}{\partial \eta} \nabla_\theta \Omega =
  \begin{bmatrix} \ell_{\log s} \\ \ell_{\log \sigma} \end{bmatrix}
  =
  \begin{bmatrix}
    0   & \nu'  & 0        \\
    -x' & -\nu' & -\delta'
  \end{bmatrix}
  \begin{bmatrix} \Omega_{x'} \\ \Omega_{\nu'} \\ \Omega_{\delta'} \end{bmatrix}
  =
  \begin{bmatrix}
    \nu' \Omega_{\nu'} \\
    -x' \Omega_{x'} - \nu' \Omega_{\nu'} - \delta' \Omega_{\delta'}
  \end{bmatrix}.
\end{align}
%
Here we use the shorthand $\Omega_{x'} \coloneqq \frac{\partial \Omega}{\partial x'}$, etc.

\subsubsection{Second derivatives (explicit)}

The Hessian $\ell_{\eta\eta}$ is found by differentiating $\ell_\eta$:
%
\begin{align}
  \ell_{\eta\eta} = \frac{\partial}{\partial \eta} (\ell_\eta) = \frac{\partial}{\partial \eta} (\Theta_\eta^T \nabla_\theta \Omega) = (\frac{\partial}{\partial \eta} \Theta_\eta^T) \nabla_\theta \Omega + \Theta_\eta^T (\frac{\partial}{\partial \eta} \nabla_\theta \Omega).
\end{align}
%
The first term, involving the tensor $\frac{\partial}{\partial \eta} \Theta_\eta^T$, gives the linear part, which we denote $H^{\text{lin}}$.
Its components are given by the contraction $\sum_k \Omega_k \frac{\partial^2 \theta_k}{\partial \eta_i \partial \eta_j}$:
%
\begin{align}
  H^{\text{lin}} & = \sum_k \Omega_k \Theta_{\eta\eta}^{(\theta_k)} = \Omega_{x'} \Theta_{\eta\eta}^{(x')} + \Omega_{\nu'} \Theta_{\eta\eta}^{(\nu')} + \Omega_{\delta'} \Theta_{\eta\eta}^{(\delta')} \\
                 & = \Omega_{x'} \begin{bsmallmatrix} 0 & 0 \\ 0 & x' \end{bsmallmatrix} + \Omega_{\nu'} \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix} + \Omega_{\delta'} \begin{bsmallmatrix} 0 & 0 \\ 0 & \delta' \end{bsmallmatrix}                                    \\
                 & = \begin{bsmallmatrix}
    \nu' \Omega_{\nu'} & -\nu' \Omega_{\nu'} \\
    -\nu' \Omega_{\nu'} & x' \Omega_{x'} + \nu' \Omega_{\nu'} + \delta' \Omega_{\delta'}
  \end{bsmallmatrix}.
\end{align}
%
Thus, the components of the symmetric matrix $H^{\text{lin}}$ are:
%
\begin{align}
  H^{\text{lin}}_{11} & = \nu' \Omega_{\nu'}                                              \\
  H^{\text{lin}}_{12} & = -\nu' \Omega_{\nu'}                                             \\
  H^{\text{lin}}_{22} & = x' \Omega_{x'} + \nu' \Omega_{\nu'} + \delta' \Omega_{\delta'}.
\end{align}
%
The second term gives the quadratic part, $H^{\text{quad}} = \Theta_\eta^T (\nabla_\theta^2 \Omega) \Theta_\eta$:
%
\begin{align}
  H^{\text{quad}} & =
  \begin{bmatrix}
    0   & \nu'  & 0        \\
    -x' & -\nu' & -\delta'
  \end{bmatrix}
  \begin{bmatrix}
    \Omega_{x'x'}      & \Omega_{x'\nu'}      & \Omega_{x'\delta'}      \\
    \Omega_{\nu'x'}    & \Omega_{\nu'\nu'}    & \Omega_{\nu'\delta'}    \\
    \Omega_{\delta'x'} & \Omega_{\delta'\nu'} & \Omega_{\delta'\delta'}
  \end{bmatrix}
  \begin{bmatrix}
    0    & -x'      \\
    \nu' & -\nu'    \\
    0    & -\delta'
  \end{bmatrix}                     \\
                  & = \begin{bsmallmatrix}
    \nu'^2 \Omega_{\nu'\nu'} & -x'\nu'\Omega_{x'\nu'} - \nu'^2\Omega_{\nu'\nu'} - \nu'\delta'\Omega_{\nu'\delta'} \\
    -x'\nu'\Omega_{x'\nu'} - \nu'^2\Omega_{\nu'\nu'} - \nu'\delta'\Omega_{\nu'\delta'} & x'^2\Omega_{x'x'} + \nu'^2\Omega_{\nu'\nu'} + \delta'^2\Omega_{\delta'\delta'} + 2x'\nu'\Omega_{x'\nu'} + 2x'\delta'\Omega_{x'\delta'} + 2\nu'\delta'\Omega_{\nu'\delta'}
  \end{bsmallmatrix}
\end{align}
%
where we use the shorthand $\Omega_{x'x'} \coloneqq \frac{\partial^2 \Omega}{\partial x'^2}$, etc.
The components of this matrix are:
%
\begin{align}
  H^{\text{quad}}_{11} & = \nu'^2 \Omega_{\nu'\nu'}                                                                                                                                                   \\
  H^{\text{quad}}_{12} & = -x'\nu'\Omega_{x'\nu'} - \nu'^2\Omega_{\nu'\nu'} - \nu'\delta'\Omega_{\nu'\delta'}                                                                                         \\
  H^{\text{quad}}_{22} & = x'^2\Omega_{x'x'} + \nu'^2\Omega_{\nu'\nu'} + \delta'^2\Omega_{\delta'\delta'} + 2x'\nu'\Omega_{x'\nu'} + 2x'\delta'\Omega_{x'\delta'} + 2\nu'\delta'\Omega_{\nu'\delta'}.
\end{align}
%
where we have used the symmetry of $\nabla_\theta^2 \Omega$.

\subsubsection{Third Derivatives and VJPs (explicit)}

We now derive the VJP contractions required for the pullbacks, $\Delta^H \bullet \ell_{\eta\eta\xi}$ and $\Delta^H \bullet \ell_{\eta\eta\eta}$, by substituting the concrete derivatives of $\theta$ into the general chain rule formulae.
We do this term-by-term for each component of the VJP.

\paragraph{VJP with respect to $\xi$: coefficient grouping}

The general expression for the $k$-th component of the VJP with respect to $\xi$ is:
%
\begin{align}
  (\Delta^H \bullet \ell_{\eta\eta\xi})_k & = \sum_{i,j,a} \left( \Delta^H_{ij} \frac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k} \right) \frac{\partial \Omega}{\partial \theta_a} \nonumber                                                                                                                                                          \\
                                          & + \sum_{i,j,a,b} \Delta^H_{ij} \left( 2 \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \frac{\partial \theta_b}{\partial \xi_k} \right) \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \nonumber \\
                                          & + \sum_{i,j,a,b,c} \left( \Delta^H_{ij} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} \right) \frac{\partial \theta_c}{\partial \xi_k} \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}.
\end{align}

\paragraph{Gradient terms (Term 1)}

The coefficient of $\frac{\partial \Omega}{\partial \theta_a}$ is $C^{(1)}_{k,a} = \sum_{i,j} \Delta^H_{ij} \frac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \xi_k}$.
This requires differentiating $\Theta_{\eta\eta}^{(\theta_a)}$ with respect to $\xi_k$:
%
\begin{align}
  \frac{\partial \Theta_{\eta\eta}^{(x')}}{\partial x}           & = \begin{bsmallmatrix} 0 & 0 \\ 0 & 1/\sigma \end{bsmallmatrix},                  &
  \frac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial \mu}       & = \frac{s}{\sigma} \begin{bsmallmatrix} 1 & -1 \\ -1 & 1 \end{bsmallmatrix}, &
  \frac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial \delta} & = \begin{bsmallmatrix} 0 & 0 \\ 0 & 1/\sigma \end{bsmallmatrix}.
\end{align}
%
All other derivatives $\frac{\partial}{\partial \xi_k} \Theta_{\eta\eta}^{(\theta_a)}$ are zero.
Contracting with $\Delta^H$ yields:
%
\begin{align}
  (\text{Term 1})_x      & = \frac{1}{\sigma} \Delta^H_{22} \Omega_{x'},                                      \\
  (\text{Term 1})_\mu    & = \frac{s}{\sigma} (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) \Omega_{\nu'}, \\
  (\text{Term 1})_\delta & = \frac{1}{\sigma} \Delta^H_{22} \Omega_{\delta'}.
\end{align}

\paragraph{Hessian terms (Term 2)}

The coefficient of $\frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b}$ is $C^{(2,\xi_k)}_{ab} = \sum_{i,j} \Delta^H_{ij} ( 2 \frac{\partial^2 \theta_a}{\partial \eta_i \partial \xi_k} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \frac{\partial \theta_b}{\partial \xi_k} )$.
We compute this for $k=x, \mu, \delta$ by substituting the relevant derivatives of $\theta$.

$C^{(2,x)}_{ab}$:
The non-zero derivatives with respect to $x$ are $\frac{\partial x'}{\partial x} = \frac{1}{\sigma}$ and $\frac{\partial^2 x'}{\partial \eta_2 \partial x} = -\frac{1}{\sigma}$:
%
\begin{align}
  C^{(2,x)}_{11} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 x'}{\partial\eta_i\partial x}\frac{\partial x'}{\partial\eta_j} + \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 x'}{\partial\eta_i\partial\eta_j}\frac{\partial x'}{\partial x} = 2(-\tfrac{1}{\sigma})(-x')\Delta^H_{22} + \frac{x'}{\sigma}\Delta^H_{22} = \frac{3x'}{\sigma}\Delta^H_{22} \\
  C^{(2,x)}_{12} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 x'}{\partial\eta_i\partial x}\frac{\partial \nu'}{\partial\eta_j} = 2(-\tfrac{1}{\sigma}) (\nu'\Delta^H_{21} - \nu'\Delta^H_{22}) = -\frac{2\nu'}{\sigma}(\Delta^H_{21} - \Delta^H_{22})                                                                                                 \\
  C^{(2,x)}_{13} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 x'}{\partial\eta_i\partial x}\frac{\partial \delta'}{\partial\eta_j} = 2(-\tfrac{1}{\sigma})(-\delta'\Delta^H_{22}) = \frac{2\delta'}{\sigma}\Delta^H_{22}                                                                                                                               \\
  C^{(2,x)}_{21} & = \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \nu'}{\partial\eta_i\partial\eta_j}\frac{\partial x'}{\partial x} = \frac{\nu'}{\sigma}(\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22})                                                                                                                                                    \\
  C^{(2,x)}_{31} & = \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \delta'}{\partial\eta_i\partial\eta_j}\frac{\partial x'}{\partial x} = \frac{\delta'}{\sigma}\Delta^H_{22}.
\end{align}
%
All other coefficients $C^{(2,x)}_{ab}$ are zero.

$C^{(2,\mu)}_{ab}$:
The non-zero derivatives with respect to $\mu$ are $\frac{\partial \nu'}{\partial \mu} = \frac{s}{\sigma}$, $\frac{\partial^2 \nu'}{\partial \eta_1 \partial \mu} = \frac{s}{\sigma}$, and $\frac{\partial^2 \nu'}{\partial \eta_2 \partial \mu} = -\frac{s}{\sigma}$.
The non-zero coefficients $C^{(2,\mu)}_{ab}$ are:
%
\begin{align}
  C^{(2,\mu)}_{12} & = \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 x'}{\partial\eta_i\partial\eta_j}\frac{\partial \nu'}{\partial \mu} = \frac{s}{\sigma} x' \Delta^H_{22}                                                                                   \\
  C^{(2,\mu)}_{21} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \nu'}{\partial\eta_i\partial \mu}\frac{\partial x'}{\partial\eta_j} = -\frac{2sx'}{\sigma}(\Delta^H_{12} - \Delta^H_{22})                                                               \\
  C^{(2,\mu)}_{22} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \nu'}{\partial\eta_i\partial \mu}\frac{\partial \nu'}{\partial\eta_j} + \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \nu'}{\partial\eta_i\partial\eta_j}\frac{\partial \nu'}{\partial \mu} \\
                   & = \frac{3s\nu'}{\sigma}(\Delta^H_{11}-2\Delta^H_{12}+\Delta^H_{22})                                                                                                                                                                   \\
  C^{(2,\mu)}_{23} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \nu'}{\partial\eta_i\partial \mu}\frac{\partial \delta'}{\partial\eta_j} = -\frac{2s\delta'}{\sigma}(\Delta^H_{12} - \Delta^H_{22})                                                     \\
  C^{(2,\mu)}_{32} & = \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \delta'}{\partial\eta_i\partial\eta_j}\frac{\partial \nu'}{\partial \mu} = \frac{s}{\sigma} \delta' \Delta^H_{22}.
\end{align}

$C^{(2,\delta)}_{ab}$:
The non-zero derivatives with respect to $\delta$ are $\frac{\partial \delta'}{\partial \delta} = \frac{1}{\sigma}$ and $\frac{\partial^2 \delta'}{\partial \eta_2 \partial \delta} = -\frac{1}{\sigma}$.
The non-zero coefficients $C^{(2,\delta)}_{ab}$ are therefore:
%
\begin{align}
  C^{(2,\delta)}_{13} & = \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 x'}{\partial\eta_i\partial\eta_j}\frac{\partial \delta'}{\partial \delta} = \frac{x'}{\sigma}\Delta^H_{22}                                                                                                                                          \\
  C^{(2,\delta)}_{23} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \delta'}{\partial\eta_i\partial \delta}\frac{\partial \nu'}{\partial\eta_j} = -\frac{2\nu'}{\sigma}(\Delta^H_{21}-\Delta^H_{22})                                                                                                                  \\
  C^{(2,\delta)}_{31} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \delta'}{\partial\eta_i\partial \delta}\frac{\partial x'}{\partial\eta_j} = \frac{2x'}{\sigma}\Delta^H_{22}                                                                                                                                       \\
  C^{(2,\delta)}_{32} & = \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \nu'}{\partial\eta_i\partial\eta_j}\frac{\partial \delta'}{\partial \delta} = \frac{\nu'}{\sigma}(\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22})                                                                                                   \\
  C^{(2,\delta)}_{33} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \delta'}{\partial\eta_i\partial \delta}\frac{\partial \delta'}{\partial\eta_j} + \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \delta'}{\partial\eta_i\partial\eta_j}\frac{\partial \delta'}{\partial \delta} = \frac{3\delta'}{\sigma}\Delta^H_{22}.
\end{align}

Summing the contributions for the Hessian terms (using $\Delta^H_{12}=\Delta^H_{21}$):
%
\begin{align}
  (\text{Term 2})_x      & = \frac{1}{\sigma} \Big[ 3 x' \Delta^H_{22} \Omega_{x'x'} + (\Delta^H_{11} - 4\Delta^H_{12} + 3\Delta^H_{22}) \nu' \Omega_{x'\nu'} + 3 \delta' \Delta^H_{22} \Omega_{x'\delta'} \Big],                                     \\
  (\text{Term 2})_\mu    & = \frac{s}{\sigma} \Big[ x'(3\Delta^H_{22} - 2\Delta^H_{12})\Omega_{x'\nu'} + 3\nu'(\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22})\Omega_{\nu'\nu'} + \delta'(3\Delta^H_{22} - 2\Delta^H_{12})\Omega_{\nu'\delta'} \Big], \\
  (\text{Term 2})_\delta & = \frac{1}{\sigma} \Big[ 3 x' \Delta^H_{22} \Omega_{x'\delta'} + \nu'(\Delta^H_{11} - 4\Delta^H_{12} + 3\Delta^H_{22}) \Omega_{\nu'\delta'} + 3\delta'\Delta^H_{22}\Omega_{\delta'\delta'} \Big].
\end{align}

\paragraph{Third-order terms (Term 3)}

The third term in the VJP expression is
%
\begin{align}
  (\text{Term 3})_{\xi_k} = \sum_{a,b,c} \left( \sum_{i,j} \Delta^H_{ij} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} \right) \frac{\partial \theta_c}{\partial \xi_k} \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}.
\end{align}
%
This expression involves all 10 unique components of the third-derivative tensor $\nabla_\theta^3 \Omega$.
However, we can exploit the structure of the coefficient multiplying the tensor to reduce the computational burden.
Let us define the \textit{upstream Hessian quadratic form} $Q$ as the $3 \times 3$ symmetric matrix whose components are given by the contraction in the parentheses:
%
\begin{align}
  Q_{ab} \coloneqq \sum_{i,j} \Delta^H_{ij} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} = (\Theta_\eta \Delta^H \Theta_\eta^T)_{ab}.
\end{align}
%
Recalling the definition of $\Theta_\eta$,
\begin{align}
  \Theta_{\eta} =
  \begin{bmatrix}
    0    & -x'      \\
    \nu' & -\nu'    \\
    0    & -\delta'
  \end{bmatrix},
\end{align}
%
and performing the matrix multiplication yields the explicit components of $Q$ (up to symmetry):
%
\begin{align}
  Q_{11} & = x'^2 \Delta^H_{22},                                      & Q_{12} & = -x' \nu' (\Delta^H_{21} - \Delta^H_{22}),     & Q_{13} & = x' \delta' \Delta^H_{22}, \\
  Q_{22} & = \nu'^2 (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}), & Q_{23} & = \nu' \delta' (\Delta^H_{22} - \Delta^H_{12}), & Q_{33} & = \delta'^2 \Delta^H_{22}.
\end{align}
%
With this definition, the third-order term can be rewritten as:
%
\begin{align}
  (\text{Term 3})_{\xi_k} = \sum_{a,b,c} Q_{ab} \frac{\partial \theta_c}{\partial \xi_k} \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} = \sum_c \frac{\partial \theta_c}{\partial \xi_k} \left( \sum_{a,b} Q_{ab} \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c} \right).
\end{align}
%
The term in the parentheses is a vector of 3 linear combinations of the third derivatives of $\Omega$.
We define this vector as the \textit{contracted tensor gradient} $T$:
%
\begin{align}
  T_c \coloneqq (Q \bullet \nabla_\theta^3 \Omega)_c = \sum_{a,b} Q_{ab} \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}.
\end{align}
%
This reduces the computational burden from evaluating 10 third-derivative components to just the 3 components of $T$.
The third-order term of the VJP is now simply $(\text{Term 3})_k = \sum_c \frac{\partial \theta_c}{\partial \xi_k} T_c$.
Since $\Theta_\xi$ is a diagonal matrix, this contraction is straightforward:
%
\begin{align}
  (\text{Term 3})_x      & = \frac{\partial \theta_1}{\partial x} T_1 + \frac{\partial \theta_2}{\partial x} T_2 + \frac{\partial \theta_3}{\partial x} T_3 = \frac{1}{\sigma} T_1,                \\
  (\text{Term 3})_\mu    & = \frac{\partial \theta_1}{\partial \mu} T_1 + \frac{\partial \theta_2}{\partial \mu} T_2 + \frac{\partial \theta_3}{\partial \mu} T_3 = \frac{s}{\sigma} T_2,          \\
  (\text{Term 3})_\delta & = \frac{\partial \theta_1}{\partial \delta} T_1 + \frac{\partial \theta_2}{\partial \delta} T_2 + \frac{\partial \theta_3}{\partial \delta} T_3 = \frac{1}{\sigma} T_3.
\end{align}

\paragraph{VJP with respect to $\eta$: coefficient grouping}

The general expression for the $l$-th component of the VJP with respect to $\eta$ is:
%
\begin{align}
  (\Delta^H \bullet \ell_{\eta\eta\eta})_l & = \sum_{i,j,a} \left( \Delta^H_{ij} \frac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \eta_l} \right) \frac{\partial \Omega}{\partial \theta_a} \nonumber                                                                                                                                                           \\
                                           & + \sum_{i,j,a,b} \Delta^H_{ij} \left( 2 \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_l} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \frac{\partial \theta_b}{\partial \eta_l} \right) \frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b} \nonumber \\
                                           & + \sum_{i,j,a,b,c} \left( \Delta^H_{ij} \frac{\partial \theta_a}{\partial \eta_i} \frac{\partial \theta_b}{\partial \eta_j} \right) \frac{\partial \theta_c}{\partial \eta_l} \frac{\partial^3 \Omega}{\partial \theta_a \partial \theta_b \partial \theta_c}.
\end{align}

\paragraph{Gradient terms (Term 1)}

The coefficient of $\frac{\partial \Omega}{\partial \theta_a}$ is $C^{(1)}_{l,a} = \sum_{i,j} \Delta^H_{ij} \frac{\partial^3 \theta_a}{\partial \eta_i \partial \eta_j \partial \eta_l}$.
We need the tensors $\frac{\partial}{\partial \eta_l} \Theta_{\eta\eta}^{(\theta_a)}$:
%
\begin{align}
  \frac{\partial \Theta_{\eta\eta}^{(x')}}{\partial \eta_1}      & = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix},                              & \frac{\partial \Theta_{\eta\eta}^{(x')}}{\partial \eta_2}      & = \begin{bsmallmatrix} 0 & 0 \\ 0 & -x' \end{bsmallmatrix},                               \\
  \frac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial \eta_1}    & = \Theta_{\eta\eta}^{(\nu')} = \begin{bsmallmatrix} \nu' & -\nu' \\ -\nu' & \nu' \end{bsmallmatrix}, & \frac{\partial \Theta_{\eta\eta}^{(\nu')}}{\partial \eta_2}    & = -\Theta_{\eta\eta}^{(\nu')} = \begin{bsmallmatrix} -\nu' & \nu' \\ \nu' & -\nu' \end{bsmallmatrix}, \\
  \frac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial \eta_1} & = \begin{bsmallmatrix} 0 & 0 \\ 0 & 0 \end{bsmallmatrix},                              & \frac{\partial \Theta_{\eta\eta}^{(\delta')}}{\partial \eta_2} & = \begin{bsmallmatrix} 0 & 0 \\ 0 & -\delta' \end{bsmallmatrix}.
\end{align}
%
Contracting with $\Delta^H$ gives the gradient terms:
%
\begin{align}
  (\text{Term 1})_{\log s}      & = \operatorname{tr}(\Delta^H \Theta_{\eta\eta}^{(\nu')}) \Omega_{\nu'} = \nu'(\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) \Omega_{\nu'}, \\
  (\text{Term 1})_{\log \sigma} & = -x'\Delta^H_{22}\Omega_{x'} - \nu'(\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22})\Omega_{\nu'} - \delta'\Delta^H_{22}\Omega_{\delta'}.
\end{align}

\paragraph{Hessian terms (Term 2)}

The coefficient of $\frac{\partial^2 \Omega}{\partial \theta_a \partial \theta_b}$ is $C^{(2,\eta_l)}_{ab} = \sum_{i,j} \Delta^H_{ij} ( 2 \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_l} \frac{\partial \theta_b}{\partial \eta_j} + \frac{\partial^2 \theta_a}{\partial \eta_i \partial \eta_j} \frac{\partial \theta_b}{\partial \eta_l} )$.
We compute this for $l=1,2$.

VJP with respect to $\log s$ ($l=1$):
The non-zero derivatives w.r.t. $\eta_1 = \log s$ are $\frac{\partial\nu'}{\partial\eta_1}=\nu'$, $\frac{\partial^2\nu'}{\partial\eta_1^2}=\nu'$, and $\frac{\partial^2\nu'}{\partial\eta_2\partial\eta_1}=-\nu'$.
The only non-zero coefficients are:
%
\begin{align}
  C^{(2,\log s)}_{12} & = \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 x'}{\partial\eta_i\partial\eta_j} \frac{\partial \nu'}{\partial\eta_1} = x'\nu' \Delta^H_{22}                                                                                                                                                     \\
  C^{(2,\log s)}_{21} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \nu'}{\partial\eta_i\partial\eta_1} \frac{\partial x'}{\partial\eta_j} = 2x'\nu'(\Delta^H_{22} - \Delta^H_{12})                                                                                                                                 \\
  C^{(2,\log s)}_{22} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2\nu'}{\partial\eta_i\partial\eta_1} \frac{\partial\nu'}{\partial\eta_j} + \sum_{i,j} \Delta^H_{ij} \frac{\partial^2\nu'}{\partial\eta_i\partial\eta_j} \frac{\partial\nu'}{\partial\eta_1} = 3\nu'^2(\Delta^H_{11}-2\Delta^H_{12}+\Delta^H_{22}) \\
  C^{(2,\log s)}_{23} & = 2 \sum_{i,j} \Delta^H_{ij} \frac{\partial^2\nu'}{\partial\eta_i\partial\eta_1} \frac{\partial\delta'}{\partial\eta_j} = 2\nu'\delta'(\Delta^H_{22} - \Delta^H_{12})                                                                                                                         \\
  C^{(2,\log s)}_{32} & = \sum_{i,j} \Delta^H_{ij} \frac{\partial^2 \delta'}{\partial\eta_i\partial\eta_j} \frac{\partial \nu'}{\partial\eta_1} = \nu'\delta' \Delta^H_{22}.
\end{align}

VJP with respect to $\log \sigma$ ($l=2$):
The derivatives w.r.t. $\eta_2 = \log \sigma$ are more numerous.
We list the non-zero coefficients:
%
\begin{align}
  C^{(2,\log\sigma)}_{11} & = -3x'^2\Delta^H_{22}                                                                                       \\
  C^{(2,\log\sigma)}_{12} & = 2x'(\nu'\Delta^H_{21}-\nu'\Delta^H_{22}) + x'(-\nu')\Delta^H_{22} = x'\nu'(2\Delta^H_{12}-3\Delta^H_{22}) \\
  C^{(2,\log\sigma)}_{13} & = -3x'\delta'\Delta^H_{22}                                                                                  \\
  C^{(2,\log\sigma)}_{21} & = 2(-\nu'(-x'\Delta^H_{12}) + \nu'(-x'\Delta^H_{22})) -x'(\nu'(\Delta^H_{11}-2\Delta^H_{12}+\Delta^H_{22})) \\
                          & = x'\nu'(-\Delta^H_{11} + 4\Delta^H_{12} - 3\Delta^H_{22})                                                  \\
  C^{(2,\log\sigma)}_{22} & = -3\nu'^2(\Delta^H_{11}-2\Delta^H_{12}+\Delta^H_{22})                                                      \\
  C^{(2,\log\sigma)}_{23} & = \nu'\delta'(-\Delta^H_{11} + 4\Delta^H_{12} - 3\Delta^H_{22})                                             \\
  C^{(2,\log\sigma)}_{31} & = -3x'\delta'\Delta^H_{22}                                                                                  \\
  C^{(2,\log\sigma)}_{32} & = \nu'\delta'(2\Delta^H_{12}-3\Delta^H_{22})                                                                \\
  C^{(2,\log\sigma)}_{33} & = -3\delta'^2\Delta^H_{22}.
\end{align}

Summing these contributions gives the full Hessian term for the VJP w.r.t. $\log s$ and $\log \sigma$:
%
\begin{align}
  (\text{Term 2})_{\log s}     & = x'\nu'(3\Delta^H_{22} - 2\Delta^H_{12}) \Omega_{x'\nu'} + 3\nu'^2(\Delta^H_{11}-2\Delta^H_{12}+\Delta^H_{22})\Omega_{\nu'\nu'} + \nu'\delta'(3\Delta^H_{22} - 2\Delta^H_{12})\Omega_{\nu'\delta'}, \\
  (\text{Term 2})_{\log\sigma} & = - 3x'^2\Delta^H_{22} \Omega_{x'x'} + x'\nu'(-\Delta^H_{11} + 6\Delta^H_{12} - 6\Delta^H_{22}) \Omega_{x'\nu'} - 6 x'\delta'\Delta^H_{22} \Omega_{x'\delta'} \nonumber                              \\ & \quad - 3\nu'^2(\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) \Omega_{\nu'\nu'} + \nu'\delta'(-\Delta^H_{11} + 6\Delta^H_{12} - 6\Delta^H_{22}) \Omega_{\nu'\delta'} \nonumber \\ & \quad - 3\delta'^2\Delta^H_{22} \Omega_{\delta'\delta'}.
\end{align}

\paragraph{Third-order terms (Term 3)}

The third-order term is:
%
\begin{align}
  (\text{Term 3})_l         & = \sum_c \frac{\partial\theta_c}{\partial\eta_l} T_c = (\Theta_\eta^T T)_l \\
  \Rightarrow \text{Term 3} & = \Theta_\eta^T T =
  \begin{bmatrix}
    0   & \nu'  & 0        \\
    -x' & -\nu' & -\delta'
  \end{bmatrix}
  \begin{bmatrix} T_1 \\ T_2 \\ T_3 \end{bmatrix} =
  \begin{bmatrix} \nu' T_2 \\ -(x'T_1 + \nu'T_2 + \delta'T_3) \end{bmatrix}.
\end{align}
%
Thus, the components of the third term are:
%
\begin{align}
  (\text{Term 3})_{\log s}      & = \nu' T_2,                           \\
  (\text{Term 3})_{\log \sigma} & = -(x' T_1 + \nu' T_2 + \delta' T_3).
\end{align}

\subsubsection{Simplified VJP expressions}

The brute-force expansion of the VJP expressions is algebraically complex and prone to implementation error.
A more robust approach is to exploit the structure of the coordinate transformation to simplify the formulae.
This is achieved by first defining a set of core VJP components for the $\xi$ derivatives, and then deriving the $\eta$ derivatives from these via simple scaling relationships.

Let the core VJP components with respect to $\theta$, denoted $\mathcal{J}_{\theta} \in \mathbb{R}^3$, be the terms of the full VJP $\Delta^H \bullet \ell_{\eta\eta\xi}$ with the scaling from the Jacobian $\Theta_\xi$ factored out.
Explicitly:
%
\begin{align}
  \mathcal{J}_{x'}      & \coloneqq \Delta^H_{22} \Omega_{x'} + 3 \Delta^H_{22} (x' \Omega_{x'x'} + \delta' \Omega_{x'\delta'}) + (\Delta^H_{11} - 4\Delta^H_{12} + 3\Delta^H_{22}) \nu' \Omega_{x'\nu'} + T_1            \\
  \mathcal{J}_{\mu'}    & \coloneqq (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) (\Omega_{\nu'} + 3\nu'\Omega_{\nu'\nu'}) + (3\Delta^H_{22} - 2\Delta^H_{12}) (x'\Omega_{x'\nu'} + \delta'\Omega_{\nu'\delta'}) + T_2 \\
  \mathcal{J}_{\delta'} & \coloneqq \Delta^H_{22} (\Omega_{\delta'} + 3 x' \Omega_{x'\delta'} + 3 \delta' \Omega_{\delta'\delta'}) + (\Delta^H_{11} - 4\Delta^H_{12} + 3\Delta^H_{22}) \nu' \Omega_{\nu'\delta'} + T_3
\end{align}
%
The full VJP with respect to $\xi$, which we denote $\mathcal{J}_{\xi_k} \coloneqq (\Delta^H \bullet \ell_{\eta\eta\xi})_k$, is then recovered by applying the diagonal Jacobian $\Theta_\xi$:
%
\begin{align}
  \mathcal{J}_x & = \frac{1}{\sigma} \mathcal{J}_{x'}, \qquad \mathcal{J}_\mu = \frac{s}{\sigma} \mathcal{J}_{\mu'}, \qquad \mathcal{J}_\delta = \frac{1}{\sigma} \mathcal{J}_{\delta'}.
\end{align}
%
The VJPs with respect to $\eta$ can be derived from these results without re-expanding the chain rule.
The partial derivatives of the coordinate transformation $\theta$ are related as follows:
%
\begin{align}
  \frac{\partial}{\partial (\log s)}      & = \frac{\nu' \sigma}{s} \frac{\partial}{\partial \mu},                                                                                                            \\
  \frac{\partial}{\partial (\log \sigma)} & = - \left( x' \sigma \frac{\partial}{\partial x} + \frac{\nu' \sigma}{s} \frac{\partial}{\partial \mu} + \delta' \sigma \frac{\partial}{\partial \delta} \right).
\end{align}
%
Since the VJP components $\mathcal{J}_{\eta_l} \coloneqq (\Delta^H \bullet \ell_{\eta\eta\eta})_l$ is the derivative of the scalar quantity $\Delta^H \bullet \ell_{\eta\eta}$ with respect to $\eta_l$, applying the chain rule gives:
%
\begin{align}
  \mathcal{J}_{\log s}      & = \frac{\nu' \sigma}{s} \mathcal{J}_{\mu} = \nu' \mathcal{J}_{\mu'},                                                                                                                                                                           \\
  \mathcal{J}_{\log \sigma} & = - \left( x' \sigma \mathcal{J}_x + \frac{\nu' \sigma}{s} \mathcal{J}_\mu + \delta' \sigma \mathcal{J}_\delta \right) = - (x' \mathcal{J}_{x'} + \nu' \mathcal{J}_{\mu'} + \delta' \mathcal{J}_{\delta'}) \nonumber                           \\
                            & = - \Big[ \Delta^H_{22} (x' \Omega_{x'} + \delta' \Omega_{\delta'}) + (\Delta^H_{11} - 2\Delta^H_{12} + \Delta^H_{22}) (\nu' \Omega_{\nu'} + 3 \nu'^2 \Omega_{\nu'\nu'}) \nonumber                                                             \\
                            & \qquad + 3 \Delta^H_{22} (x'^2 \Omega_{x'x'} + 2 x'\delta' \Omega_{x'\delta'} + \delta'^2 \Omega_{\delta'\delta'}) + (\Delta^H_{11} - 6\Delta^H_{12} + 6\Delta^H_{22}) (x' \nu' \Omega_{x'\nu'} + \nu' \delta' \Omega_{\nu'\delta'}) \nonumber \\
                            & \qquad + (x' T_1 + \nu' T_2 + \delta' T_3) \Big]
\end{align}
%
This formulation computes all five VJP components by first computing the three core components $\mathcal{J}_{\theta}$ and then applying the change of variables, which is simpler and more computationally efficient than evaluating the large explicit expression for $\mathcal{J}_{\log \sigma}$.

\end{document}
